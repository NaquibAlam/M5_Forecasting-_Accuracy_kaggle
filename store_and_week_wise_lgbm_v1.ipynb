{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "import math, decimal\n",
    "import shutil\n",
    "\n",
    "from math import ceil\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from tqdm.auto import tqdm as tqdm\n",
    "import logging\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import os\n",
    "import psutil\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "dec = decimal.Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go through the following on __Kaggle__ for more implementation details: \n",
    "* https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216\n",
    "* https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "* https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "* https://www.kaggle.com/kyakovlev/m5-custom-features\n",
    "* https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(setting, seed, raw_train_file, raw_price_file, raw_calendar_file, raw_submission_file):\n",
    "    '''\n",
    "    this function is used to get all the global variables (apart from those which are defined as constants in other cell)\n",
    "    which will be used in other functions.\n",
    "    '''\n",
    "    data_dir_path = Path(setting['data_dir_path'])\n",
    "\n",
    "    raw_dir_path = data_dir_path / 'raw'\n",
    "    raw_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_name = Path(setting['output_name'])\n",
    "    output_dir_path = data_dir_path / 'output' / output_name\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    result_dir_path = output_dir_path / 'result'\n",
    "    result_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    work_dir_path = output_dir_path / 'work'\n",
    "    work_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_dir_path = output_dir_path / 'model'\n",
    "    model_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    set_seed(seed)\n",
    "   \n",
    "    end_train_day_x_list = [int(fold_id) for fold_id in setting['fold_id_list_csv'].split(',')]\n",
    "    end_train_day_default = 1941\n",
    "\n",
    "    for end_train_day_x in end_train_day_x_list:\n",
    "        (result_dir_path / str(end_train_day_x)).mkdir(parents=True, exist_ok=True)\n",
    "        (work_dir_path / str(end_train_day_x)).mkdir(parents=True, exist_ok=True)\n",
    "        (model_dir_path / str(end_train_day_x)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    end_train_day_x = None\n",
    "\n",
    "    prediction_horizon_list = [int(prediction_horizon) for prediction_horizon in\n",
    "                                    setting['prediction_horizon_list_csv'].split(',')]\n",
    "    prediction_horizon = None\n",
    "    prediction_horizon_prev = None\n",
    "\n",
    "    raw_train_path = raw_dir_path / raw_train_file\n",
    "    raw_price_path = raw_dir_path / raw_price_file\n",
    "    raw_calendar_path = raw_dir_path / raw_calendar_file\n",
    "    raw_submission_path = raw_dir_path / raw_submission_file\n",
    "\n",
    "    #These features lead to overfitting or these values are not present in test set, that's why model will\n",
    "    #train without these features.\n",
    "    remove_features = ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', target] \n",
    "    enable_features = None\n",
    "    mean_features = setting['mean_features']\n",
    "\n",
    "    return (raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, \\\n",
    "            end_train_day_default, prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, \\\n",
    "            raw_submission_path, remove_features, enable_features, mean_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    '''\n",
    "    this function is used to reduce the memory used to store a df.\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "                start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0] / 2. ** 30, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_evaluation(period=100, show_stdv=True, level=logging.INFO):\n",
    "    def _callback(env):\n",
    "        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:\n",
    "            result = '\\t'.join(\n",
    "                [lgb.callback._format_eval_result(x, show_stdv) for x in env.evaluation_result_list])\n",
    "            print(level, '[{}]\\t{}'.format(env.iteration + 1, result))\n",
    "\n",
    "    _callback.order = 10\n",
    "    return _callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('#################### load_data #######################')\n",
    "    train_df = pd.read_csv(raw_train_path)\n",
    "    prices_df = pd.read_csv(raw_price_path)\n",
    "    calendar_df = pd.read_csv(raw_calendar_path)\n",
    "    submission_df = pd.read_csv(raw_submission_path)\n",
    "    \n",
    "    print('train_df.shape', train_df.shape)\n",
    "    print('prices_df.shape', prices_df.shape)\n",
    "    print('calendar_df.shape', calendar_df.shape)\n",
    "    print('submission_df.shape', submission_df.shape)\n",
    "    \n",
    "    return train_df, prices_df, calendar_df, submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_base(train_df, prices_df, calendar_df):\n",
    "    '''\n",
    "    converts horizontal to vertical data and generates dataframe with base features and save as pickle file.\n",
    "    '''\n",
    "    #grid_df will have data till d_(end_train_day_x + predcition_horizon). \n",
    "    #e.g: if end_train_day_x= 1913, predcition_horizon = 7, then d_1 to d_1920 data is there.\n",
    "    #d_1913 is there for train and last 7 days of data is there for holdout/validation.\n",
    "    #after training with data till 1913, we will generate the forecast for d_1914 to d_1920 and compare\n",
    "    #with actual sales numbers (holdout data).\n",
    "    #same goes for grid_df for prices and calendars. They will have the same number of rows which will be concatenated\n",
    "    #along the columns later to get all the base features, and price and calendar related features. \n",
    "\n",
    "    print('################# generate_grid_base ###################')\n",
    "    print('melting to convert horizontal to vertical data')\n",
    "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    # after melting, for each row in horizontal data, we will have 1941 (d_1 to d_1941) rows in vertical data.\n",
    "    grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name= target)\n",
    "    # assert grid_df.shape[0] == train_df.shape[0]*1941\n",
    "    print('grid_df.shape after melting (vertical data)', grid_df.shape)\n",
    "\n",
    "    print('generate holdout data for validation by removing days before end_train_day_x')\n",
    "    num_before = grid_df.shape[0]\n",
    "    grid_df['d_org'] = grid_df['d']\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "    #data from end_train_day_x to (end_train_day_x + prediction_horizon) is saved as holdout dataset\n",
    "    #which will be used to assess the model performance and could also be used for hyperparam tuning.\n",
    "    holdout_df = grid_df[(grid_df['d'] > end_train_day_x) & \\\n",
    "                             (grid_df['d'] <= end_train_day_x + prediction_horizon)][main_index_list + [target]]\n",
    "    \n",
    "    holdout_df.to_csv(holdout_path, index=False)\n",
    "\n",
    "    grid_df = grid_df[grid_df['d'] <= end_train_day_x]\n",
    "    grid_df['d'] = grid_df['d_org']\n",
    "    grid_df = grid_df.drop('d_org', axis=1)\n",
    "    num_after = grid_df.shape[0]\n",
    "    print(num_before, '-->', num_after)\n",
    "\n",
    "    #data from end_train_day_x to (end_train_day_x + prediction_horizon) is being added as the test data which will be predicted using\n",
    "    #the trained model and then this preds will be compared with true labels of holdout set.\n",
    "    print('add test days')\n",
    "    add_grid = pd.DataFrame()\n",
    "    for i in range(prediction_horizon):\n",
    "        temp_df = train_df[index_columns]\n",
    "        temp_df = temp_df.drop_duplicates()\n",
    "        temp_df['d'] = 'd_' + str(end_train_day_x + i + 1)\n",
    "        temp_df[target] = np.nan #target is set as NaN since it is going to be predicted using the trained model\n",
    "        add_grid = pd.concat([add_grid, temp_df])\n",
    "\n",
    "    grid_df = pd.concat([grid_df, add_grid])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "    del temp_df, add_grid\n",
    "    del train_df\n",
    "    \n",
    "    #these features should be treated as categorical variables in lightgbm because of category data types.\n",
    "    #but it always a good idea to exclusively specify these features as categorical_feature param in lightgbm\n",
    "    print('convert to category')\n",
    "    for col in index_columns:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "    #we are calculating the release week which tells when an item in an store started to be sold for the first time.\n",
    "    print('calc release week')\n",
    "    release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id', 'item_id', 'release']\n",
    "    grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
    "    del release_df\n",
    "    grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "    #calculating release week relative to start of data (the item which was sold for the very first time)\n",
    "    #this is being done to lower the range of values for release week by taking the relative value so that\n",
    "    #it can be represented using int16 and memory usage could be reduced.\n",
    "    print('convert release to int16')\n",
    "    grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "    grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "    print('save grid_base')\n",
    "    grid_df.to_pickle(grid_base_path)\n",
    "\n",
    "    print('grid_df.shape', grid_df.shape)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_price(prices_df, calendar_df):\n",
    "    '''\n",
    "    Generates dataframe with price related features and save as pickle file\n",
    "    '''\n",
    "    print('################## generate_grid_price #################')\n",
    "    print('loading grid_base')\n",
    "    grid_df = pd.read_pickle(grid_base_path)\n",
    "    \n",
    "    #calculating max/min/mean/std price of an item in an store across all the weeks it has been sold.\n",
    "    prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "    prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "    prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "    prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
    "    prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "    prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "    calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "    del calendar_prices\n",
    "    \n",
    "    #calculating rate of price change day-by-day, across the month, and across the year\n",
    "    prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])[\n",
    "        'sell_price'].transform(lambda x: x.shift(1))\n",
    "    prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])[\n",
    "        'sell_price'].transform('mean')\n",
    "    prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])[\n",
    "        'sell_price'].transform('mean')\n",
    "    \n",
    "    #calculating the fractional part of the price/max/min\n",
    "    prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]\n",
    "    prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]\n",
    "    prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]\n",
    "\n",
    "    del prices_df['month'], prices_df['year']\n",
    "\n",
    "    print('merge prices')\n",
    "    original_columns = list(grid_df)\n",
    "    grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "    grid_df = grid_df[main_index_list + keep_columns]\n",
    "    grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "    print('save grid_price')\n",
    "    grid_df.to_pickle(grid_price_path)\n",
    "    del prices_df\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def generate_grid_calendar(calendar_df):\n",
    "    '''\n",
    "    Generates dataframe with date, event and promotion related features and save as pickle file\n",
    "    '''\n",
    "    print('################## generate_grid_calendar ####################')\n",
    "    grid_df = pd.read_pickle(grid_base_path)\n",
    "    grid_df = grid_df[main_index_list]\n",
    "    \n",
    "    #moon phase is one feature which have been found to give some improvement. \n",
    "    #perhaps it affects shopping habits shomehow\n",
    "    #This code produces moon phase coded 0 to 7:\n",
    "    def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "        diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)\n",
    "        days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "        lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "        phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "        return int(phase_index) & 7\n",
    "\n",
    "    calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)\n",
    "\n",
    "    # Merge calendar partly\n",
    "    icols = ['date', 'd', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'moon']\n",
    "\n",
    "    grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "    #these features should be treated as categorical variables in lightgbm because of category data types.\n",
    "    #but it always a good idea to exclusively specify these features as categorical_feature param in lightgbm\n",
    "    icols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "    for col in icols:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "    grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8) #day of the momnth\n",
    "    grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8) #week of the year\n",
    "    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8) #month of the year\n",
    "    grid_df['tm_y'] = grid_df['date'].dt.year #year\n",
    "    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8) #year in relative to data start year\n",
    "    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x / 7)).astype(np.int8) #week of the month\n",
    "\n",
    "    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8) #day of the week\n",
    "    grid_df['tm_w_end'] = (grid_df['tm_dw'] >= 5).astype(np.int8) #is_weekend\n",
    "    del grid_df['date']\n",
    "\n",
    "    grid_df.to_pickle(grid_calendar_path)\n",
    "\n",
    "    del calendar_df\n",
    "    del grid_df\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_grid_base():\n",
    "    print('############## modify_grid_base ####################')\n",
    "    grid_df = pd.read_pickle(grid_base_path)\n",
    "    #remove d_ part and just retain the number part for easier comparison.\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "    del grid_df['wm_yr_wk']\n",
    "    grid_df.to_pickle(grid_base_path)\n",
    "    del grid_df\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_file_path():\n",
    "    '''\n",
    "    updates the file path depending on which end_train_day_x it is running for.\n",
    "    There are different folders to save the work, result, and model related data\n",
    "    for different end_train_day_x value.\n",
    "    '''\n",
    "    grid_base_path = work_dir_path / f'grid_base_{prediction_horizon}.pkl'\n",
    "    grid_price_path = work_dir_path / f'grid_price_{prediction_horizon}.pkl'\n",
    "    grid_calendar_path = work_dir_path / f'grid_calendar_{prediction_horizon}.pkl'\n",
    "    holdout_path = result_dir_path / 'holdout.csv'\n",
    "    lag_feature_path = work_dir_path / f'lag_feature_{prediction_horizon}.pkl'\n",
    "    target_encoding_feature_path = work_dir_path / f'target_encoding_{prediction_horizon}.pkl'\n",
    "    result_submission_path = result_dir_path / 'submission.csv'\n",
    "    return (grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "    lag_feature_path, target_encoding_feature_path, result_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dir_path():\n",
    "    global result_dir_path, work_dir_path, model_dir_path #to modify the same variables which have been defined as global\n",
    "    result_dir_path = output_dir_path / 'result'\n",
    "    result_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    work_dir_path = output_dir_path / 'work'\n",
    "    work_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_dir_path = output_dir_path / 'work'\n",
    "    model_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predict_horizon():\n",
    "    update_file_path()\n",
    "    num_lag_day_list = []\n",
    "    #lag feature length depends on the prediction horizon (7, 14, 21 , 28). \n",
    "    #lag length varies from prediction_horizon to (prediction_horizon + num_lag_day)\n",
    "    #for predection horizon=28, why lag from 28-42 and not 1-15? Because for test data (future dates),we wont have the sales numbers \n",
    "    #for 1-27 lags since these lags will also be in future for 1-28 days out forecast.\n",
    "    #The lags are different for each week model. Model w1's (1-7 days out) lags are {7, 8, 9, ..., 21 }. \n",
    "    #Model w4's (21-28 days out) lags are {28, 29, 30, ..., 42}\n",
    "    for col in range(prediction_horizon, prediction_horizon + num_lag_day):\n",
    "        num_lag_day_list.append(col)\n",
    "\n",
    "    num_shift_rolling_day_list = []\n",
    "    for num_shift_day in [1, 7, 14]:\n",
    "        for num_rolling_day in [7, 14, 30, 60]:\n",
    "            num_shift_rolling_day_list.append([num_shift_day, num_rolling_day])\n",
    "    return (num_lag_day_list, num_shift_rolling_day_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_full(train_df, prices_df, calendar_df):\n",
    "    '''\n",
    "    generates base, price and calendar related features and saves those as pickle files.\n",
    "    '''\n",
    "    generate_grid_base(train_df, prices_df, calendar_df)\n",
    "    generate_grid_price(prices_df, calendar_df)\n",
    "    generate_grid_calendar(calendar_df)\n",
    "    modify_grid_base()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lag_feature(grid_base_path, lag_feature_path, num_lag_day_list, num_rolling_day_list, recursive_feature_flag, num_shift_rolling_day_list):\n",
    "    '''\n",
    "    generate lag and rolling (mean/std) features and save as pickle file.\n",
    "    '''\n",
    "    print('################ generate_lag features ################')\n",
    "    print('load grid_base')\n",
    "    grid_df = pd.read_pickle(grid_base_path)\n",
    "\n",
    "    grid_df = grid_df[['id', 'd', 'sales']]\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('creating lags')\n",
    "\n",
    "    grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in num_lag_day_list\n",
    "        for col in [target]\n",
    "    })\n",
    "\n",
    "    for col in list(grid_df):\n",
    "        if 'lag' in col:\n",
    "            grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('create rolling aggs (mean/std)')\n",
    "    # why it is getting shifted by \"prediction_horizon\" before taking rolling mean/std? for the test data (future dates), \n",
    "    #during our forecast horizon we wont have the sales numbers to take the rolling mean/std, that's why first we have to \n",
    "    #shift by \"prediction_horizon\" and then take the rolling mean/std. So effectively, for a prediction_horizon of 28 and \n",
    "    #rolling mean/std of 7 days is effcetively rolling mean/std of 35 days. for each week you would have to shift those \n",
    "    #features forward by 7 days for the first week (1-7 days model), 14 days for the second (8-14 days model), etc.\n",
    "\n",
    "    for num_rolling_day in num_rolling_day_list:\n",
    "        print('rolling period', num_rolling_day)\n",
    "        grid_df['rolling_mean_' + str(num_rolling_day)] = grid_df.groupby(['id'])[target].transform(\n",
    "            lambda x: x.shift(prediction_horizon).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "        grid_df['rolling_std_' + str(num_rolling_day)] = grid_df.groupby(['id'])[target].transform(\n",
    "            lambda x: x.shift(prediction_horizon).rolling(num_rolling_day).std()).astype(np.float16)\n",
    "        \n",
    "    #its better not to use recursive features since they introduce leakage in some way.\n",
    "    if recursive_feature_flag:\n",
    "        for num_shift_rolling_day in num_shift_rolling_day_list:\n",
    "            num_shift_day = num_shift_rolling_day[0]\n",
    "            num_rolling_day = num_shift_rolling_day[1]\n",
    "            col_name = 'rolling_mean_tmp_' + str(num_shift_day) + '_' + str(num_rolling_day)\n",
    "            grid_df[col_name] = grid_df.groupby(['id'])[target].transform(\n",
    "                lambda x: x.shift(num_shift_day).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "\n",
    "    print('save lag_feature')\n",
    "    grid_df.to_pickle(lag_feature_path)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_encoding_feature(grid_base_path):\n",
    "    '''\n",
    "    calculate mean encoding features of categorical variables or pairs and save as pickle file.\n",
    "    '''\n",
    "    print('################ generate_target_encoding_feature ################')\n",
    "    set_seed(seed)\n",
    "\n",
    "    grid_df = pd.read_pickle(grid_base_path)\n",
    "    # to be sure we don't have leakage in our validation set, that's why target encoding is calculated using train data only and\n",
    "    # the same encoded value for any group is filled for validation data.\n",
    "    grid_df[target][grid_df['d'] > (end_train_day_x - prediction_horizon)] = np.nan\n",
    "    base_cols = list(grid_df)\n",
    "    \n",
    "    # Because of memory issues we can't use many features.\n",
    "    icols = [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "    # There are several ways to do \"mean\" encoding. 1) K-fold scheme 2)LOO (leave one out) 3)Smoothed/regularized 4)Catboost encoder etc.\n",
    "    # We will use simple target encoding by std and mean agg\n",
    "    \n",
    "    for col in icols:\n",
    "        print('encoding', col)\n",
    "        col_name = '_' + '_'.join(col) + '_'\n",
    "        grid_df['enc' + col_name + 'mean'] = grid_df.groupby(col)[target].transform('mean').astype(np.float16)\n",
    "        grid_df['enc' + col_name + 'std'] = grid_df.groupby(col)[target].transform('std').astype(np.float16)\n",
    "\n",
    "    keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "    grid_df = grid_df[['id', 'd'] + keep_cols]\n",
    "\n",
    "    print('save target_encoding_feature')\n",
    "    grid_df.to_pickle(target_encoding_feature_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid_full():\n",
    "    '''\n",
    "    loads base, price and calendar features in one dataframe post concatenation.\n",
    "    '''\n",
    "    print('############ load_grid_full #####################')\n",
    "    grid_df = pd.concat([pd.read_pickle(grid_base_path),\n",
    "                         pd.read_pickle(grid_price_path).iloc[:, 2:], #first 2 columns are ['id', 'd'] which already come from base df\n",
    "                         pd.read_pickle(grid_calendar_path).iloc[:, 2:]],\n",
    "                        axis=1)\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df_full = load_grid_full()\n",
    "# grid_df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.store_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid_by_store(store_id):\n",
    "    '''\n",
    "    loads all features (base, price, calendar, lag and encoding) in one dataframe for a given store or all stores.\n",
    "    '''\n",
    "    print(\"Loading the data for store: {}\".format(store_id))\n",
    "    df = load_grid_full()\n",
    "\n",
    "    if store_id != 'all':\n",
    "        df = df[df['store_id'] == store_id]\n",
    "    # With memory limits we have to read lags and mean encoding features separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned, we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(target_encoding_feature_path)[mean_features] \n",
    "    #select only those rows which are present in df since df2 contains data for all stores and but we have to choose data for \n",
    "    #a given store_id\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "\n",
    "    df3 = pd.read_pickle(lag_feature_path).iloc[:, 3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "\n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2\n",
    "\n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3\n",
    "    \n",
    "    #model will train only with enable_features after getting rid of remove_features\n",
    "    enable_features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id', 'd', target] + enable_features]\n",
    "\n",
    "    df = df[df['d'] >= start_train_day_x].reset_index(drop=True)\n",
    "\n",
    "    return df, enable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, enable_features= load_grid_by_store(\"CA_1\")\n",
    "# df.columns\n",
    "# enable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_test(store_id_set_list):\n",
    "    '''\n",
    "    load the test data (dummy test data) for all stores in one dataframe.\n",
    "    '''\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in store_id_set_list:\n",
    "        temp_df = pd.read_pickle(\n",
    "            work_dir_path / f'test_{store_id}_{prediction_horizon}.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    return base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_test = load_base_test(train_df.store_id.unique().tolist())\n",
    "# base_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_evaluation(period=100, show_stdv=True, level=logging.INFO):\n",
    "    '''\n",
    "    logs the evaluation metric while training \n",
    "    '''\n",
    "    print('#################### log_evaluation #######################')\n",
    "    def _callback(env):\n",
    "        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:\n",
    "            result = '\\t'.join(\n",
    "                [lgb.callback._format_eval_result(x, show_stdv) for x in env.evaluation_result_list])\n",
    "            print(level, '[{}]\\t{}'.format(env.iteration + 1, result))\n",
    "\n",
    "    _callback.order = 10\n",
    "    return _callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_df, calendar_df, prices_df, submission_df):\n",
    "    \n",
    "    print('######################################## train_and_predict #####################################################')\n",
    "    #following parameters have been found after CV.\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 2 ** 11 - 1,\n",
    "        'min_data_in_leaf': 2 ** 12 - 1,\n",
    "        'feature_fraction': 0.5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 1, #1400\n",
    "        'boost_from_average': False,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "\n",
    "    set_seed(seed)\n",
    "    lgb_params['seed'] = seed\n",
    "    #load the list of all 10 different stores.\n",
    "    store_id_set_list = list(train_df['store_id'].unique())\n",
    "\n",
    "    feature_importance_all_df = pd.DataFrame()\n",
    "    for store_index, store_id in enumerate(store_id_set_list):\n",
    "        print('train', store_id)\n",
    "        # Get grid (all features) for current store\n",
    "        grid_df, enable_features = load_grid_by_store(store_id)\n",
    "        enable_features = enable_features\n",
    "        #partition train, valid and test (dummy) data\n",
    "        train_mask = grid_df['d'] <= end_train_day_x\n",
    "        valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - prediction_horizon))\n",
    "        preds_mask = grid_df['d'] > (end_train_day_x - 100)\n",
    "        print(\"train_mask horizon: d_{}-d_{}\".format(min(grid_df[train_mask][\"d\"]), max(grid_df[train_mask][\"d\"])))\n",
    "        print(\"valid_mask horizon: d_{}-d_{}\".format(min(grid_df[valid_mask][\"d\"]), max(grid_df[valid_mask][\"d\"])))\n",
    "        print(\"pred_mask horizon: d_{}-d_{}\".format(min(grid_df[preds_mask][\"d\"]), max(grid_df[preds_mask][\"d\"])))\n",
    "        print('[{3} - {4}] train {0}/{1} {2}'.format(\n",
    "            store_index + 1, len(store_id_set_list), store_id,\n",
    "            end_train_day_x, prediction_horizon))\n",
    "        if export_all_flag:\n",
    "            print('export train')\n",
    "            grid_df[train_mask].to_csv(\n",
    "                result_dir_path / ('exp_train_' + store_id + '.csv'), index=False)\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][enable_features],\n",
    "                                 label=grid_df[train_mask][target])\n",
    "\n",
    "        if export_all_flag:\n",
    "            print('export valid')\n",
    "            grid_df[valid_mask].to_csv(\n",
    "                result_dir_path / ('exp_valid_' + store_id + '.csv'), index=False)\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "                                 label=grid_df[valid_mask][target])\n",
    "\n",
    "        if export_all_flag:\n",
    "            print('export test')\n",
    "            grid_df[preds_mask].to_csv(\n",
    "                result_dir_path / ('exp_test_' + store_id + '.csv'), index=False)\n",
    "\n",
    "        if export_all_flag:\n",
    "            print('export train_valid_test')\n",
    "            grid_df[train_mask | valid_mask | preds_mask].to_csv(\n",
    "                result_dir_path / ('exp_train_valid_test_' + store_id + '.csv'), index=False)\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "                                 label=grid_df[valid_mask][target])\n",
    "\n",
    "        # Saving part of the dataset (dummy test data) for later predictions\n",
    "        # Removing features that we need to calculate recursively\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        if recursive_feature_flag:\n",
    "            keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "            grid_df = grid_df[keep_cols]\n",
    "        #this \"test_{store_id}_{prediction_horizon}.pkl\" file will be loaded later and go through\n",
    "        #prediction and prediction will be saved as pred_h_df and pred_v_df\n",
    "        grid_df.to_pickle(work_dir_path / f'test_{store_id}_{prediction_horizon}.pkl')\n",
    "        del grid_df\n",
    "\n",
    "        set_seed(seed)\n",
    "        #lightgbm model training starts here\n",
    "        estimator = lgb.train(lgb_params,\n",
    "                              train_data,\n",
    "                              valid_sets=[valid_data],\n",
    "                              verbose_eval=True, #False\n",
    "                              callbacks=[log_evaluation(period=100)],\n",
    "                              )\n",
    "\n",
    "        model_name = str(\n",
    "            model_dir_path / f'lgb_model_{store_id}_{prediction_horizon}.bin')\n",
    "        #feature importance for the store it is running currently\n",
    "        feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),\n",
    "                                                   columns=['feature_name', 'importance'])\n",
    "        feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)\n",
    "        feature_importance_store_df['store_id'] = store_id\n",
    "        #save the feature importance for a given store and prediction_horizon\n",
    "        feature_importance_store_df.to_csv(\n",
    "            result_dir_path / ('feature_importance_{0}_{1}.csv'.format(\n",
    "                store_id, prediction_horizon)), index=False)\n",
    "        #concat to store the feature importance for all the stores and a given prediction_horizon\n",
    "        feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])\n",
    "        #save the model for the given store and prediction_horizon.\n",
    "        # Save model - it's not real '.bin' but a pickle file\n",
    "        # estimator = lgb.Booster(model_file='model.txt') can only predict with the best iteration (or the saving iteration)\n",
    "        # pickle.dump gives us more flexibilitylike estimator.predict(TEST, num_iteration=100)\n",
    "        # num_iteration - number of iteration want to predict with, NULL or <= 0 means use best iteration\n",
    "        pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "        del train_data, valid_data, estimator\n",
    "        gc.collect()\n",
    "\n",
    "    print('aggregate feature importance')\n",
    "    feature_importance_all_df.to_csv(result_dir_path / 'feature_importance_all_{0}.csv'.format(\n",
    "        prediction_horizon), index=False)\n",
    "    #aggregtaing (mean/std) the importance of all the features (for a given prediction_horizon) across all the stores.\n",
    "    feature_importance_agg_df = feature_importance_all_df.groupby(\n",
    "        'feature_name')['importance'].agg(['mean', 'std']).reset_index()\n",
    "    feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']\n",
    "    feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)\n",
    "    feature_importance_agg_df.to_csv(result_dir_path / 'feature_importance_agg_{0}.csv'.format(\n",
    "        prediction_horizon), index=False)\n",
    "    \n",
    "    print('load base_test')\n",
    "    #load the base_test data (which was saved as grid_df[preds_mask]) to perform the prediction using trained model.\n",
    "    base_test = load_base_test(store_id_set_list)\n",
    "\n",
    "    if export_all_flag:\n",
    "        base_test.to_csv(\n",
    "            result_dir_path / 'exp_base_test_{0}_a.csv'.format(prediction_horizon),\n",
    "            index=False)\n",
    "    #now we are populating the pred_v_df and pred_h_df for the test data for a given prediction_horizon\n",
    "    if prediction_horizon_prev > 0:\n",
    "        pred_v_prev_df = None\n",
    "        for ph in prediction_horizon_list:\n",
    "            if ph <= prediction_horizon_prev:\n",
    "                pred_v_temp_df = pd.read_csv(result_dir_path / 'pred_v_{}.csv'.format(ph))\n",
    "                pred_v_prev_df = pd.concat([pred_v_prev_df, pred_v_temp_df])\n",
    "        for predict_day in range(1, prediction_horizon_prev + 1):\n",
    "            base_test[target][base_test['d'] == (end_train_day_x + predict_day)] = \\\n",
    "                pred_v_prev_df[target][\n",
    "                    pred_v_prev_df['d'] == (end_train_day_x + predict_day)].values\n",
    "\n",
    "    if export_all_flag:\n",
    "        base_test.to_csv(\n",
    "            result_dir_path / 'exp_base_test_{0}_b.csv'.format(prediction_horizon),\n",
    "            index=False)\n",
    "\n",
    "    main_time = time.time()\n",
    "    pred_h_df = pd.DataFrame()\n",
    "    #looping for each predict_day 1-by-1. when prediction_horizon=7 then prediction_horizon_prev will be 0 and it will \n",
    "    #loop for 1-7 predict day. \n",
    "    #when prediction_horizon=14 then prediction_horizon_prev will be 7 and it will loop for 8-14 predict_day\n",
    "    for predict_day in range(prediction_horizon_prev + 1, prediction_horizon + 1):\n",
    "        print('predict day{:02d}'.format(predict_day))\n",
    "        start_time = time.time()\n",
    "        grid_df = base_test.copy()\n",
    "        #usually we set recursive_feature_flag to False.\n",
    "        if recursive_feature_flag:\n",
    "            print('[{0} - {1}] calculate recursive features'.format(\n",
    "                end_train_day_x, prediction_horizon))\n",
    "            for num_shift_rolling_day in num_shift_rolling_day_list:\n",
    "                num_shift_day = num_shift_rolling_day[0]\n",
    "                num_rolling_day = num_shift_rolling_day[1]\n",
    "                lag_df = base_test[['id', 'd', target]]\n",
    "                col_name = 'rolling_mean_tmp_' + str(num_shift_day) + '_' + str(num_rolling_day)\n",
    "                lag_df[col_name] = lag_df.groupby(['id'])[target].transform(\n",
    "                    lambda x: x.shift(num_shift_day).rolling(num_rolling_day).mean())\n",
    "                grid_df = pd.concat([grid_df, lag_df[[col_name]]], axis=1)\n",
    "\n",
    "        day_mask = base_test['d'] == (end_train_day_x + predict_day)\n",
    "        if export_all_flag:\n",
    "            print('export recursive_features')\n",
    "            grid_df[day_mask].to_csv(result_dir_path / 'exp_recursive_features_{0}_{1}.csv'.format(\n",
    "                prediction_horizon, predict_day), index=False)\n",
    "        for store_index, store_id in enumerate(store_id_set_list):\n",
    "            # Read all our models and make predictions for each day/store pairs\n",
    "            print('[{3} - {4}] predict {0}/{1} {2} day {5}'.format(\n",
    "                store_index + 1, len(store_id_set_list), store_id,\n",
    "                end_train_day_x, prediction_horizon, predict_day))\n",
    "\n",
    "            model_path = str(\n",
    "                model_dir_path / f'lgb_model_{store_id}_{prediction_horizon}.bin')\n",
    "            #loading the model for a given store and prediction_horizon\n",
    "            estimator = pickle.load(open(model_path, 'rb'))\n",
    "            #selecting the test_data for given store_id and given predict_day\n",
    "            if store_id != 'all':\n",
    "                store_mask = base_test['store_id'] == store_id\n",
    "                mask = (day_mask) & (store_mask)\n",
    "            else:\n",
    "                mask = day_mask\n",
    "\n",
    "            if export_all_flag:\n",
    "                print('export pred')\n",
    "                grid_df[mask].to_csv(\n",
    "                    result_dir_path / (\n",
    "                            'exp_pred_' + store_id + '_day_' + str(predict_day) + '.csv'), index=False)\n",
    "            #predict the base_test for a given predict_day and store\n",
    "            base_test[target][mask] = estimator.predict(grid_df[mask][enable_features])\n",
    "\n",
    "        temp_df = base_test[day_mask][['id', target]]\n",
    "        temp_df.columns = ['id', 'F' + str(predict_day)]\n",
    "        #merge the dataframe to populate the prediction columns for different predict_days (1-by-1) for a given prediction_horizon\n",
    "        if 'id' in list(pred_h_df):\n",
    "            pred_h_df = pred_h_df.merge(temp_df, on=['id'], how='left')\n",
    "        else:\n",
    "            pred_h_df = temp_df.copy()\n",
    "\n",
    "        del temp_df\n",
    "\n",
    "    if export_all_flag:\n",
    "        base_test.to_csv(\n",
    "            result_dir_path / 'exp_base_test_{0}_c.csv'.format(prediction_horizon),\n",
    "            index=False)\n",
    "    #save pred_h_df for a given prediction_horizon\n",
    "    pred_h_df.to_csv(result_dir_path / 'pred_h_{}.csv'.format(\n",
    "        prediction_horizon), index=False)\n",
    "    #populate the predcition in vertical pred df using the prediction output on base_test for all stores and given prediction_horizon.\n",
    "    pred_v_df = base_test[\n",
    "        (base_test['d'] >= end_train_day_x + prediction_horizon_prev + 1) *\n",
    "        (base_test['d'] < end_train_day_x + prediction_horizon + 1)\n",
    "        ][main_index_list + [target]]\n",
    "    #save pred_v_df for a given prediction_horizon\n",
    "    pred_v_df.to_csv(result_dir_path / 'pred_v_{}.csv'.format(prediction_horizon),\n",
    "                     index=False)\n",
    "\n",
    "    return pred_h_df, pred_v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "    '''\n",
    "    this class is for WRMSSE calculation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame,\n",
    "                 calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 'all'  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')] \\\n",
    "            .columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')] \\\n",
    "            .columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df],\n",
    "                                 axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id) \\\n",
    "                [valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns] \\\n",
    "            .set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index() \\\n",
    "            .rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left',\n",
    "                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']) \\\n",
    "                        .unstack(level=2)['value'] \\\n",
    "                        .loc[zip(self.train_df.item_id, self.train_df.store_id), :] \\\n",
    "            .reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns],\n",
    "                               weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame,\n",
    "                                       np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape \\\n",
    "               == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds,\n",
    "                                       columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns],\n",
    "                                 valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n",
    "            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n",
    "\n",
    "            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n",
    "            setattr(self, f'lv{i + 1}_scores', lv_scores)\n",
    "\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1,\n",
    "                                  sort=False).prod(axis=1)\n",
    "\n",
    "            all_scores.append(lv_scores[~lv_scores.isin([np.inf])].sum())\n",
    "        self.all_scores = all_scores\n",
    "\n",
    "        return np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wrmsse(train_df, prices_df, calendar_df, submission_df, all_preds):\n",
    "    '''\n",
    "    calculates Weighted Root Mean Squared Scaled Error (RMSSE) which is the evaluation parameter for this competition.\n",
    "    '''\n",
    "    print('############################# calc_wrmsse ###################################')\n",
    "    temp_df = train_df\n",
    "    print('adjust end of train period')\n",
    "    num_before = train_df.shape\n",
    "    num_diff_days = end_train_day_default - end_train_day_x - \\\n",
    "                    prediction_horizon\n",
    "    if num_diff_days > 0:\n",
    "        temp_df = train_df.iloc[:, :-1 * num_diff_days]\n",
    "    num_after = temp_df.shape\n",
    "    print(num_before, '-->', num_after)\n",
    "\n",
    "    train_fold_df = temp_df.iloc[:, :-28]\n",
    "    valid_fold_df = temp_df.iloc[:, -28:].copy()\n",
    "\n",
    "    valid_preds = submission_df[submission_df['id'].str.contains('evaluation')][['id']]\n",
    "    valid_preds = valid_preds.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "    valid_preds = valid_preds.drop('id', axis=1)\n",
    "    valid_preds.columns = valid_fold_df.columns\n",
    "    train_fold_df.to_csv(result_dir_path / 'eval_wrmsse_train.csv', index=False)\n",
    "    valid_fold_df.to_csv(result_dir_path / 'eval_wrmsse_test.csv', index=False)\n",
    "    valid_preds.to_csv(result_dir_path / 'eval_wrmsse_pred.csv', index=False)\n",
    "\n",
    "    evaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar_df, prices_df)\n",
    "    wrmsse = evaluator.score(valid_preds)\n",
    "    print('wrmsse', wrmsse)\n",
    "\n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Global variables\n",
    "base_dir = \"/home/jupyter/m5-forecasting-kaggle/\"\n",
    "raw_train_file = \"sales_train_evaluation.csv\"\n",
    "raw_price_file = \"sell_prices.csv\"\n",
    "raw_calendar_file = \"calendar.csv\"\n",
    "raw_submission_file = \"sample_submission.csv\"\n",
    "main_index_list = ['id', 'd']\n",
    "target= \"sales\" #target variable which needs to be forecasted\n",
    "start_train_day_x = 1 # We can skip some rows (Nans/faster training)\n",
    "recursive_feature_flag = False #whether we want to use recursive features or not. Usually we avoid using recursive features to avoid data leakage.\n",
    "seed = 42 # We want all things to be as deterministic as possible\n",
    "export_all_flag = False #whether we want to save some intermediate files and results or not (usually False)\n",
    "\n",
    "num_lag_day = 15 #number of lag features to be created\n",
    "num_rolling_day_list = [7, 14, 30, 60, 180] #different window size for rolling mean/std features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few other global variables which is provided as dict.\n",
    "setting = dict()\n",
    "setting['data_dir_path'] =  base_dir +\"Data/\"\n",
    "setting['output_name'] = \"default\"\n",
    "# list of End days of our train set. Serves as multiple validation sets for robust evaluation of the model \n",
    "#this different evaluation sets can also be utilised for tuning the hyper-parameters.\n",
    "setting['fold_id_list_csv'] = '1941,1913,1885,1857,1829,1577' \n",
    "setting['prediction_horizon_list_csv']=  '7,14,21,28' #list of prediction horizon. We are building 4 different models, one for each of 4 weeks (1-7, 8-14, 15-21, 22-28 days).\n",
    "#Because of memory issues we can't use many encoding features. Model will train only with following encoding features.\n",
    "setting['mean_features'] = [\n",
    "                            'enc_cat_id_mean', 'enc_cat_id_std',\n",
    "                            'enc_dept_id_mean', 'enc_dept_id_std',\n",
    "                            'enc_item_id_mean', 'enc_item_id_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/raw'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/output/default'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/output/default/result'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/output/default/work'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/output/default/model'),\n",
       " [1941, 1913, 1885, 1857, 1829, 1577],\n",
       " [7, 14, 21, 28],\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/raw/sales_train_evaluation.csv'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/raw/sell_prices.csv'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/raw/calendar.csv'),\n",
       " PosixPath('/home/jupyter/m5-forecasting-kaggle/Data/raw/sample_submission.csv'),\n",
       " ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', 'sales'],\n",
       " None,\n",
       " ['enc_cat_id_mean',\n",
       "  'enc_cat_id_std',\n",
       "  'enc_dept_id_mean',\n",
       "  'enc_dept_id_std',\n",
       "  'enc_item_id_mean',\n",
       "  'enc_item_id_std'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to get some other global variables using set_params() function\n",
    "raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, end_train_day_default, \\\n",
    "           prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path, \\\n",
    "           remove_features, enable_features, mean_features = set_params(setting, seed, raw_train_file, raw_price_file, raw_calendar_file, raw_submission_file)\n",
    "\n",
    "raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, \\\n",
    "           prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path, \\\n",
    "           remove_features, enable_features, mean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n",
      "#################### load_data #######################\n",
      "train_df.shape (30490, 1947)\n",
      "prices_df.shape (6841121, 4)\n",
      "calendar_df.shape (1969, 14)\n",
      "submission_df.shape (60980, 29)\n",
      "----------------- fold_id 1941 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 59181090\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (59394520, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.64949\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.21691\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.78407\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.98329\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.15367\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.63709\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.62066\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.56234\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.09731\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1935-d_1941\n",
      "pred_mask horizon: d_1842-d_1948\n",
      "[1941 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.57806\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1941 - 7] predict 1/10 CA_1 day 1\n",
      "[1941 - 7] predict 2/10 CA_2 day 1\n",
      "[1941 - 7] predict 3/10 CA_3 day 1\n",
      "[1941 - 7] predict 4/10 CA_4 day 1\n",
      "[1941 - 7] predict 5/10 TX_1 day 1\n",
      "[1941 - 7] predict 6/10 TX_2 day 1\n",
      "[1941 - 7] predict 7/10 TX_3 day 1\n",
      "[1941 - 7] predict 8/10 WI_1 day 1\n",
      "[1941 - 7] predict 9/10 WI_2 day 1\n",
      "[1941 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1941 - 7] predict 1/10 CA_1 day 2\n",
      "[1941 - 7] predict 2/10 CA_2 day 2\n",
      "[1941 - 7] predict 3/10 CA_3 day 2\n",
      "[1941 - 7] predict 4/10 CA_4 day 2\n",
      "[1941 - 7] predict 5/10 TX_1 day 2\n",
      "[1941 - 7] predict 6/10 TX_2 day 2\n",
      "[1941 - 7] predict 7/10 TX_3 day 2\n",
      "[1941 - 7] predict 8/10 WI_1 day 2\n",
      "[1941 - 7] predict 9/10 WI_2 day 2\n",
      "[1941 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1941 - 7] predict 1/10 CA_1 day 3\n",
      "[1941 - 7] predict 2/10 CA_2 day 3\n",
      "[1941 - 7] predict 3/10 CA_3 day 3\n",
      "[1941 - 7] predict 4/10 CA_4 day 3\n",
      "[1941 - 7] predict 5/10 TX_1 day 3\n",
      "[1941 - 7] predict 6/10 TX_2 day 3\n",
      "[1941 - 7] predict 7/10 TX_3 day 3\n",
      "[1941 - 7] predict 8/10 WI_1 day 3\n",
      "[1941 - 7] predict 9/10 WI_2 day 3\n",
      "[1941 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1941 - 7] predict 1/10 CA_1 day 4\n",
      "[1941 - 7] predict 2/10 CA_2 day 4\n",
      "[1941 - 7] predict 3/10 CA_3 day 4\n",
      "[1941 - 7] predict 4/10 CA_4 day 4\n",
      "[1941 - 7] predict 5/10 TX_1 day 4\n",
      "[1941 - 7] predict 6/10 TX_2 day 4\n",
      "[1941 - 7] predict 7/10 TX_3 day 4\n",
      "[1941 - 7] predict 8/10 WI_1 day 4\n",
      "[1941 - 7] predict 9/10 WI_2 day 4\n",
      "[1941 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1941 - 7] predict 1/10 CA_1 day 5\n",
      "[1941 - 7] predict 2/10 CA_2 day 5\n",
      "[1941 - 7] predict 3/10 CA_3 day 5\n",
      "[1941 - 7] predict 4/10 CA_4 day 5\n",
      "[1941 - 7] predict 5/10 TX_1 day 5\n",
      "[1941 - 7] predict 6/10 TX_2 day 5\n",
      "[1941 - 7] predict 7/10 TX_3 day 5\n",
      "[1941 - 7] predict 8/10 WI_1 day 5\n",
      "[1941 - 7] predict 9/10 WI_2 day 5\n",
      "[1941 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1941 - 7] predict 1/10 CA_1 day 6\n",
      "[1941 - 7] predict 2/10 CA_2 day 6\n",
      "[1941 - 7] predict 3/10 CA_3 day 6\n",
      "[1941 - 7] predict 4/10 CA_4 day 6\n",
      "[1941 - 7] predict 5/10 TX_1 day 6\n",
      "[1941 - 7] predict 6/10 TX_2 day 6\n",
      "[1941 - 7] predict 7/10 TX_3 day 6\n",
      "[1941 - 7] predict 8/10 WI_1 day 6\n",
      "[1941 - 7] predict 9/10 WI_2 day 6\n",
      "[1941 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1941 - 7] predict 1/10 CA_1 day 7\n",
      "[1941 - 7] predict 2/10 CA_2 day 7\n",
      "[1941 - 7] predict 3/10 CA_3 day 7\n",
      "[1941 - 7] predict 4/10 CA_4 day 7\n",
      "[1941 - 7] predict 5/10 TX_1 day 7\n",
      "[1941 - 7] predict 6/10 TX_2 day 7\n",
      "[1941 - 7] predict 7/10 TX_3 day 7\n",
      "[1941 - 7] predict 8/10 WI_1 day 7\n",
      "[1941 - 7] predict 9/10 WI_2 day 7\n",
      "[1941 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1941 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 59181090\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (59607950, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.66037\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.19985\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.88706\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.00776\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.19858\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.72055\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.66715\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.60496\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.06979\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1928-d_1941\n",
      "pred_mask horizon: d_1842-d_1955\n",
      "[1941 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.82684\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1941 - 14] predict 1/10 CA_1 day 8\n",
      "[1941 - 14] predict 2/10 CA_2 day 8\n",
      "[1941 - 14] predict 3/10 CA_3 day 8\n",
      "[1941 - 14] predict 4/10 CA_4 day 8\n",
      "[1941 - 14] predict 5/10 TX_1 day 8\n",
      "[1941 - 14] predict 6/10 TX_2 day 8\n",
      "[1941 - 14] predict 7/10 TX_3 day 8\n",
      "[1941 - 14] predict 8/10 WI_1 day 8\n",
      "[1941 - 14] predict 9/10 WI_2 day 8\n",
      "[1941 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1941 - 14] predict 1/10 CA_1 day 9\n",
      "[1941 - 14] predict 2/10 CA_2 day 9\n",
      "[1941 - 14] predict 3/10 CA_3 day 9\n",
      "[1941 - 14] predict 4/10 CA_4 day 9\n",
      "[1941 - 14] predict 5/10 TX_1 day 9\n",
      "[1941 - 14] predict 6/10 TX_2 day 9\n",
      "[1941 - 14] predict 7/10 TX_3 day 9\n",
      "[1941 - 14] predict 8/10 WI_1 day 9\n",
      "[1941 - 14] predict 9/10 WI_2 day 9\n",
      "[1941 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1941 - 14] predict 1/10 CA_1 day 10\n",
      "[1941 - 14] predict 2/10 CA_2 day 10\n",
      "[1941 - 14] predict 3/10 CA_3 day 10\n",
      "[1941 - 14] predict 4/10 CA_4 day 10\n",
      "[1941 - 14] predict 5/10 TX_1 day 10\n",
      "[1941 - 14] predict 6/10 TX_2 day 10\n",
      "[1941 - 14] predict 7/10 TX_3 day 10\n",
      "[1941 - 14] predict 8/10 WI_1 day 10\n",
      "[1941 - 14] predict 9/10 WI_2 day 10\n",
      "[1941 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1941 - 14] predict 1/10 CA_1 day 11\n",
      "[1941 - 14] predict 2/10 CA_2 day 11\n",
      "[1941 - 14] predict 3/10 CA_3 day 11\n",
      "[1941 - 14] predict 4/10 CA_4 day 11\n",
      "[1941 - 14] predict 5/10 TX_1 day 11\n",
      "[1941 - 14] predict 6/10 TX_2 day 11\n",
      "[1941 - 14] predict 7/10 TX_3 day 11\n",
      "[1941 - 14] predict 8/10 WI_1 day 11\n",
      "[1941 - 14] predict 9/10 WI_2 day 11\n",
      "[1941 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1941 - 14] predict 1/10 CA_1 day 12\n",
      "[1941 - 14] predict 2/10 CA_2 day 12\n",
      "[1941 - 14] predict 3/10 CA_3 day 12\n",
      "[1941 - 14] predict 4/10 CA_4 day 12\n",
      "[1941 - 14] predict 5/10 TX_1 day 12\n",
      "[1941 - 14] predict 6/10 TX_2 day 12\n",
      "[1941 - 14] predict 7/10 TX_3 day 12\n",
      "[1941 - 14] predict 8/10 WI_1 day 12\n",
      "[1941 - 14] predict 9/10 WI_2 day 12\n",
      "[1941 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1941 - 14] predict 1/10 CA_1 day 13\n",
      "[1941 - 14] predict 2/10 CA_2 day 13\n",
      "[1941 - 14] predict 3/10 CA_3 day 13\n",
      "[1941 - 14] predict 4/10 CA_4 day 13\n",
      "[1941 - 14] predict 5/10 TX_1 day 13\n",
      "[1941 - 14] predict 6/10 TX_2 day 13\n",
      "[1941 - 14] predict 7/10 TX_3 day 13\n",
      "[1941 - 14] predict 8/10 WI_1 day 13\n",
      "[1941 - 14] predict 9/10 WI_2 day 13\n",
      "[1941 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1941 - 14] predict 1/10 CA_1 day 14\n",
      "[1941 - 14] predict 2/10 CA_2 day 14\n",
      "[1941 - 14] predict 3/10 CA_3 day 14\n",
      "[1941 - 14] predict 4/10 CA_4 day 14\n",
      "[1941 - 14] predict 5/10 TX_1 day 14\n",
      "[1941 - 14] predict 6/10 TX_2 day 14\n",
      "[1941 - 14] predict 7/10 TX_3 day 14\n",
      "[1941 - 14] predict 8/10 WI_1 day 14\n",
      "[1941 - 14] predict 9/10 WI_2 day 14\n",
      "[1941 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1941 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 59181090\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (59821380, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.65067\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.18676\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.88827\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.97018\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.11046\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.70844\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.65602\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.59273\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.23368\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1921-d_1941\n",
      "pred_mask horizon: d_1842-d_1962\n",
      "[1941 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.9501\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1941 - 21] predict 1/10 CA_1 day 15\n",
      "[1941 - 21] predict 2/10 CA_2 day 15\n",
      "[1941 - 21] predict 3/10 CA_3 day 15\n",
      "[1941 - 21] predict 4/10 CA_4 day 15\n",
      "[1941 - 21] predict 5/10 TX_1 day 15\n",
      "[1941 - 21] predict 6/10 TX_2 day 15\n",
      "[1941 - 21] predict 7/10 TX_3 day 15\n",
      "[1941 - 21] predict 8/10 WI_1 day 15\n",
      "[1941 - 21] predict 9/10 WI_2 day 15\n",
      "[1941 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1941 - 21] predict 1/10 CA_1 day 16\n",
      "[1941 - 21] predict 2/10 CA_2 day 16\n",
      "[1941 - 21] predict 3/10 CA_3 day 16\n",
      "[1941 - 21] predict 4/10 CA_4 day 16\n",
      "[1941 - 21] predict 5/10 TX_1 day 16\n",
      "[1941 - 21] predict 6/10 TX_2 day 16\n",
      "[1941 - 21] predict 7/10 TX_3 day 16\n",
      "[1941 - 21] predict 8/10 WI_1 day 16\n",
      "[1941 - 21] predict 9/10 WI_2 day 16\n",
      "[1941 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1941 - 21] predict 1/10 CA_1 day 17\n",
      "[1941 - 21] predict 2/10 CA_2 day 17\n",
      "[1941 - 21] predict 3/10 CA_3 day 17\n",
      "[1941 - 21] predict 4/10 CA_4 day 17\n",
      "[1941 - 21] predict 5/10 TX_1 day 17\n",
      "[1941 - 21] predict 6/10 TX_2 day 17\n",
      "[1941 - 21] predict 7/10 TX_3 day 17\n",
      "[1941 - 21] predict 8/10 WI_1 day 17\n",
      "[1941 - 21] predict 9/10 WI_2 day 17\n",
      "[1941 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1941 - 21] predict 1/10 CA_1 day 18\n",
      "[1941 - 21] predict 2/10 CA_2 day 18\n",
      "[1941 - 21] predict 3/10 CA_3 day 18\n",
      "[1941 - 21] predict 4/10 CA_4 day 18\n",
      "[1941 - 21] predict 5/10 TX_1 day 18\n",
      "[1941 - 21] predict 6/10 TX_2 day 18\n",
      "[1941 - 21] predict 7/10 TX_3 day 18\n",
      "[1941 - 21] predict 8/10 WI_1 day 18\n",
      "[1941 - 21] predict 9/10 WI_2 day 18\n",
      "[1941 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1941 - 21] predict 1/10 CA_1 day 19\n",
      "[1941 - 21] predict 2/10 CA_2 day 19\n",
      "[1941 - 21] predict 3/10 CA_3 day 19\n",
      "[1941 - 21] predict 4/10 CA_4 day 19\n",
      "[1941 - 21] predict 5/10 TX_1 day 19\n",
      "[1941 - 21] predict 6/10 TX_2 day 19\n",
      "[1941 - 21] predict 7/10 TX_3 day 19\n",
      "[1941 - 21] predict 8/10 WI_1 day 19\n",
      "[1941 - 21] predict 9/10 WI_2 day 19\n",
      "[1941 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1941 - 21] predict 1/10 CA_1 day 20\n",
      "[1941 - 21] predict 2/10 CA_2 day 20\n",
      "[1941 - 21] predict 3/10 CA_3 day 20\n",
      "[1941 - 21] predict 4/10 CA_4 day 20\n",
      "[1941 - 21] predict 5/10 TX_1 day 20\n",
      "[1941 - 21] predict 6/10 TX_2 day 20\n",
      "[1941 - 21] predict 7/10 TX_3 day 20\n",
      "[1941 - 21] predict 8/10 WI_1 day 20\n",
      "[1941 - 21] predict 9/10 WI_2 day 20\n",
      "[1941 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1941 - 21] predict 1/10 CA_1 day 21\n",
      "[1941 - 21] predict 2/10 CA_2 day 21\n",
      "[1941 - 21] predict 3/10 CA_3 day 21\n",
      "[1941 - 21] predict 4/10 CA_4 day 21\n",
      "[1941 - 21] predict 5/10 TX_1 day 21\n",
      "[1941 - 21] predict 6/10 TX_2 day 21\n",
      "[1941 - 21] predict 7/10 TX_3 day 21\n",
      "[1941 - 21] predict 8/10 WI_1 day 21\n",
      "[1941 - 21] predict 9/10 WI_2 day 21\n",
      "[1941 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1941 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 59181090\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (60034810, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.61322\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.17804\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.88976\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.9393\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.09758\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.68747\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.60803\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.56914\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.94325\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1941\n",
      "valid_mask horizon: d_1914-d_1941\n",
      "pred_mask horizon: d_1842-d_1969\n",
      "[1941 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.7961\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1941 - 28] predict 1/10 CA_1 day 22\n",
      "[1941 - 28] predict 2/10 CA_2 day 22\n",
      "[1941 - 28] predict 3/10 CA_3 day 22\n",
      "[1941 - 28] predict 4/10 CA_4 day 22\n",
      "[1941 - 28] predict 5/10 TX_1 day 22\n",
      "[1941 - 28] predict 6/10 TX_2 day 22\n",
      "[1941 - 28] predict 7/10 TX_3 day 22\n",
      "[1941 - 28] predict 8/10 WI_1 day 22\n",
      "[1941 - 28] predict 9/10 WI_2 day 22\n",
      "[1941 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1941 - 28] predict 1/10 CA_1 day 23\n",
      "[1941 - 28] predict 2/10 CA_2 day 23\n",
      "[1941 - 28] predict 3/10 CA_3 day 23\n",
      "[1941 - 28] predict 4/10 CA_4 day 23\n",
      "[1941 - 28] predict 5/10 TX_1 day 23\n",
      "[1941 - 28] predict 6/10 TX_2 day 23\n",
      "[1941 - 28] predict 7/10 TX_3 day 23\n",
      "[1941 - 28] predict 8/10 WI_1 day 23\n",
      "[1941 - 28] predict 9/10 WI_2 day 23\n",
      "[1941 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1941 - 28] predict 1/10 CA_1 day 24\n",
      "[1941 - 28] predict 2/10 CA_2 day 24\n",
      "[1941 - 28] predict 3/10 CA_3 day 24\n",
      "[1941 - 28] predict 4/10 CA_4 day 24\n",
      "[1941 - 28] predict 5/10 TX_1 day 24\n",
      "[1941 - 28] predict 6/10 TX_2 day 24\n",
      "[1941 - 28] predict 7/10 TX_3 day 24\n",
      "[1941 - 28] predict 8/10 WI_1 day 24\n",
      "[1941 - 28] predict 9/10 WI_2 day 24\n",
      "[1941 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1941 - 28] predict 1/10 CA_1 day 25\n",
      "[1941 - 28] predict 2/10 CA_2 day 25\n",
      "[1941 - 28] predict 3/10 CA_3 day 25\n",
      "[1941 - 28] predict 4/10 CA_4 day 25\n",
      "[1941 - 28] predict 5/10 TX_1 day 25\n",
      "[1941 - 28] predict 6/10 TX_2 day 25\n",
      "[1941 - 28] predict 7/10 TX_3 day 25\n",
      "[1941 - 28] predict 8/10 WI_1 day 25\n",
      "[1941 - 28] predict 9/10 WI_2 day 25\n",
      "[1941 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1941 - 28] predict 1/10 CA_1 day 26\n",
      "[1941 - 28] predict 2/10 CA_2 day 26\n",
      "[1941 - 28] predict 3/10 CA_3 day 26\n",
      "[1941 - 28] predict 4/10 CA_4 day 26\n",
      "[1941 - 28] predict 5/10 TX_1 day 26\n",
      "[1941 - 28] predict 6/10 TX_2 day 26\n",
      "[1941 - 28] predict 7/10 TX_3 day 26\n",
      "[1941 - 28] predict 8/10 WI_1 day 26\n",
      "[1941 - 28] predict 9/10 WI_2 day 26\n",
      "[1941 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1941 - 28] predict 1/10 CA_1 day 27\n",
      "[1941 - 28] predict 2/10 CA_2 day 27\n",
      "[1941 - 28] predict 3/10 CA_3 day 27\n",
      "[1941 - 28] predict 4/10 CA_4 day 27\n",
      "[1941 - 28] predict 5/10 TX_1 day 27\n",
      "[1941 - 28] predict 6/10 TX_2 day 27\n",
      "[1941 - 28] predict 7/10 TX_3 day 27\n",
      "[1941 - 28] predict 8/10 WI_1 day 27\n",
      "[1941 - 28] predict 9/10 WI_2 day 27\n",
      "[1941 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1941 - 28] predict 1/10 CA_1 day 28\n",
      "[1941 - 28] predict 2/10 CA_2 day 28\n",
      "[1941 - 28] predict 3/10 CA_3 day 28\n",
      "[1941 - 28] predict 4/10 CA_4 day 28\n",
      "[1941 - 28] predict 5/10 TX_1 day 28\n",
      "[1941 - 28] predict 6/10 TX_2 day 28\n",
      "[1941 - 28] predict 7/10 TX_3 day 28\n",
      "[1941 - 28] predict 8/10 WI_1 day 28\n",
      "[1941 - 28] predict 9/10 WI_2 day 28\n",
      "[1941 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (0, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "no holdout\n",
      "generate submission\n",
      "----------------- fold_id 1913 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 58327370\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (58540800, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.29952\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.06956\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.83008\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.92739\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.07144\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.33156\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.2699\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.45112\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.05039\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1907-d_1913\n",
      "pred_mask horizon: d_1814-d_1920\n",
      "[1913 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.59021\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1913 - 7] predict 1/10 CA_1 day 1\n",
      "[1913 - 7] predict 2/10 CA_2 day 1\n",
      "[1913 - 7] predict 3/10 CA_3 day 1\n",
      "[1913 - 7] predict 4/10 CA_4 day 1\n",
      "[1913 - 7] predict 5/10 TX_1 day 1\n",
      "[1913 - 7] predict 6/10 TX_2 day 1\n",
      "[1913 - 7] predict 7/10 TX_3 day 1\n",
      "[1913 - 7] predict 8/10 WI_1 day 1\n",
      "[1913 - 7] predict 9/10 WI_2 day 1\n",
      "[1913 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1913 - 7] predict 1/10 CA_1 day 2\n",
      "[1913 - 7] predict 2/10 CA_2 day 2\n",
      "[1913 - 7] predict 3/10 CA_3 day 2\n",
      "[1913 - 7] predict 4/10 CA_4 day 2\n",
      "[1913 - 7] predict 5/10 TX_1 day 2\n",
      "[1913 - 7] predict 6/10 TX_2 day 2\n",
      "[1913 - 7] predict 7/10 TX_3 day 2\n",
      "[1913 - 7] predict 8/10 WI_1 day 2\n",
      "[1913 - 7] predict 9/10 WI_2 day 2\n",
      "[1913 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1913 - 7] predict 1/10 CA_1 day 3\n",
      "[1913 - 7] predict 2/10 CA_2 day 3\n",
      "[1913 - 7] predict 3/10 CA_3 day 3\n",
      "[1913 - 7] predict 4/10 CA_4 day 3\n",
      "[1913 - 7] predict 5/10 TX_1 day 3\n",
      "[1913 - 7] predict 6/10 TX_2 day 3\n",
      "[1913 - 7] predict 7/10 TX_3 day 3\n",
      "[1913 - 7] predict 8/10 WI_1 day 3\n",
      "[1913 - 7] predict 9/10 WI_2 day 3\n",
      "[1913 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1913 - 7] predict 1/10 CA_1 day 4\n",
      "[1913 - 7] predict 2/10 CA_2 day 4\n",
      "[1913 - 7] predict 3/10 CA_3 day 4\n",
      "[1913 - 7] predict 4/10 CA_4 day 4\n",
      "[1913 - 7] predict 5/10 TX_1 day 4\n",
      "[1913 - 7] predict 6/10 TX_2 day 4\n",
      "[1913 - 7] predict 7/10 TX_3 day 4\n",
      "[1913 - 7] predict 8/10 WI_1 day 4\n",
      "[1913 - 7] predict 9/10 WI_2 day 4\n",
      "[1913 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1913 - 7] predict 1/10 CA_1 day 5\n",
      "[1913 - 7] predict 2/10 CA_2 day 5\n",
      "[1913 - 7] predict 3/10 CA_3 day 5\n",
      "[1913 - 7] predict 4/10 CA_4 day 5\n",
      "[1913 - 7] predict 5/10 TX_1 day 5\n",
      "[1913 - 7] predict 6/10 TX_2 day 5\n",
      "[1913 - 7] predict 7/10 TX_3 day 5\n",
      "[1913 - 7] predict 8/10 WI_1 day 5\n",
      "[1913 - 7] predict 9/10 WI_2 day 5\n",
      "[1913 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1913 - 7] predict 1/10 CA_1 day 6\n",
      "[1913 - 7] predict 2/10 CA_2 day 6\n",
      "[1913 - 7] predict 3/10 CA_3 day 6\n",
      "[1913 - 7] predict 4/10 CA_4 day 6\n",
      "[1913 - 7] predict 5/10 TX_1 day 6\n",
      "[1913 - 7] predict 6/10 TX_2 day 6\n",
      "[1913 - 7] predict 7/10 TX_3 day 6\n",
      "[1913 - 7] predict 8/10 WI_1 day 6\n",
      "[1913 - 7] predict 9/10 WI_2 day 6\n",
      "[1913 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1913 - 7] predict 1/10 CA_1 day 7\n",
      "[1913 - 7] predict 2/10 CA_2 day 7\n",
      "[1913 - 7] predict 3/10 CA_3 day 7\n",
      "[1913 - 7] predict 4/10 CA_4 day 7\n",
      "[1913 - 7] predict 5/10 TX_1 day 7\n",
      "[1913 - 7] predict 6/10 TX_2 day 7\n",
      "[1913 - 7] predict 7/10 TX_3 day 7\n",
      "[1913 - 7] predict 8/10 WI_1 day 7\n",
      "[1913 - 7] predict 9/10 WI_2 day 7\n",
      "[1913 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1913 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 58327370\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (58754230, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.32055\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.02713\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.05969\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.87084\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.99191\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.42714\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.29665\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.49813\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.69733\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1900-d_1913\n",
      "pred_mask horizon: d_1814-d_1927\n",
      "[1913 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.80387\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1913 - 14] predict 1/10 CA_1 day 8\n",
      "[1913 - 14] predict 2/10 CA_2 day 8\n",
      "[1913 - 14] predict 3/10 CA_3 day 8\n",
      "[1913 - 14] predict 4/10 CA_4 day 8\n",
      "[1913 - 14] predict 5/10 TX_1 day 8\n",
      "[1913 - 14] predict 6/10 TX_2 day 8\n",
      "[1913 - 14] predict 7/10 TX_3 day 8\n",
      "[1913 - 14] predict 8/10 WI_1 day 8\n",
      "[1913 - 14] predict 9/10 WI_2 day 8\n",
      "[1913 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1913 - 14] predict 1/10 CA_1 day 9\n",
      "[1913 - 14] predict 2/10 CA_2 day 9\n",
      "[1913 - 14] predict 3/10 CA_3 day 9\n",
      "[1913 - 14] predict 4/10 CA_4 day 9\n",
      "[1913 - 14] predict 5/10 TX_1 day 9\n",
      "[1913 - 14] predict 6/10 TX_2 day 9\n",
      "[1913 - 14] predict 7/10 TX_3 day 9\n",
      "[1913 - 14] predict 8/10 WI_1 day 9\n",
      "[1913 - 14] predict 9/10 WI_2 day 9\n",
      "[1913 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1913 - 14] predict 1/10 CA_1 day 10\n",
      "[1913 - 14] predict 2/10 CA_2 day 10\n",
      "[1913 - 14] predict 3/10 CA_3 day 10\n",
      "[1913 - 14] predict 4/10 CA_4 day 10\n",
      "[1913 - 14] predict 5/10 TX_1 day 10\n",
      "[1913 - 14] predict 6/10 TX_2 day 10\n",
      "[1913 - 14] predict 7/10 TX_3 day 10\n",
      "[1913 - 14] predict 8/10 WI_1 day 10\n",
      "[1913 - 14] predict 9/10 WI_2 day 10\n",
      "[1913 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1913 - 14] predict 1/10 CA_1 day 11\n",
      "[1913 - 14] predict 2/10 CA_2 day 11\n",
      "[1913 - 14] predict 3/10 CA_3 day 11\n",
      "[1913 - 14] predict 4/10 CA_4 day 11\n",
      "[1913 - 14] predict 5/10 TX_1 day 11\n",
      "[1913 - 14] predict 6/10 TX_2 day 11\n",
      "[1913 - 14] predict 7/10 TX_3 day 11\n",
      "[1913 - 14] predict 8/10 WI_1 day 11\n",
      "[1913 - 14] predict 9/10 WI_2 day 11\n",
      "[1913 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1913 - 14] predict 1/10 CA_1 day 12\n",
      "[1913 - 14] predict 2/10 CA_2 day 12\n",
      "[1913 - 14] predict 3/10 CA_3 day 12\n",
      "[1913 - 14] predict 4/10 CA_4 day 12\n",
      "[1913 - 14] predict 5/10 TX_1 day 12\n",
      "[1913 - 14] predict 6/10 TX_2 day 12\n",
      "[1913 - 14] predict 7/10 TX_3 day 12\n",
      "[1913 - 14] predict 8/10 WI_1 day 12\n",
      "[1913 - 14] predict 9/10 WI_2 day 12\n",
      "[1913 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1913 - 14] predict 1/10 CA_1 day 13\n",
      "[1913 - 14] predict 2/10 CA_2 day 13\n",
      "[1913 - 14] predict 3/10 CA_3 day 13\n",
      "[1913 - 14] predict 4/10 CA_4 day 13\n",
      "[1913 - 14] predict 5/10 TX_1 day 13\n",
      "[1913 - 14] predict 6/10 TX_2 day 13\n",
      "[1913 - 14] predict 7/10 TX_3 day 13\n",
      "[1913 - 14] predict 8/10 WI_1 day 13\n",
      "[1913 - 14] predict 9/10 WI_2 day 13\n",
      "[1913 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1913 - 14] predict 1/10 CA_1 day 14\n",
      "[1913 - 14] predict 2/10 CA_2 day 14\n",
      "[1913 - 14] predict 3/10 CA_3 day 14\n",
      "[1913 - 14] predict 4/10 CA_4 day 14\n",
      "[1913 - 14] predict 5/10 TX_1 day 14\n",
      "[1913 - 14] predict 6/10 TX_2 day 14\n",
      "[1913 - 14] predict 7/10 TX_3 day 14\n",
      "[1913 - 14] predict 8/10 WI_1 day 14\n",
      "[1913 - 14] predict 9/10 WI_2 day 14\n",
      "[1913 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1913 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 58327370\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (58967660, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.40706\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.05799\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.14053\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.85955\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.05132\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.5167\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.30717\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.51609\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.02\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1893-d_1913\n",
      "pred_mask horizon: d_1814-d_1934\n",
      "[1913 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.83341\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1913 - 21] predict 1/10 CA_1 day 15\n",
      "[1913 - 21] predict 2/10 CA_2 day 15\n",
      "[1913 - 21] predict 3/10 CA_3 day 15\n",
      "[1913 - 21] predict 4/10 CA_4 day 15\n",
      "[1913 - 21] predict 5/10 TX_1 day 15\n",
      "[1913 - 21] predict 6/10 TX_2 day 15\n",
      "[1913 - 21] predict 7/10 TX_3 day 15\n",
      "[1913 - 21] predict 8/10 WI_1 day 15\n",
      "[1913 - 21] predict 9/10 WI_2 day 15\n",
      "[1913 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1913 - 21] predict 1/10 CA_1 day 16\n",
      "[1913 - 21] predict 2/10 CA_2 day 16\n",
      "[1913 - 21] predict 3/10 CA_3 day 16\n",
      "[1913 - 21] predict 4/10 CA_4 day 16\n",
      "[1913 - 21] predict 5/10 TX_1 day 16\n",
      "[1913 - 21] predict 6/10 TX_2 day 16\n",
      "[1913 - 21] predict 7/10 TX_3 day 16\n",
      "[1913 - 21] predict 8/10 WI_1 day 16\n",
      "[1913 - 21] predict 9/10 WI_2 day 16\n",
      "[1913 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1913 - 21] predict 1/10 CA_1 day 17\n",
      "[1913 - 21] predict 2/10 CA_2 day 17\n",
      "[1913 - 21] predict 3/10 CA_3 day 17\n",
      "[1913 - 21] predict 4/10 CA_4 day 17\n",
      "[1913 - 21] predict 5/10 TX_1 day 17\n",
      "[1913 - 21] predict 6/10 TX_2 day 17\n",
      "[1913 - 21] predict 7/10 TX_3 day 17\n",
      "[1913 - 21] predict 8/10 WI_1 day 17\n",
      "[1913 - 21] predict 9/10 WI_2 day 17\n",
      "[1913 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1913 - 21] predict 1/10 CA_1 day 18\n",
      "[1913 - 21] predict 2/10 CA_2 day 18\n",
      "[1913 - 21] predict 3/10 CA_3 day 18\n",
      "[1913 - 21] predict 4/10 CA_4 day 18\n",
      "[1913 - 21] predict 5/10 TX_1 day 18\n",
      "[1913 - 21] predict 6/10 TX_2 day 18\n",
      "[1913 - 21] predict 7/10 TX_3 day 18\n",
      "[1913 - 21] predict 8/10 WI_1 day 18\n",
      "[1913 - 21] predict 9/10 WI_2 day 18\n",
      "[1913 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1913 - 21] predict 1/10 CA_1 day 19\n",
      "[1913 - 21] predict 2/10 CA_2 day 19\n",
      "[1913 - 21] predict 3/10 CA_3 day 19\n",
      "[1913 - 21] predict 4/10 CA_4 day 19\n",
      "[1913 - 21] predict 5/10 TX_1 day 19\n",
      "[1913 - 21] predict 6/10 TX_2 day 19\n",
      "[1913 - 21] predict 7/10 TX_3 day 19\n",
      "[1913 - 21] predict 8/10 WI_1 day 19\n",
      "[1913 - 21] predict 9/10 WI_2 day 19\n",
      "[1913 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1913 - 21] predict 1/10 CA_1 day 20\n",
      "[1913 - 21] predict 2/10 CA_2 day 20\n",
      "[1913 - 21] predict 3/10 CA_3 day 20\n",
      "[1913 - 21] predict 4/10 CA_4 day 20\n",
      "[1913 - 21] predict 5/10 TX_1 day 20\n",
      "[1913 - 21] predict 6/10 TX_2 day 20\n",
      "[1913 - 21] predict 7/10 TX_3 day 20\n",
      "[1913 - 21] predict 8/10 WI_1 day 20\n",
      "[1913 - 21] predict 9/10 WI_2 day 20\n",
      "[1913 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1913 - 21] predict 1/10 CA_1 day 21\n",
      "[1913 - 21] predict 2/10 CA_2 day 21\n",
      "[1913 - 21] predict 3/10 CA_3 day 21\n",
      "[1913 - 21] predict 4/10 CA_4 day 21\n",
      "[1913 - 21] predict 5/10 TX_1 day 21\n",
      "[1913 - 21] predict 6/10 TX_2 day 21\n",
      "[1913 - 21] predict 7/10 TX_3 day 21\n",
      "[1913 - 21] predict 8/10 WI_1 day 21\n",
      "[1913 - 21] predict 9/10 WI_2 day 21\n",
      "[1913 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1913 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 58327370\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (59181090, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.47989\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.09345\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.09666\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.87022\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.04277\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.53074\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.27724\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.51695\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.91396\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1913\n",
      "valid_mask horizon: d_1886-d_1913\n",
      "pred_mask horizon: d_1814-d_1941\n",
      "[1913 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.81248\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1913 - 28] predict 1/10 CA_1 day 22\n",
      "[1913 - 28] predict 2/10 CA_2 day 22\n",
      "[1913 - 28] predict 3/10 CA_3 day 22\n",
      "[1913 - 28] predict 4/10 CA_4 day 22\n",
      "[1913 - 28] predict 5/10 TX_1 day 22\n",
      "[1913 - 28] predict 6/10 TX_2 day 22\n",
      "[1913 - 28] predict 7/10 TX_3 day 22\n",
      "[1913 - 28] predict 8/10 WI_1 day 22\n",
      "[1913 - 28] predict 9/10 WI_2 day 22\n",
      "[1913 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1913 - 28] predict 1/10 CA_1 day 23\n",
      "[1913 - 28] predict 2/10 CA_2 day 23\n",
      "[1913 - 28] predict 3/10 CA_3 day 23\n",
      "[1913 - 28] predict 4/10 CA_4 day 23\n",
      "[1913 - 28] predict 5/10 TX_1 day 23\n",
      "[1913 - 28] predict 6/10 TX_2 day 23\n",
      "[1913 - 28] predict 7/10 TX_3 day 23\n",
      "[1913 - 28] predict 8/10 WI_1 day 23\n",
      "[1913 - 28] predict 9/10 WI_2 day 23\n",
      "[1913 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1913 - 28] predict 1/10 CA_1 day 24\n",
      "[1913 - 28] predict 2/10 CA_2 day 24\n",
      "[1913 - 28] predict 3/10 CA_3 day 24\n",
      "[1913 - 28] predict 4/10 CA_4 day 24\n",
      "[1913 - 28] predict 5/10 TX_1 day 24\n",
      "[1913 - 28] predict 6/10 TX_2 day 24\n",
      "[1913 - 28] predict 7/10 TX_3 day 24\n",
      "[1913 - 28] predict 8/10 WI_1 day 24\n",
      "[1913 - 28] predict 9/10 WI_2 day 24\n",
      "[1913 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1913 - 28] predict 1/10 CA_1 day 25\n",
      "[1913 - 28] predict 2/10 CA_2 day 25\n",
      "[1913 - 28] predict 3/10 CA_3 day 25\n",
      "[1913 - 28] predict 4/10 CA_4 day 25\n",
      "[1913 - 28] predict 5/10 TX_1 day 25\n",
      "[1913 - 28] predict 6/10 TX_2 day 25\n",
      "[1913 - 28] predict 7/10 TX_3 day 25\n",
      "[1913 - 28] predict 8/10 WI_1 day 25\n",
      "[1913 - 28] predict 9/10 WI_2 day 25\n",
      "[1913 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1913 - 28] predict 1/10 CA_1 day 26\n",
      "[1913 - 28] predict 2/10 CA_2 day 26\n",
      "[1913 - 28] predict 3/10 CA_3 day 26\n",
      "[1913 - 28] predict 4/10 CA_4 day 26\n",
      "[1913 - 28] predict 5/10 TX_1 day 26\n",
      "[1913 - 28] predict 6/10 TX_2 day 26\n",
      "[1913 - 28] predict 7/10 TX_3 day 26\n",
      "[1913 - 28] predict 8/10 WI_1 day 26\n",
      "[1913 - 28] predict 9/10 WI_2 day 26\n",
      "[1913 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1913 - 28] predict 1/10 CA_1 day 27\n",
      "[1913 - 28] predict 2/10 CA_2 day 27\n",
      "[1913 - 28] predict 3/10 CA_3 day 27\n",
      "[1913 - 28] predict 4/10 CA_4 day 27\n",
      "[1913 - 28] predict 5/10 TX_1 day 27\n",
      "[1913 - 28] predict 6/10 TX_2 day 27\n",
      "[1913 - 28] predict 7/10 TX_3 day 27\n",
      "[1913 - 28] predict 8/10 WI_1 day 27\n",
      "[1913 - 28] predict 9/10 WI_2 day 27\n",
      "[1913 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1913 - 28] predict 1/10 CA_1 day 28\n",
      "[1913 - 28] predict 2/10 CA_2 day 28\n",
      "[1913 - 28] predict 3/10 CA_3 day 28\n",
      "[1913 - 28] predict 4/10 CA_4 day 28\n",
      "[1913 - 28] predict 5/10 TX_1 day 28\n",
      "[1913 - 28] predict 6/10 TX_2 day 28\n",
      "[1913 - 28] predict 7/10 TX_3 day 28\n",
      "[1913 - 28] predict 8/10 WI_1 day 28\n",
      "[1913 - 28] predict 9/10 WI_2 day 28\n",
      "[1913 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (853720, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "calc metrics\n",
      "result_df.shape (853720, 3)\n",
      "############################# calc_wrmsse ###################################\n",
      "adjust end of train period\n",
      "(30490, 1947) --> (30490, 1947)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8686d97caa41a997c93928030248fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wrmsse 2.5256279621941418\n",
      "   fold_id metric_name  metric_value\n",
      "0     1913      wrmsse      2.525628\n",
      "1     1913        rmse      3.639888\n",
      "----------------- fold_id 1885 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 57473650\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (57687080, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.57749\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.4299\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.48898\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.01441\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.32524\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.0028\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.69024\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.75496\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.8678\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1879-d_1885\n",
      "pred_mask horizon: d_1786-d_1892\n",
      "[1885 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.72845\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1885 - 7] predict 1/10 CA_1 day 1\n",
      "[1885 - 7] predict 2/10 CA_2 day 1\n",
      "[1885 - 7] predict 3/10 CA_3 day 1\n",
      "[1885 - 7] predict 4/10 CA_4 day 1\n",
      "[1885 - 7] predict 5/10 TX_1 day 1\n",
      "[1885 - 7] predict 6/10 TX_2 day 1\n",
      "[1885 - 7] predict 7/10 TX_3 day 1\n",
      "[1885 - 7] predict 8/10 WI_1 day 1\n",
      "[1885 - 7] predict 9/10 WI_2 day 1\n",
      "[1885 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1885 - 7] predict 1/10 CA_1 day 2\n",
      "[1885 - 7] predict 2/10 CA_2 day 2\n",
      "[1885 - 7] predict 3/10 CA_3 day 2\n",
      "[1885 - 7] predict 4/10 CA_4 day 2\n",
      "[1885 - 7] predict 5/10 TX_1 day 2\n",
      "[1885 - 7] predict 6/10 TX_2 day 2\n",
      "[1885 - 7] predict 7/10 TX_3 day 2\n",
      "[1885 - 7] predict 8/10 WI_1 day 2\n",
      "[1885 - 7] predict 9/10 WI_2 day 2\n",
      "[1885 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1885 - 7] predict 1/10 CA_1 day 3\n",
      "[1885 - 7] predict 2/10 CA_2 day 3\n",
      "[1885 - 7] predict 3/10 CA_3 day 3\n",
      "[1885 - 7] predict 4/10 CA_4 day 3\n",
      "[1885 - 7] predict 5/10 TX_1 day 3\n",
      "[1885 - 7] predict 6/10 TX_2 day 3\n",
      "[1885 - 7] predict 7/10 TX_3 day 3\n",
      "[1885 - 7] predict 8/10 WI_1 day 3\n",
      "[1885 - 7] predict 9/10 WI_2 day 3\n",
      "[1885 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1885 - 7] predict 1/10 CA_1 day 4\n",
      "[1885 - 7] predict 2/10 CA_2 day 4\n",
      "[1885 - 7] predict 3/10 CA_3 day 4\n",
      "[1885 - 7] predict 4/10 CA_4 day 4\n",
      "[1885 - 7] predict 5/10 TX_1 day 4\n",
      "[1885 - 7] predict 6/10 TX_2 day 4\n",
      "[1885 - 7] predict 7/10 TX_3 day 4\n",
      "[1885 - 7] predict 8/10 WI_1 day 4\n",
      "[1885 - 7] predict 9/10 WI_2 day 4\n",
      "[1885 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1885 - 7] predict 1/10 CA_1 day 5\n",
      "[1885 - 7] predict 2/10 CA_2 day 5\n",
      "[1885 - 7] predict 3/10 CA_3 day 5\n",
      "[1885 - 7] predict 4/10 CA_4 day 5\n",
      "[1885 - 7] predict 5/10 TX_1 day 5\n",
      "[1885 - 7] predict 6/10 TX_2 day 5\n",
      "[1885 - 7] predict 7/10 TX_3 day 5\n",
      "[1885 - 7] predict 8/10 WI_1 day 5\n",
      "[1885 - 7] predict 9/10 WI_2 day 5\n",
      "[1885 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1885 - 7] predict 1/10 CA_1 day 6\n",
      "[1885 - 7] predict 2/10 CA_2 day 6\n",
      "[1885 - 7] predict 3/10 CA_3 day 6\n",
      "[1885 - 7] predict 4/10 CA_4 day 6\n",
      "[1885 - 7] predict 5/10 TX_1 day 6\n",
      "[1885 - 7] predict 6/10 TX_2 day 6\n",
      "[1885 - 7] predict 7/10 TX_3 day 6\n",
      "[1885 - 7] predict 8/10 WI_1 day 6\n",
      "[1885 - 7] predict 9/10 WI_2 day 6\n",
      "[1885 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1885 - 7] predict 1/10 CA_1 day 7\n",
      "[1885 - 7] predict 2/10 CA_2 day 7\n",
      "[1885 - 7] predict 3/10 CA_3 day 7\n",
      "[1885 - 7] predict 4/10 CA_4 day 7\n",
      "[1885 - 7] predict 5/10 TX_1 day 7\n",
      "[1885 - 7] predict 6/10 TX_2 day 7\n",
      "[1885 - 7] predict 7/10 TX_3 day 7\n",
      "[1885 - 7] predict 8/10 WI_1 day 7\n",
      "[1885 - 7] predict 9/10 WI_2 day 7\n",
      "[1885 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1885 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 57473650\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (57900510, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.51309\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.96274\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.21059\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.94461\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.12459\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.9984\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.62173\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.64955\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.27313\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1872-d_1885\n",
      "pred_mask horizon: d_1786-d_1899\n",
      "[1885 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.81114\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1885 - 14] predict 1/10 CA_1 day 8\n",
      "[1885 - 14] predict 2/10 CA_2 day 8\n",
      "[1885 - 14] predict 3/10 CA_3 day 8\n",
      "[1885 - 14] predict 4/10 CA_4 day 8\n",
      "[1885 - 14] predict 5/10 TX_1 day 8\n",
      "[1885 - 14] predict 6/10 TX_2 day 8\n",
      "[1885 - 14] predict 7/10 TX_3 day 8\n",
      "[1885 - 14] predict 8/10 WI_1 day 8\n",
      "[1885 - 14] predict 9/10 WI_2 day 8\n",
      "[1885 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1885 - 14] predict 1/10 CA_1 day 9\n",
      "[1885 - 14] predict 2/10 CA_2 day 9\n",
      "[1885 - 14] predict 3/10 CA_3 day 9\n",
      "[1885 - 14] predict 4/10 CA_4 day 9\n",
      "[1885 - 14] predict 5/10 TX_1 day 9\n",
      "[1885 - 14] predict 6/10 TX_2 day 9\n",
      "[1885 - 14] predict 7/10 TX_3 day 9\n",
      "[1885 - 14] predict 8/10 WI_1 day 9\n",
      "[1885 - 14] predict 9/10 WI_2 day 9\n",
      "[1885 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1885 - 14] predict 1/10 CA_1 day 10\n",
      "[1885 - 14] predict 2/10 CA_2 day 10\n",
      "[1885 - 14] predict 3/10 CA_3 day 10\n",
      "[1885 - 14] predict 4/10 CA_4 day 10\n",
      "[1885 - 14] predict 5/10 TX_1 day 10\n",
      "[1885 - 14] predict 6/10 TX_2 day 10\n",
      "[1885 - 14] predict 7/10 TX_3 day 10\n",
      "[1885 - 14] predict 8/10 WI_1 day 10\n",
      "[1885 - 14] predict 9/10 WI_2 day 10\n",
      "[1885 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1885 - 14] predict 1/10 CA_1 day 11\n",
      "[1885 - 14] predict 2/10 CA_2 day 11\n",
      "[1885 - 14] predict 3/10 CA_3 day 11\n",
      "[1885 - 14] predict 4/10 CA_4 day 11\n",
      "[1885 - 14] predict 5/10 TX_1 day 11\n",
      "[1885 - 14] predict 6/10 TX_2 day 11\n",
      "[1885 - 14] predict 7/10 TX_3 day 11\n",
      "[1885 - 14] predict 8/10 WI_1 day 11\n",
      "[1885 - 14] predict 9/10 WI_2 day 11\n",
      "[1885 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1885 - 14] predict 1/10 CA_1 day 12\n",
      "[1885 - 14] predict 2/10 CA_2 day 12\n",
      "[1885 - 14] predict 3/10 CA_3 day 12\n",
      "[1885 - 14] predict 4/10 CA_4 day 12\n",
      "[1885 - 14] predict 5/10 TX_1 day 12\n",
      "[1885 - 14] predict 6/10 TX_2 day 12\n",
      "[1885 - 14] predict 7/10 TX_3 day 12\n",
      "[1885 - 14] predict 8/10 WI_1 day 12\n",
      "[1885 - 14] predict 9/10 WI_2 day 12\n",
      "[1885 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1885 - 14] predict 1/10 CA_1 day 13\n",
      "[1885 - 14] predict 2/10 CA_2 day 13\n",
      "[1885 - 14] predict 3/10 CA_3 day 13\n",
      "[1885 - 14] predict 4/10 CA_4 day 13\n",
      "[1885 - 14] predict 5/10 TX_1 day 13\n",
      "[1885 - 14] predict 6/10 TX_2 day 13\n",
      "[1885 - 14] predict 7/10 TX_3 day 13\n",
      "[1885 - 14] predict 8/10 WI_1 day 13\n",
      "[1885 - 14] predict 9/10 WI_2 day 13\n",
      "[1885 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1885 - 14] predict 1/10 CA_1 day 14\n",
      "[1885 - 14] predict 2/10 CA_2 day 14\n",
      "[1885 - 14] predict 3/10 CA_3 day 14\n",
      "[1885 - 14] predict 4/10 CA_4 day 14\n",
      "[1885 - 14] predict 5/10 TX_1 day 14\n",
      "[1885 - 14] predict 6/10 TX_2 day 14\n",
      "[1885 - 14] predict 7/10 TX_3 day 14\n",
      "[1885 - 14] predict 8/10 WI_1 day 14\n",
      "[1885 - 14] predict 9/10 WI_2 day 14\n",
      "[1885 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1885 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 57473650\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (58113940, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.45424\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.03744\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.21053\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.89501\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.06157\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.93705\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.56988\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.61003\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.51902\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1865-d_1885\n",
      "pred_mask horizon: d_1786-d_1906\n",
      "[1885 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.80654\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1885 - 21] predict 1/10 CA_1 day 15\n",
      "[1885 - 21] predict 2/10 CA_2 day 15\n",
      "[1885 - 21] predict 3/10 CA_3 day 15\n",
      "[1885 - 21] predict 4/10 CA_4 day 15\n",
      "[1885 - 21] predict 5/10 TX_1 day 15\n",
      "[1885 - 21] predict 6/10 TX_2 day 15\n",
      "[1885 - 21] predict 7/10 TX_3 day 15\n",
      "[1885 - 21] predict 8/10 WI_1 day 15\n",
      "[1885 - 21] predict 9/10 WI_2 day 15\n",
      "[1885 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1885 - 21] predict 1/10 CA_1 day 16\n",
      "[1885 - 21] predict 2/10 CA_2 day 16\n",
      "[1885 - 21] predict 3/10 CA_3 day 16\n",
      "[1885 - 21] predict 4/10 CA_4 day 16\n",
      "[1885 - 21] predict 5/10 TX_1 day 16\n",
      "[1885 - 21] predict 6/10 TX_2 day 16\n",
      "[1885 - 21] predict 7/10 TX_3 day 16\n",
      "[1885 - 21] predict 8/10 WI_1 day 16\n",
      "[1885 - 21] predict 9/10 WI_2 day 16\n",
      "[1885 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1885 - 21] predict 1/10 CA_1 day 17\n",
      "[1885 - 21] predict 2/10 CA_2 day 17\n",
      "[1885 - 21] predict 3/10 CA_3 day 17\n",
      "[1885 - 21] predict 4/10 CA_4 day 17\n",
      "[1885 - 21] predict 5/10 TX_1 day 17\n",
      "[1885 - 21] predict 6/10 TX_2 day 17\n",
      "[1885 - 21] predict 7/10 TX_3 day 17\n",
      "[1885 - 21] predict 8/10 WI_1 day 17\n",
      "[1885 - 21] predict 9/10 WI_2 day 17\n",
      "[1885 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1885 - 21] predict 1/10 CA_1 day 18\n",
      "[1885 - 21] predict 2/10 CA_2 day 18\n",
      "[1885 - 21] predict 3/10 CA_3 day 18\n",
      "[1885 - 21] predict 4/10 CA_4 day 18\n",
      "[1885 - 21] predict 5/10 TX_1 day 18\n",
      "[1885 - 21] predict 6/10 TX_2 day 18\n",
      "[1885 - 21] predict 7/10 TX_3 day 18\n",
      "[1885 - 21] predict 8/10 WI_1 day 18\n",
      "[1885 - 21] predict 9/10 WI_2 day 18\n",
      "[1885 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1885 - 21] predict 1/10 CA_1 day 19\n",
      "[1885 - 21] predict 2/10 CA_2 day 19\n",
      "[1885 - 21] predict 3/10 CA_3 day 19\n",
      "[1885 - 21] predict 4/10 CA_4 day 19\n",
      "[1885 - 21] predict 5/10 TX_1 day 19\n",
      "[1885 - 21] predict 6/10 TX_2 day 19\n",
      "[1885 - 21] predict 7/10 TX_3 day 19\n",
      "[1885 - 21] predict 8/10 WI_1 day 19\n",
      "[1885 - 21] predict 9/10 WI_2 day 19\n",
      "[1885 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1885 - 21] predict 1/10 CA_1 day 20\n",
      "[1885 - 21] predict 2/10 CA_2 day 20\n",
      "[1885 - 21] predict 3/10 CA_3 day 20\n",
      "[1885 - 21] predict 4/10 CA_4 day 20\n",
      "[1885 - 21] predict 5/10 TX_1 day 20\n",
      "[1885 - 21] predict 6/10 TX_2 day 20\n",
      "[1885 - 21] predict 7/10 TX_3 day 20\n",
      "[1885 - 21] predict 8/10 WI_1 day 20\n",
      "[1885 - 21] predict 9/10 WI_2 day 20\n",
      "[1885 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1885 - 21] predict 1/10 CA_1 day 21\n",
      "[1885 - 21] predict 2/10 CA_2 day 21\n",
      "[1885 - 21] predict 3/10 CA_3 day 21\n",
      "[1885 - 21] predict 4/10 CA_4 day 21\n",
      "[1885 - 21] predict 5/10 TX_1 day 21\n",
      "[1885 - 21] predict 6/10 TX_2 day 21\n",
      "[1885 - 21] predict 7/10 TX_3 day 21\n",
      "[1885 - 21] predict 8/10 WI_1 day 21\n",
      "[1885 - 21] predict 9/10 WI_2 day 21\n",
      "[1885 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1885 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 57473650\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (58327370, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.43392\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.01535\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.20655\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.92255\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.01686\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.90241\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.52858\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.61653\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.71524\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1885\n",
      "valid_mask horizon: d_1858-d_1885\n",
      "pred_mask horizon: d_1786-d_1913\n",
      "[1885 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.87547\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1885 - 28] predict 1/10 CA_1 day 22\n",
      "[1885 - 28] predict 2/10 CA_2 day 22\n",
      "[1885 - 28] predict 3/10 CA_3 day 22\n",
      "[1885 - 28] predict 4/10 CA_4 day 22\n",
      "[1885 - 28] predict 5/10 TX_1 day 22\n",
      "[1885 - 28] predict 6/10 TX_2 day 22\n",
      "[1885 - 28] predict 7/10 TX_3 day 22\n",
      "[1885 - 28] predict 8/10 WI_1 day 22\n",
      "[1885 - 28] predict 9/10 WI_2 day 22\n",
      "[1885 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1885 - 28] predict 1/10 CA_1 day 23\n",
      "[1885 - 28] predict 2/10 CA_2 day 23\n",
      "[1885 - 28] predict 3/10 CA_3 day 23\n",
      "[1885 - 28] predict 4/10 CA_4 day 23\n",
      "[1885 - 28] predict 5/10 TX_1 day 23\n",
      "[1885 - 28] predict 6/10 TX_2 day 23\n",
      "[1885 - 28] predict 7/10 TX_3 day 23\n",
      "[1885 - 28] predict 8/10 WI_1 day 23\n",
      "[1885 - 28] predict 9/10 WI_2 day 23\n",
      "[1885 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1885 - 28] predict 1/10 CA_1 day 24\n",
      "[1885 - 28] predict 2/10 CA_2 day 24\n",
      "[1885 - 28] predict 3/10 CA_3 day 24\n",
      "[1885 - 28] predict 4/10 CA_4 day 24\n",
      "[1885 - 28] predict 5/10 TX_1 day 24\n",
      "[1885 - 28] predict 6/10 TX_2 day 24\n",
      "[1885 - 28] predict 7/10 TX_3 day 24\n",
      "[1885 - 28] predict 8/10 WI_1 day 24\n",
      "[1885 - 28] predict 9/10 WI_2 day 24\n",
      "[1885 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1885 - 28] predict 1/10 CA_1 day 25\n",
      "[1885 - 28] predict 2/10 CA_2 day 25\n",
      "[1885 - 28] predict 3/10 CA_3 day 25\n",
      "[1885 - 28] predict 4/10 CA_4 day 25\n",
      "[1885 - 28] predict 5/10 TX_1 day 25\n",
      "[1885 - 28] predict 6/10 TX_2 day 25\n",
      "[1885 - 28] predict 7/10 TX_3 day 25\n",
      "[1885 - 28] predict 8/10 WI_1 day 25\n",
      "[1885 - 28] predict 9/10 WI_2 day 25\n",
      "[1885 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1885 - 28] predict 1/10 CA_1 day 26\n",
      "[1885 - 28] predict 2/10 CA_2 day 26\n",
      "[1885 - 28] predict 3/10 CA_3 day 26\n",
      "[1885 - 28] predict 4/10 CA_4 day 26\n",
      "[1885 - 28] predict 5/10 TX_1 day 26\n",
      "[1885 - 28] predict 6/10 TX_2 day 26\n",
      "[1885 - 28] predict 7/10 TX_3 day 26\n",
      "[1885 - 28] predict 8/10 WI_1 day 26\n",
      "[1885 - 28] predict 9/10 WI_2 day 26\n",
      "[1885 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1885 - 28] predict 1/10 CA_1 day 27\n",
      "[1885 - 28] predict 2/10 CA_2 day 27\n",
      "[1885 - 28] predict 3/10 CA_3 day 27\n",
      "[1885 - 28] predict 4/10 CA_4 day 27\n",
      "[1885 - 28] predict 5/10 TX_1 day 27\n",
      "[1885 - 28] predict 6/10 TX_2 day 27\n",
      "[1885 - 28] predict 7/10 TX_3 day 27\n",
      "[1885 - 28] predict 8/10 WI_1 day 27\n",
      "[1885 - 28] predict 9/10 WI_2 day 27\n",
      "[1885 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1885 - 28] predict 1/10 CA_1 day 28\n",
      "[1885 - 28] predict 2/10 CA_2 day 28\n",
      "[1885 - 28] predict 3/10 CA_3 day 28\n",
      "[1885 - 28] predict 4/10 CA_4 day 28\n",
      "[1885 - 28] predict 5/10 TX_1 day 28\n",
      "[1885 - 28] predict 6/10 TX_2 day 28\n",
      "[1885 - 28] predict 7/10 TX_3 day 28\n",
      "[1885 - 28] predict 8/10 WI_1 day 28\n",
      "[1885 - 28] predict 9/10 WI_2 day 28\n",
      "[1885 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (853720, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "calc metrics\n",
      "result_df.shape (853720, 3)\n",
      "############################# calc_wrmsse ###################################\n",
      "adjust end of train period\n",
      "(30490, 1947) --> (30490, 1919)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777a2aeeb9a64ae1992394d78a48bff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wrmsse 2.426944112530406\n",
      "   fold_id metric_name  metric_value\n",
      "0     1885      wrmsse      2.426944\n",
      "1     1885        rmse      3.586252\n",
      "----------------- fold_id 1857 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 56619930\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (56833360, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.37412\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.06015\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.05103\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.83138\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.6999\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.71619\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.3346\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.46934\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.94524\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1851-d_1857\n",
      "pred_mask horizon: d_1758-d_1864\n",
      "[1857 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.45459\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1857 - 7] predict 1/10 CA_1 day 1\n",
      "[1857 - 7] predict 2/10 CA_2 day 1\n",
      "[1857 - 7] predict 3/10 CA_3 day 1\n",
      "[1857 - 7] predict 4/10 CA_4 day 1\n",
      "[1857 - 7] predict 5/10 TX_1 day 1\n",
      "[1857 - 7] predict 6/10 TX_2 day 1\n",
      "[1857 - 7] predict 7/10 TX_3 day 1\n",
      "[1857 - 7] predict 8/10 WI_1 day 1\n",
      "[1857 - 7] predict 9/10 WI_2 day 1\n",
      "[1857 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1857 - 7] predict 1/10 CA_1 day 2\n",
      "[1857 - 7] predict 2/10 CA_2 day 2\n",
      "[1857 - 7] predict 3/10 CA_3 day 2\n",
      "[1857 - 7] predict 4/10 CA_4 day 2\n",
      "[1857 - 7] predict 5/10 TX_1 day 2\n",
      "[1857 - 7] predict 6/10 TX_2 day 2\n",
      "[1857 - 7] predict 7/10 TX_3 day 2\n",
      "[1857 - 7] predict 8/10 WI_1 day 2\n",
      "[1857 - 7] predict 9/10 WI_2 day 2\n",
      "[1857 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1857 - 7] predict 1/10 CA_1 day 3\n",
      "[1857 - 7] predict 2/10 CA_2 day 3\n",
      "[1857 - 7] predict 3/10 CA_3 day 3\n",
      "[1857 - 7] predict 4/10 CA_4 day 3\n",
      "[1857 - 7] predict 5/10 TX_1 day 3\n",
      "[1857 - 7] predict 6/10 TX_2 day 3\n",
      "[1857 - 7] predict 7/10 TX_3 day 3\n",
      "[1857 - 7] predict 8/10 WI_1 day 3\n",
      "[1857 - 7] predict 9/10 WI_2 day 3\n",
      "[1857 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1857 - 7] predict 1/10 CA_1 day 4\n",
      "[1857 - 7] predict 2/10 CA_2 day 4\n",
      "[1857 - 7] predict 3/10 CA_3 day 4\n",
      "[1857 - 7] predict 4/10 CA_4 day 4\n",
      "[1857 - 7] predict 5/10 TX_1 day 4\n",
      "[1857 - 7] predict 6/10 TX_2 day 4\n",
      "[1857 - 7] predict 7/10 TX_3 day 4\n",
      "[1857 - 7] predict 8/10 WI_1 day 4\n",
      "[1857 - 7] predict 9/10 WI_2 day 4\n",
      "[1857 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1857 - 7] predict 1/10 CA_1 day 5\n",
      "[1857 - 7] predict 2/10 CA_2 day 5\n",
      "[1857 - 7] predict 3/10 CA_3 day 5\n",
      "[1857 - 7] predict 4/10 CA_4 day 5\n",
      "[1857 - 7] predict 5/10 TX_1 day 5\n",
      "[1857 - 7] predict 6/10 TX_2 day 5\n",
      "[1857 - 7] predict 7/10 TX_3 day 5\n",
      "[1857 - 7] predict 8/10 WI_1 day 5\n",
      "[1857 - 7] predict 9/10 WI_2 day 5\n",
      "[1857 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1857 - 7] predict 1/10 CA_1 day 6\n",
      "[1857 - 7] predict 2/10 CA_2 day 6\n",
      "[1857 - 7] predict 3/10 CA_3 day 6\n",
      "[1857 - 7] predict 4/10 CA_4 day 6\n",
      "[1857 - 7] predict 5/10 TX_1 day 6\n",
      "[1857 - 7] predict 6/10 TX_2 day 6\n",
      "[1857 - 7] predict 7/10 TX_3 day 6\n",
      "[1857 - 7] predict 8/10 WI_1 day 6\n",
      "[1857 - 7] predict 9/10 WI_2 day 6\n",
      "[1857 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1857 - 7] predict 1/10 CA_1 day 7\n",
      "[1857 - 7] predict 2/10 CA_2 day 7\n",
      "[1857 - 7] predict 3/10 CA_3 day 7\n",
      "[1857 - 7] predict 4/10 CA_4 day 7\n",
      "[1857 - 7] predict 5/10 TX_1 day 7\n",
      "[1857 - 7] predict 6/10 TX_2 day 7\n",
      "[1857 - 7] predict 7/10 TX_3 day 7\n",
      "[1857 - 7] predict 8/10 WI_1 day 7\n",
      "[1857 - 7] predict 9/10 WI_2 day 7\n",
      "[1857 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1857 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 56619930\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (57046790, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.48826\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.06721\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.09462\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.82885\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.90707\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.94483\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.54651\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.56287\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.63196\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1844-d_1857\n",
      "pred_mask horizon: d_1758-d_1871\n",
      "[1857 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.79092\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1857 - 14] predict 1/10 CA_1 day 8\n",
      "[1857 - 14] predict 2/10 CA_2 day 8\n",
      "[1857 - 14] predict 3/10 CA_3 day 8\n",
      "[1857 - 14] predict 4/10 CA_4 day 8\n",
      "[1857 - 14] predict 5/10 TX_1 day 8\n",
      "[1857 - 14] predict 6/10 TX_2 day 8\n",
      "[1857 - 14] predict 7/10 TX_3 day 8\n",
      "[1857 - 14] predict 8/10 WI_1 day 8\n",
      "[1857 - 14] predict 9/10 WI_2 day 8\n",
      "[1857 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1857 - 14] predict 1/10 CA_1 day 9\n",
      "[1857 - 14] predict 2/10 CA_2 day 9\n",
      "[1857 - 14] predict 3/10 CA_3 day 9\n",
      "[1857 - 14] predict 4/10 CA_4 day 9\n",
      "[1857 - 14] predict 5/10 TX_1 day 9\n",
      "[1857 - 14] predict 6/10 TX_2 day 9\n",
      "[1857 - 14] predict 7/10 TX_3 day 9\n",
      "[1857 - 14] predict 8/10 WI_1 day 9\n",
      "[1857 - 14] predict 9/10 WI_2 day 9\n",
      "[1857 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1857 - 14] predict 1/10 CA_1 day 10\n",
      "[1857 - 14] predict 2/10 CA_2 day 10\n",
      "[1857 - 14] predict 3/10 CA_3 day 10\n",
      "[1857 - 14] predict 4/10 CA_4 day 10\n",
      "[1857 - 14] predict 5/10 TX_1 day 10\n",
      "[1857 - 14] predict 6/10 TX_2 day 10\n",
      "[1857 - 14] predict 7/10 TX_3 day 10\n",
      "[1857 - 14] predict 8/10 WI_1 day 10\n",
      "[1857 - 14] predict 9/10 WI_2 day 10\n",
      "[1857 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1857 - 14] predict 1/10 CA_1 day 11\n",
      "[1857 - 14] predict 2/10 CA_2 day 11\n",
      "[1857 - 14] predict 3/10 CA_3 day 11\n",
      "[1857 - 14] predict 4/10 CA_4 day 11\n",
      "[1857 - 14] predict 5/10 TX_1 day 11\n",
      "[1857 - 14] predict 6/10 TX_2 day 11\n",
      "[1857 - 14] predict 7/10 TX_3 day 11\n",
      "[1857 - 14] predict 8/10 WI_1 day 11\n",
      "[1857 - 14] predict 9/10 WI_2 day 11\n",
      "[1857 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1857 - 14] predict 1/10 CA_1 day 12\n",
      "[1857 - 14] predict 2/10 CA_2 day 12\n",
      "[1857 - 14] predict 3/10 CA_3 day 12\n",
      "[1857 - 14] predict 4/10 CA_4 day 12\n",
      "[1857 - 14] predict 5/10 TX_1 day 12\n",
      "[1857 - 14] predict 6/10 TX_2 day 12\n",
      "[1857 - 14] predict 7/10 TX_3 day 12\n",
      "[1857 - 14] predict 8/10 WI_1 day 12\n",
      "[1857 - 14] predict 9/10 WI_2 day 12\n",
      "[1857 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1857 - 14] predict 1/10 CA_1 day 13\n",
      "[1857 - 14] predict 2/10 CA_2 day 13\n",
      "[1857 - 14] predict 3/10 CA_3 day 13\n",
      "[1857 - 14] predict 4/10 CA_4 day 13\n",
      "[1857 - 14] predict 5/10 TX_1 day 13\n",
      "[1857 - 14] predict 6/10 TX_2 day 13\n",
      "[1857 - 14] predict 7/10 TX_3 day 13\n",
      "[1857 - 14] predict 8/10 WI_1 day 13\n",
      "[1857 - 14] predict 9/10 WI_2 day 13\n",
      "[1857 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1857 - 14] predict 1/10 CA_1 day 14\n",
      "[1857 - 14] predict 2/10 CA_2 day 14\n",
      "[1857 - 14] predict 3/10 CA_3 day 14\n",
      "[1857 - 14] predict 4/10 CA_4 day 14\n",
      "[1857 - 14] predict 5/10 TX_1 day 14\n",
      "[1857 - 14] predict 6/10 TX_2 day 14\n",
      "[1857 - 14] predict 7/10 TX_3 day 14\n",
      "[1857 - 14] predict 8/10 WI_1 day 14\n",
      "[1857 - 14] predict 9/10 WI_2 day 14\n",
      "[1857 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1857 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 56619930\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.43979\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.06395\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.13751\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.86839\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.95703\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.06216\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.66827\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.58604\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.78696\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1837-d_1857\n",
      "pred_mask horizon: d_1758-d_1878\n",
      "[1857 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.03696\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1857 - 21] predict 1/10 CA_1 day 15\n",
      "[1857 - 21] predict 2/10 CA_2 day 15\n",
      "[1857 - 21] predict 3/10 CA_3 day 15\n",
      "[1857 - 21] predict 4/10 CA_4 day 15\n",
      "[1857 - 21] predict 5/10 TX_1 day 15\n",
      "[1857 - 21] predict 6/10 TX_2 day 15\n",
      "[1857 - 21] predict 7/10 TX_3 day 15\n",
      "[1857 - 21] predict 8/10 WI_1 day 15\n",
      "[1857 - 21] predict 9/10 WI_2 day 15\n",
      "[1857 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1857 - 21] predict 1/10 CA_1 day 16\n",
      "[1857 - 21] predict 2/10 CA_2 day 16\n",
      "[1857 - 21] predict 3/10 CA_3 day 16\n",
      "[1857 - 21] predict 4/10 CA_4 day 16\n",
      "[1857 - 21] predict 5/10 TX_1 day 16\n",
      "[1857 - 21] predict 6/10 TX_2 day 16\n",
      "[1857 - 21] predict 7/10 TX_3 day 16\n",
      "[1857 - 21] predict 8/10 WI_1 day 16\n",
      "[1857 - 21] predict 9/10 WI_2 day 16\n",
      "[1857 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1857 - 21] predict 1/10 CA_1 day 17\n",
      "[1857 - 21] predict 2/10 CA_2 day 17\n",
      "[1857 - 21] predict 3/10 CA_3 day 17\n",
      "[1857 - 21] predict 4/10 CA_4 day 17\n",
      "[1857 - 21] predict 5/10 TX_1 day 17\n",
      "[1857 - 21] predict 6/10 TX_2 day 17\n",
      "[1857 - 21] predict 7/10 TX_3 day 17\n",
      "[1857 - 21] predict 8/10 WI_1 day 17\n",
      "[1857 - 21] predict 9/10 WI_2 day 17\n",
      "[1857 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1857 - 21] predict 1/10 CA_1 day 18\n",
      "[1857 - 21] predict 2/10 CA_2 day 18\n",
      "[1857 - 21] predict 3/10 CA_3 day 18\n",
      "[1857 - 21] predict 4/10 CA_4 day 18\n",
      "[1857 - 21] predict 5/10 TX_1 day 18\n",
      "[1857 - 21] predict 6/10 TX_2 day 18\n",
      "[1857 - 21] predict 7/10 TX_3 day 18\n",
      "[1857 - 21] predict 8/10 WI_1 day 18\n",
      "[1857 - 21] predict 9/10 WI_2 day 18\n",
      "[1857 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1857 - 21] predict 1/10 CA_1 day 19\n",
      "[1857 - 21] predict 2/10 CA_2 day 19\n",
      "[1857 - 21] predict 3/10 CA_3 day 19\n",
      "[1857 - 21] predict 4/10 CA_4 day 19\n",
      "[1857 - 21] predict 5/10 TX_1 day 19\n",
      "[1857 - 21] predict 6/10 TX_2 day 19\n",
      "[1857 - 21] predict 7/10 TX_3 day 19\n",
      "[1857 - 21] predict 8/10 WI_1 day 19\n",
      "[1857 - 21] predict 9/10 WI_2 day 19\n",
      "[1857 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1857 - 21] predict 1/10 CA_1 day 20\n",
      "[1857 - 21] predict 2/10 CA_2 day 20\n",
      "[1857 - 21] predict 3/10 CA_3 day 20\n",
      "[1857 - 21] predict 4/10 CA_4 day 20\n",
      "[1857 - 21] predict 5/10 TX_1 day 20\n",
      "[1857 - 21] predict 6/10 TX_2 day 20\n",
      "[1857 - 21] predict 7/10 TX_3 day 20\n",
      "[1857 - 21] predict 8/10 WI_1 day 20\n",
      "[1857 - 21] predict 9/10 WI_2 day 20\n",
      "[1857 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1857 - 21] predict 1/10 CA_1 day 21\n",
      "[1857 - 21] predict 2/10 CA_2 day 21\n",
      "[1857 - 21] predict 3/10 CA_3 day 21\n",
      "[1857 - 21] predict 4/10 CA_4 day 21\n",
      "[1857 - 21] predict 5/10 TX_1 day 21\n",
      "[1857 - 21] predict 6/10 TX_2 day 21\n",
      "[1857 - 21] predict 7/10 TX_3 day 21\n",
      "[1857 - 21] predict 8/10 WI_1 day 21\n",
      "[1857 - 21] predict 9/10 WI_2 day 21\n",
      "[1857 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1857 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 56619930\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (57473650, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.48105\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.07543\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.16383\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.89531\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.96028\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.01315\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.68336\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.56716\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.87584\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1857\n",
      "valid_mask horizon: d_1830-d_1857\n",
      "pred_mask horizon: d_1758-d_1885\n",
      "[1857 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.02127\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1857 - 28] predict 1/10 CA_1 day 22\n",
      "[1857 - 28] predict 2/10 CA_2 day 22\n",
      "[1857 - 28] predict 3/10 CA_3 day 22\n",
      "[1857 - 28] predict 4/10 CA_4 day 22\n",
      "[1857 - 28] predict 5/10 TX_1 day 22\n",
      "[1857 - 28] predict 6/10 TX_2 day 22\n",
      "[1857 - 28] predict 7/10 TX_3 day 22\n",
      "[1857 - 28] predict 8/10 WI_1 day 22\n",
      "[1857 - 28] predict 9/10 WI_2 day 22\n",
      "[1857 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1857 - 28] predict 1/10 CA_1 day 23\n",
      "[1857 - 28] predict 2/10 CA_2 day 23\n",
      "[1857 - 28] predict 3/10 CA_3 day 23\n",
      "[1857 - 28] predict 4/10 CA_4 day 23\n",
      "[1857 - 28] predict 5/10 TX_1 day 23\n",
      "[1857 - 28] predict 6/10 TX_2 day 23\n",
      "[1857 - 28] predict 7/10 TX_3 day 23\n",
      "[1857 - 28] predict 8/10 WI_1 day 23\n",
      "[1857 - 28] predict 9/10 WI_2 day 23\n",
      "[1857 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1857 - 28] predict 1/10 CA_1 day 24\n",
      "[1857 - 28] predict 2/10 CA_2 day 24\n",
      "[1857 - 28] predict 3/10 CA_3 day 24\n",
      "[1857 - 28] predict 4/10 CA_4 day 24\n",
      "[1857 - 28] predict 5/10 TX_1 day 24\n",
      "[1857 - 28] predict 6/10 TX_2 day 24\n",
      "[1857 - 28] predict 7/10 TX_3 day 24\n",
      "[1857 - 28] predict 8/10 WI_1 day 24\n",
      "[1857 - 28] predict 9/10 WI_2 day 24\n",
      "[1857 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1857 - 28] predict 1/10 CA_1 day 25\n",
      "[1857 - 28] predict 2/10 CA_2 day 25\n",
      "[1857 - 28] predict 3/10 CA_3 day 25\n",
      "[1857 - 28] predict 4/10 CA_4 day 25\n",
      "[1857 - 28] predict 5/10 TX_1 day 25\n",
      "[1857 - 28] predict 6/10 TX_2 day 25\n",
      "[1857 - 28] predict 7/10 TX_3 day 25\n",
      "[1857 - 28] predict 8/10 WI_1 day 25\n",
      "[1857 - 28] predict 9/10 WI_2 day 25\n",
      "[1857 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1857 - 28] predict 1/10 CA_1 day 26\n",
      "[1857 - 28] predict 2/10 CA_2 day 26\n",
      "[1857 - 28] predict 3/10 CA_3 day 26\n",
      "[1857 - 28] predict 4/10 CA_4 day 26\n",
      "[1857 - 28] predict 5/10 TX_1 day 26\n",
      "[1857 - 28] predict 6/10 TX_2 day 26\n",
      "[1857 - 28] predict 7/10 TX_3 day 26\n",
      "[1857 - 28] predict 8/10 WI_1 day 26\n",
      "[1857 - 28] predict 9/10 WI_2 day 26\n",
      "[1857 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1857 - 28] predict 1/10 CA_1 day 27\n",
      "[1857 - 28] predict 2/10 CA_2 day 27\n",
      "[1857 - 28] predict 3/10 CA_3 day 27\n",
      "[1857 - 28] predict 4/10 CA_4 day 27\n",
      "[1857 - 28] predict 5/10 TX_1 day 27\n",
      "[1857 - 28] predict 6/10 TX_2 day 27\n",
      "[1857 - 28] predict 7/10 TX_3 day 27\n",
      "[1857 - 28] predict 8/10 WI_1 day 27\n",
      "[1857 - 28] predict 9/10 WI_2 day 27\n",
      "[1857 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1857 - 28] predict 1/10 CA_1 day 28\n",
      "[1857 - 28] predict 2/10 CA_2 day 28\n",
      "[1857 - 28] predict 3/10 CA_3 day 28\n",
      "[1857 - 28] predict 4/10 CA_4 day 28\n",
      "[1857 - 28] predict 5/10 TX_1 day 28\n",
      "[1857 - 28] predict 6/10 TX_2 day 28\n",
      "[1857 - 28] predict 7/10 TX_3 day 28\n",
      "[1857 - 28] predict 8/10 WI_1 day 28\n",
      "[1857 - 28] predict 9/10 WI_2 day 28\n",
      "[1857 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (853720, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "calc metrics\n",
      "result_df.shape (853720, 3)\n",
      "############################# calc_wrmsse ###################################\n",
      "adjust end of train period\n",
      "(30490, 1947) --> (30490, 1891)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ed83a374dd4c2985b62634236fc0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wrmsse 2.410408997233953\n",
      "   fold_id metric_name  metric_value\n",
      "0     1857      wrmsse      2.410409\n",
      "1     1857        rmse      3.640562\n",
      "----------------- fold_id 1829 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 55766210\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (55979640, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.12539\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.93222\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.98971\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.92986\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.57092\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.289\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.27097\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.50849\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.69948\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1823-d_1829\n",
      "pred_mask horizon: d_1730-d_1836\n",
      "[1829 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.23128\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1829 - 7] predict 1/10 CA_1 day 1\n",
      "[1829 - 7] predict 2/10 CA_2 day 1\n",
      "[1829 - 7] predict 3/10 CA_3 day 1\n",
      "[1829 - 7] predict 4/10 CA_4 day 1\n",
      "[1829 - 7] predict 5/10 TX_1 day 1\n",
      "[1829 - 7] predict 6/10 TX_2 day 1\n",
      "[1829 - 7] predict 7/10 TX_3 day 1\n",
      "[1829 - 7] predict 8/10 WI_1 day 1\n",
      "[1829 - 7] predict 9/10 WI_2 day 1\n",
      "[1829 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1829 - 7] predict 1/10 CA_1 day 2\n",
      "[1829 - 7] predict 2/10 CA_2 day 2\n",
      "[1829 - 7] predict 3/10 CA_3 day 2\n",
      "[1829 - 7] predict 4/10 CA_4 day 2\n",
      "[1829 - 7] predict 5/10 TX_1 day 2\n",
      "[1829 - 7] predict 6/10 TX_2 day 2\n",
      "[1829 - 7] predict 7/10 TX_3 day 2\n",
      "[1829 - 7] predict 8/10 WI_1 day 2\n",
      "[1829 - 7] predict 9/10 WI_2 day 2\n",
      "[1829 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1829 - 7] predict 1/10 CA_1 day 3\n",
      "[1829 - 7] predict 2/10 CA_2 day 3\n",
      "[1829 - 7] predict 3/10 CA_3 day 3\n",
      "[1829 - 7] predict 4/10 CA_4 day 3\n",
      "[1829 - 7] predict 5/10 TX_1 day 3\n",
      "[1829 - 7] predict 6/10 TX_2 day 3\n",
      "[1829 - 7] predict 7/10 TX_3 day 3\n",
      "[1829 - 7] predict 8/10 WI_1 day 3\n",
      "[1829 - 7] predict 9/10 WI_2 day 3\n",
      "[1829 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1829 - 7] predict 1/10 CA_1 day 4\n",
      "[1829 - 7] predict 2/10 CA_2 day 4\n",
      "[1829 - 7] predict 3/10 CA_3 day 4\n",
      "[1829 - 7] predict 4/10 CA_4 day 4\n",
      "[1829 - 7] predict 5/10 TX_1 day 4\n",
      "[1829 - 7] predict 6/10 TX_2 day 4\n",
      "[1829 - 7] predict 7/10 TX_3 day 4\n",
      "[1829 - 7] predict 8/10 WI_1 day 4\n",
      "[1829 - 7] predict 9/10 WI_2 day 4\n",
      "[1829 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1829 - 7] predict 1/10 CA_1 day 5\n",
      "[1829 - 7] predict 2/10 CA_2 day 5\n",
      "[1829 - 7] predict 3/10 CA_3 day 5\n",
      "[1829 - 7] predict 4/10 CA_4 day 5\n",
      "[1829 - 7] predict 5/10 TX_1 day 5\n",
      "[1829 - 7] predict 6/10 TX_2 day 5\n",
      "[1829 - 7] predict 7/10 TX_3 day 5\n",
      "[1829 - 7] predict 8/10 WI_1 day 5\n",
      "[1829 - 7] predict 9/10 WI_2 day 5\n",
      "[1829 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1829 - 7] predict 1/10 CA_1 day 6\n",
      "[1829 - 7] predict 2/10 CA_2 day 6\n",
      "[1829 - 7] predict 3/10 CA_3 day 6\n",
      "[1829 - 7] predict 4/10 CA_4 day 6\n",
      "[1829 - 7] predict 5/10 TX_1 day 6\n",
      "[1829 - 7] predict 6/10 TX_2 day 6\n",
      "[1829 - 7] predict 7/10 TX_3 day 6\n",
      "[1829 - 7] predict 8/10 WI_1 day 6\n",
      "[1829 - 7] predict 9/10 WI_2 day 6\n",
      "[1829 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1829 - 7] predict 1/10 CA_1 day 7\n",
      "[1829 - 7] predict 2/10 CA_2 day 7\n",
      "[1829 - 7] predict 3/10 CA_3 day 7\n",
      "[1829 - 7] predict 4/10 CA_4 day 7\n",
      "[1829 - 7] predict 5/10 TX_1 day 7\n",
      "[1829 - 7] predict 6/10 TX_2 day 7\n",
      "[1829 - 7] predict 7/10 TX_3 day 7\n",
      "[1829 - 7] predict 8/10 WI_1 day 7\n",
      "[1829 - 7] predict 9/10 WI_2 day 7\n",
      "[1829 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1829 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 55766210\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (56193070, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.15655\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.97224\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.10501\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.88404\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.65122\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.37974\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.27655\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.43355\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.79236\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1816-d_1829\n",
      "pred_mask horizon: d_1730-d_1843\n",
      "[1829 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.23249\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1829 - 14] predict 1/10 CA_1 day 8\n",
      "[1829 - 14] predict 2/10 CA_2 day 8\n",
      "[1829 - 14] predict 3/10 CA_3 day 8\n",
      "[1829 - 14] predict 4/10 CA_4 day 8\n",
      "[1829 - 14] predict 5/10 TX_1 day 8\n",
      "[1829 - 14] predict 6/10 TX_2 day 8\n",
      "[1829 - 14] predict 7/10 TX_3 day 8\n",
      "[1829 - 14] predict 8/10 WI_1 day 8\n",
      "[1829 - 14] predict 9/10 WI_2 day 8\n",
      "[1829 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1829 - 14] predict 1/10 CA_1 day 9\n",
      "[1829 - 14] predict 2/10 CA_2 day 9\n",
      "[1829 - 14] predict 3/10 CA_3 day 9\n",
      "[1829 - 14] predict 4/10 CA_4 day 9\n",
      "[1829 - 14] predict 5/10 TX_1 day 9\n",
      "[1829 - 14] predict 6/10 TX_2 day 9\n",
      "[1829 - 14] predict 7/10 TX_3 day 9\n",
      "[1829 - 14] predict 8/10 WI_1 day 9\n",
      "[1829 - 14] predict 9/10 WI_2 day 9\n",
      "[1829 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1829 - 14] predict 1/10 CA_1 day 10\n",
      "[1829 - 14] predict 2/10 CA_2 day 10\n",
      "[1829 - 14] predict 3/10 CA_3 day 10\n",
      "[1829 - 14] predict 4/10 CA_4 day 10\n",
      "[1829 - 14] predict 5/10 TX_1 day 10\n",
      "[1829 - 14] predict 6/10 TX_2 day 10\n",
      "[1829 - 14] predict 7/10 TX_3 day 10\n",
      "[1829 - 14] predict 8/10 WI_1 day 10\n",
      "[1829 - 14] predict 9/10 WI_2 day 10\n",
      "[1829 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1829 - 14] predict 1/10 CA_1 day 11\n",
      "[1829 - 14] predict 2/10 CA_2 day 11\n",
      "[1829 - 14] predict 3/10 CA_3 day 11\n",
      "[1829 - 14] predict 4/10 CA_4 day 11\n",
      "[1829 - 14] predict 5/10 TX_1 day 11\n",
      "[1829 - 14] predict 6/10 TX_2 day 11\n",
      "[1829 - 14] predict 7/10 TX_3 day 11\n",
      "[1829 - 14] predict 8/10 WI_1 day 11\n",
      "[1829 - 14] predict 9/10 WI_2 day 11\n",
      "[1829 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1829 - 14] predict 1/10 CA_1 day 12\n",
      "[1829 - 14] predict 2/10 CA_2 day 12\n",
      "[1829 - 14] predict 3/10 CA_3 day 12\n",
      "[1829 - 14] predict 4/10 CA_4 day 12\n",
      "[1829 - 14] predict 5/10 TX_1 day 12\n",
      "[1829 - 14] predict 6/10 TX_2 day 12\n",
      "[1829 - 14] predict 7/10 TX_3 day 12\n",
      "[1829 - 14] predict 8/10 WI_1 day 12\n",
      "[1829 - 14] predict 9/10 WI_2 day 12\n",
      "[1829 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1829 - 14] predict 1/10 CA_1 day 13\n",
      "[1829 - 14] predict 2/10 CA_2 day 13\n",
      "[1829 - 14] predict 3/10 CA_3 day 13\n",
      "[1829 - 14] predict 4/10 CA_4 day 13\n",
      "[1829 - 14] predict 5/10 TX_1 day 13\n",
      "[1829 - 14] predict 6/10 TX_2 day 13\n",
      "[1829 - 14] predict 7/10 TX_3 day 13\n",
      "[1829 - 14] predict 8/10 WI_1 day 13\n",
      "[1829 - 14] predict 9/10 WI_2 day 13\n",
      "[1829 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1829 - 14] predict 1/10 CA_1 day 14\n",
      "[1829 - 14] predict 2/10 CA_2 day 14\n",
      "[1829 - 14] predict 3/10 CA_3 day 14\n",
      "[1829 - 14] predict 4/10 CA_4 day 14\n",
      "[1829 - 14] predict 5/10 TX_1 day 14\n",
      "[1829 - 14] predict 6/10 TX_2 day 14\n",
      "[1829 - 14] predict 7/10 TX_3 day 14\n",
      "[1829 - 14] predict 8/10 WI_1 day 14\n",
      "[1829 - 14] predict 9/10 WI_2 day 14\n",
      "[1829 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1829 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 55766210\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (56406500, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.19144\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.97995\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.02669\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.85579\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.76646\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.4573\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.30407\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.46999\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.06694\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1809-d_1829\n",
      "pred_mask horizon: d_1730-d_1850\n",
      "[1829 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.50098\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1829 - 21] predict 1/10 CA_1 day 15\n",
      "[1829 - 21] predict 2/10 CA_2 day 15\n",
      "[1829 - 21] predict 3/10 CA_3 day 15\n",
      "[1829 - 21] predict 4/10 CA_4 day 15\n",
      "[1829 - 21] predict 5/10 TX_1 day 15\n",
      "[1829 - 21] predict 6/10 TX_2 day 15\n",
      "[1829 - 21] predict 7/10 TX_3 day 15\n",
      "[1829 - 21] predict 8/10 WI_1 day 15\n",
      "[1829 - 21] predict 9/10 WI_2 day 15\n",
      "[1829 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1829 - 21] predict 1/10 CA_1 day 16\n",
      "[1829 - 21] predict 2/10 CA_2 day 16\n",
      "[1829 - 21] predict 3/10 CA_3 day 16\n",
      "[1829 - 21] predict 4/10 CA_4 day 16\n",
      "[1829 - 21] predict 5/10 TX_1 day 16\n",
      "[1829 - 21] predict 6/10 TX_2 day 16\n",
      "[1829 - 21] predict 7/10 TX_3 day 16\n",
      "[1829 - 21] predict 8/10 WI_1 day 16\n",
      "[1829 - 21] predict 9/10 WI_2 day 16\n",
      "[1829 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1829 - 21] predict 1/10 CA_1 day 17\n",
      "[1829 - 21] predict 2/10 CA_2 day 17\n",
      "[1829 - 21] predict 3/10 CA_3 day 17\n",
      "[1829 - 21] predict 4/10 CA_4 day 17\n",
      "[1829 - 21] predict 5/10 TX_1 day 17\n",
      "[1829 - 21] predict 6/10 TX_2 day 17\n",
      "[1829 - 21] predict 7/10 TX_3 day 17\n",
      "[1829 - 21] predict 8/10 WI_1 day 17\n",
      "[1829 - 21] predict 9/10 WI_2 day 17\n",
      "[1829 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1829 - 21] predict 1/10 CA_1 day 18\n",
      "[1829 - 21] predict 2/10 CA_2 day 18\n",
      "[1829 - 21] predict 3/10 CA_3 day 18\n",
      "[1829 - 21] predict 4/10 CA_4 day 18\n",
      "[1829 - 21] predict 5/10 TX_1 day 18\n",
      "[1829 - 21] predict 6/10 TX_2 day 18\n",
      "[1829 - 21] predict 7/10 TX_3 day 18\n",
      "[1829 - 21] predict 8/10 WI_1 day 18\n",
      "[1829 - 21] predict 9/10 WI_2 day 18\n",
      "[1829 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1829 - 21] predict 1/10 CA_1 day 19\n",
      "[1829 - 21] predict 2/10 CA_2 day 19\n",
      "[1829 - 21] predict 3/10 CA_3 day 19\n",
      "[1829 - 21] predict 4/10 CA_4 day 19\n",
      "[1829 - 21] predict 5/10 TX_1 day 19\n",
      "[1829 - 21] predict 6/10 TX_2 day 19\n",
      "[1829 - 21] predict 7/10 TX_3 day 19\n",
      "[1829 - 21] predict 8/10 WI_1 day 19\n",
      "[1829 - 21] predict 9/10 WI_2 day 19\n",
      "[1829 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1829 - 21] predict 1/10 CA_1 day 20\n",
      "[1829 - 21] predict 2/10 CA_2 day 20\n",
      "[1829 - 21] predict 3/10 CA_3 day 20\n",
      "[1829 - 21] predict 4/10 CA_4 day 20\n",
      "[1829 - 21] predict 5/10 TX_1 day 20\n",
      "[1829 - 21] predict 6/10 TX_2 day 20\n",
      "[1829 - 21] predict 7/10 TX_3 day 20\n",
      "[1829 - 21] predict 8/10 WI_1 day 20\n",
      "[1829 - 21] predict 9/10 WI_2 day 20\n",
      "[1829 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1829 - 21] predict 1/10 CA_1 day 21\n",
      "[1829 - 21] predict 2/10 CA_2 day 21\n",
      "[1829 - 21] predict 3/10 CA_3 day 21\n",
      "[1829 - 21] predict 4/10 CA_4 day 21\n",
      "[1829 - 21] predict 5/10 TX_1 day 21\n",
      "[1829 - 21] predict 6/10 TX_2 day 21\n",
      "[1829 - 21] predict 7/10 TX_3 day 21\n",
      "[1829 - 21] predict 8/10 WI_1 day 21\n",
      "[1829 - 21] predict 9/10 WI_2 day 21\n",
      "[1829 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1829 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 55766210\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (56619930, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.26394\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.00023\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.11763\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.8787\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.80484\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.44008\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.28141\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.4831\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.18882\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1829\n",
      "valid_mask horizon: d_1802-d_1829\n",
      "pred_mask horizon: d_1730-d_1857\n",
      "[1829 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.59856\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1829 - 28] predict 1/10 CA_1 day 22\n",
      "[1829 - 28] predict 2/10 CA_2 day 22\n",
      "[1829 - 28] predict 3/10 CA_3 day 22\n",
      "[1829 - 28] predict 4/10 CA_4 day 22\n",
      "[1829 - 28] predict 5/10 TX_1 day 22\n",
      "[1829 - 28] predict 6/10 TX_2 day 22\n",
      "[1829 - 28] predict 7/10 TX_3 day 22\n",
      "[1829 - 28] predict 8/10 WI_1 day 22\n",
      "[1829 - 28] predict 9/10 WI_2 day 22\n",
      "[1829 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1829 - 28] predict 1/10 CA_1 day 23\n",
      "[1829 - 28] predict 2/10 CA_2 day 23\n",
      "[1829 - 28] predict 3/10 CA_3 day 23\n",
      "[1829 - 28] predict 4/10 CA_4 day 23\n",
      "[1829 - 28] predict 5/10 TX_1 day 23\n",
      "[1829 - 28] predict 6/10 TX_2 day 23\n",
      "[1829 - 28] predict 7/10 TX_3 day 23\n",
      "[1829 - 28] predict 8/10 WI_1 day 23\n",
      "[1829 - 28] predict 9/10 WI_2 day 23\n",
      "[1829 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1829 - 28] predict 1/10 CA_1 day 24\n",
      "[1829 - 28] predict 2/10 CA_2 day 24\n",
      "[1829 - 28] predict 3/10 CA_3 day 24\n",
      "[1829 - 28] predict 4/10 CA_4 day 24\n",
      "[1829 - 28] predict 5/10 TX_1 day 24\n",
      "[1829 - 28] predict 6/10 TX_2 day 24\n",
      "[1829 - 28] predict 7/10 TX_3 day 24\n",
      "[1829 - 28] predict 8/10 WI_1 day 24\n",
      "[1829 - 28] predict 9/10 WI_2 day 24\n",
      "[1829 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1829 - 28] predict 1/10 CA_1 day 25\n",
      "[1829 - 28] predict 2/10 CA_2 day 25\n",
      "[1829 - 28] predict 3/10 CA_3 day 25\n",
      "[1829 - 28] predict 4/10 CA_4 day 25\n",
      "[1829 - 28] predict 5/10 TX_1 day 25\n",
      "[1829 - 28] predict 6/10 TX_2 day 25\n",
      "[1829 - 28] predict 7/10 TX_3 day 25\n",
      "[1829 - 28] predict 8/10 WI_1 day 25\n",
      "[1829 - 28] predict 9/10 WI_2 day 25\n",
      "[1829 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1829 - 28] predict 1/10 CA_1 day 26\n",
      "[1829 - 28] predict 2/10 CA_2 day 26\n",
      "[1829 - 28] predict 3/10 CA_3 day 26\n",
      "[1829 - 28] predict 4/10 CA_4 day 26\n",
      "[1829 - 28] predict 5/10 TX_1 day 26\n",
      "[1829 - 28] predict 6/10 TX_2 day 26\n",
      "[1829 - 28] predict 7/10 TX_3 day 26\n",
      "[1829 - 28] predict 8/10 WI_1 day 26\n",
      "[1829 - 28] predict 9/10 WI_2 day 26\n",
      "[1829 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1829 - 28] predict 1/10 CA_1 day 27\n",
      "[1829 - 28] predict 2/10 CA_2 day 27\n",
      "[1829 - 28] predict 3/10 CA_3 day 27\n",
      "[1829 - 28] predict 4/10 CA_4 day 27\n",
      "[1829 - 28] predict 5/10 TX_1 day 27\n",
      "[1829 - 28] predict 6/10 TX_2 day 27\n",
      "[1829 - 28] predict 7/10 TX_3 day 27\n",
      "[1829 - 28] predict 8/10 WI_1 day 27\n",
      "[1829 - 28] predict 9/10 WI_2 day 27\n",
      "[1829 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1829 - 28] predict 1/10 CA_1 day 28\n",
      "[1829 - 28] predict 2/10 CA_2 day 28\n",
      "[1829 - 28] predict 3/10 CA_3 day 28\n",
      "[1829 - 28] predict 4/10 CA_4 day 28\n",
      "[1829 - 28] predict 5/10 TX_1 day 28\n",
      "[1829 - 28] predict 6/10 TX_2 day 28\n",
      "[1829 - 28] predict 7/10 TX_3 day 28\n",
      "[1829 - 28] predict 8/10 WI_1 day 28\n",
      "[1829 - 28] predict 9/10 WI_2 day 28\n",
      "[1829 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (853720, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "calc metrics\n",
      "result_df.shape (853720, 3)\n",
      "############################# calc_wrmsse ###################################\n",
      "adjust end of train period\n",
      "(30490, 1947) --> (30490, 1863)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5fe2d088fe49f89048cc7d8c1a2a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wrmsse 2.395455234896503\n",
      "   fold_id metric_name  metric_value\n",
      "0     1829      wrmsse      2.395455\n",
      "1     1829        rmse      3.698429\n",
      "----------------- fold_id 1577 predict_horizon 7\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 48082730\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (48296160, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.47453\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.98705\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.48961\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.87364\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.6017\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.22495\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.39852\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.25817\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.92757\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1571-d_1577\n",
      "pred_mask horizon: d_1478-d_1584\n",
      "[1577 - 7] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.49246\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day01\n",
      "[1577 - 7] predict 1/10 CA_1 day 1\n",
      "[1577 - 7] predict 2/10 CA_2 day 1\n",
      "[1577 - 7] predict 3/10 CA_3 day 1\n",
      "[1577 - 7] predict 4/10 CA_4 day 1\n",
      "[1577 - 7] predict 5/10 TX_1 day 1\n",
      "[1577 - 7] predict 6/10 TX_2 day 1\n",
      "[1577 - 7] predict 7/10 TX_3 day 1\n",
      "[1577 - 7] predict 8/10 WI_1 day 1\n",
      "[1577 - 7] predict 9/10 WI_2 day 1\n",
      "[1577 - 7] predict 10/10 WI_3 day 1\n",
      "predict day02\n",
      "[1577 - 7] predict 1/10 CA_1 day 2\n",
      "[1577 - 7] predict 2/10 CA_2 day 2\n",
      "[1577 - 7] predict 3/10 CA_3 day 2\n",
      "[1577 - 7] predict 4/10 CA_4 day 2\n",
      "[1577 - 7] predict 5/10 TX_1 day 2\n",
      "[1577 - 7] predict 6/10 TX_2 day 2\n",
      "[1577 - 7] predict 7/10 TX_3 day 2\n",
      "[1577 - 7] predict 8/10 WI_1 day 2\n",
      "[1577 - 7] predict 9/10 WI_2 day 2\n",
      "[1577 - 7] predict 10/10 WI_3 day 2\n",
      "predict day03\n",
      "[1577 - 7] predict 1/10 CA_1 day 3\n",
      "[1577 - 7] predict 2/10 CA_2 day 3\n",
      "[1577 - 7] predict 3/10 CA_3 day 3\n",
      "[1577 - 7] predict 4/10 CA_4 day 3\n",
      "[1577 - 7] predict 5/10 TX_1 day 3\n",
      "[1577 - 7] predict 6/10 TX_2 day 3\n",
      "[1577 - 7] predict 7/10 TX_3 day 3\n",
      "[1577 - 7] predict 8/10 WI_1 day 3\n",
      "[1577 - 7] predict 9/10 WI_2 day 3\n",
      "[1577 - 7] predict 10/10 WI_3 day 3\n",
      "predict day04\n",
      "[1577 - 7] predict 1/10 CA_1 day 4\n",
      "[1577 - 7] predict 2/10 CA_2 day 4\n",
      "[1577 - 7] predict 3/10 CA_3 day 4\n",
      "[1577 - 7] predict 4/10 CA_4 day 4\n",
      "[1577 - 7] predict 5/10 TX_1 day 4\n",
      "[1577 - 7] predict 6/10 TX_2 day 4\n",
      "[1577 - 7] predict 7/10 TX_3 day 4\n",
      "[1577 - 7] predict 8/10 WI_1 day 4\n",
      "[1577 - 7] predict 9/10 WI_2 day 4\n",
      "[1577 - 7] predict 10/10 WI_3 day 4\n",
      "predict day05\n",
      "[1577 - 7] predict 1/10 CA_1 day 5\n",
      "[1577 - 7] predict 2/10 CA_2 day 5\n",
      "[1577 - 7] predict 3/10 CA_3 day 5\n",
      "[1577 - 7] predict 4/10 CA_4 day 5\n",
      "[1577 - 7] predict 5/10 TX_1 day 5\n",
      "[1577 - 7] predict 6/10 TX_2 day 5\n",
      "[1577 - 7] predict 7/10 TX_3 day 5\n",
      "[1577 - 7] predict 8/10 WI_1 day 5\n",
      "[1577 - 7] predict 9/10 WI_2 day 5\n",
      "[1577 - 7] predict 10/10 WI_3 day 5\n",
      "predict day06\n",
      "[1577 - 7] predict 1/10 CA_1 day 6\n",
      "[1577 - 7] predict 2/10 CA_2 day 6\n",
      "[1577 - 7] predict 3/10 CA_3 day 6\n",
      "[1577 - 7] predict 4/10 CA_4 day 6\n",
      "[1577 - 7] predict 5/10 TX_1 day 6\n",
      "[1577 - 7] predict 6/10 TX_2 day 6\n",
      "[1577 - 7] predict 7/10 TX_3 day 6\n",
      "[1577 - 7] predict 8/10 WI_1 day 6\n",
      "[1577 - 7] predict 9/10 WI_2 day 6\n",
      "[1577 - 7] predict 10/10 WI_3 day 6\n",
      "predict day07\n",
      "[1577 - 7] predict 1/10 CA_1 day 7\n",
      "[1577 - 7] predict 2/10 CA_2 day 7\n",
      "[1577 - 7] predict 3/10 CA_3 day 7\n",
      "[1577 - 7] predict 4/10 CA_4 day 7\n",
      "[1577 - 7] predict 5/10 TX_1 day 7\n",
      "[1577 - 7] predict 6/10 TX_2 day 7\n",
      "[1577 - 7] predict 7/10 TX_3 day 7\n",
      "[1577 - 7] predict 8/10 WI_1 day 7\n",
      "[1577 - 7] predict 9/10 WI_2 day 7\n",
      "[1577 - 7] predict 10/10 WI_3 day 7\n",
      "clear work_dir\n",
      "----------------- fold_id 1577 predict_horizon 14\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 48082730\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (48509590, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.55467\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.01513\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 4.75966\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.77807\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.75787\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.44873\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.69372\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.3586\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.48358\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1564-d_1577\n",
      "pred_mask horizon: d_1478-d_1591\n",
      "[1577 - 14] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.70198\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day08\n",
      "[1577 - 14] predict 1/10 CA_1 day 8\n",
      "[1577 - 14] predict 2/10 CA_2 day 8\n",
      "[1577 - 14] predict 3/10 CA_3 day 8\n",
      "[1577 - 14] predict 4/10 CA_4 day 8\n",
      "[1577 - 14] predict 5/10 TX_1 day 8\n",
      "[1577 - 14] predict 6/10 TX_2 day 8\n",
      "[1577 - 14] predict 7/10 TX_3 day 8\n",
      "[1577 - 14] predict 8/10 WI_1 day 8\n",
      "[1577 - 14] predict 9/10 WI_2 day 8\n",
      "[1577 - 14] predict 10/10 WI_3 day 8\n",
      "predict day09\n",
      "[1577 - 14] predict 1/10 CA_1 day 9\n",
      "[1577 - 14] predict 2/10 CA_2 day 9\n",
      "[1577 - 14] predict 3/10 CA_3 day 9\n",
      "[1577 - 14] predict 4/10 CA_4 day 9\n",
      "[1577 - 14] predict 5/10 TX_1 day 9\n",
      "[1577 - 14] predict 6/10 TX_2 day 9\n",
      "[1577 - 14] predict 7/10 TX_3 day 9\n",
      "[1577 - 14] predict 8/10 WI_1 day 9\n",
      "[1577 - 14] predict 9/10 WI_2 day 9\n",
      "[1577 - 14] predict 10/10 WI_3 day 9\n",
      "predict day10\n",
      "[1577 - 14] predict 1/10 CA_1 day 10\n",
      "[1577 - 14] predict 2/10 CA_2 day 10\n",
      "[1577 - 14] predict 3/10 CA_3 day 10\n",
      "[1577 - 14] predict 4/10 CA_4 day 10\n",
      "[1577 - 14] predict 5/10 TX_1 day 10\n",
      "[1577 - 14] predict 6/10 TX_2 day 10\n",
      "[1577 - 14] predict 7/10 TX_3 day 10\n",
      "[1577 - 14] predict 8/10 WI_1 day 10\n",
      "[1577 - 14] predict 9/10 WI_2 day 10\n",
      "[1577 - 14] predict 10/10 WI_3 day 10\n",
      "predict day11\n",
      "[1577 - 14] predict 1/10 CA_1 day 11\n",
      "[1577 - 14] predict 2/10 CA_2 day 11\n",
      "[1577 - 14] predict 3/10 CA_3 day 11\n",
      "[1577 - 14] predict 4/10 CA_4 day 11\n",
      "[1577 - 14] predict 5/10 TX_1 day 11\n",
      "[1577 - 14] predict 6/10 TX_2 day 11\n",
      "[1577 - 14] predict 7/10 TX_3 day 11\n",
      "[1577 - 14] predict 8/10 WI_1 day 11\n",
      "[1577 - 14] predict 9/10 WI_2 day 11\n",
      "[1577 - 14] predict 10/10 WI_3 day 11\n",
      "predict day12\n",
      "[1577 - 14] predict 1/10 CA_1 day 12\n",
      "[1577 - 14] predict 2/10 CA_2 day 12\n",
      "[1577 - 14] predict 3/10 CA_3 day 12\n",
      "[1577 - 14] predict 4/10 CA_4 day 12\n",
      "[1577 - 14] predict 5/10 TX_1 day 12\n",
      "[1577 - 14] predict 6/10 TX_2 day 12\n",
      "[1577 - 14] predict 7/10 TX_3 day 12\n",
      "[1577 - 14] predict 8/10 WI_1 day 12\n",
      "[1577 - 14] predict 9/10 WI_2 day 12\n",
      "[1577 - 14] predict 10/10 WI_3 day 12\n",
      "predict day13\n",
      "[1577 - 14] predict 1/10 CA_1 day 13\n",
      "[1577 - 14] predict 2/10 CA_2 day 13\n",
      "[1577 - 14] predict 3/10 CA_3 day 13\n",
      "[1577 - 14] predict 4/10 CA_4 day 13\n",
      "[1577 - 14] predict 5/10 TX_1 day 13\n",
      "[1577 - 14] predict 6/10 TX_2 day 13\n",
      "[1577 - 14] predict 7/10 TX_3 day 13\n",
      "[1577 - 14] predict 8/10 WI_1 day 13\n",
      "[1577 - 14] predict 9/10 WI_2 day 13\n",
      "[1577 - 14] predict 10/10 WI_3 day 13\n",
      "predict day14\n",
      "[1577 - 14] predict 1/10 CA_1 day 14\n",
      "[1577 - 14] predict 2/10 CA_2 day 14\n",
      "[1577 - 14] predict 3/10 CA_3 day 14\n",
      "[1577 - 14] predict 4/10 CA_4 day 14\n",
      "[1577 - 14] predict 5/10 TX_1 day 14\n",
      "[1577 - 14] predict 6/10 TX_2 day 14\n",
      "[1577 - 14] predict 7/10 TX_3 day 14\n",
      "[1577 - 14] predict 8/10 WI_1 day 14\n",
      "[1577 - 14] predict 9/10 WI_2 day 14\n",
      "[1577 - 14] predict 10/10 WI_3 day 14\n",
      "clear work_dir\n",
      "----------------- fold_id 1577 predict_horizon 21\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 48082730\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (48723020, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.75161\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.98543\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.11781\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.84498\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.74508\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.48561\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.7934\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.38734\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.6998\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1557-d_1577\n",
      "pred_mask horizon: d_1478-d_1598\n",
      "[1577 - 21] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.92001\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day15\n",
      "[1577 - 21] predict 1/10 CA_1 day 15\n",
      "[1577 - 21] predict 2/10 CA_2 day 15\n",
      "[1577 - 21] predict 3/10 CA_3 day 15\n",
      "[1577 - 21] predict 4/10 CA_4 day 15\n",
      "[1577 - 21] predict 5/10 TX_1 day 15\n",
      "[1577 - 21] predict 6/10 TX_2 day 15\n",
      "[1577 - 21] predict 7/10 TX_3 day 15\n",
      "[1577 - 21] predict 8/10 WI_1 day 15\n",
      "[1577 - 21] predict 9/10 WI_2 day 15\n",
      "[1577 - 21] predict 10/10 WI_3 day 15\n",
      "predict day16\n",
      "[1577 - 21] predict 1/10 CA_1 day 16\n",
      "[1577 - 21] predict 2/10 CA_2 day 16\n",
      "[1577 - 21] predict 3/10 CA_3 day 16\n",
      "[1577 - 21] predict 4/10 CA_4 day 16\n",
      "[1577 - 21] predict 5/10 TX_1 day 16\n",
      "[1577 - 21] predict 6/10 TX_2 day 16\n",
      "[1577 - 21] predict 7/10 TX_3 day 16\n",
      "[1577 - 21] predict 8/10 WI_1 day 16\n",
      "[1577 - 21] predict 9/10 WI_2 day 16\n",
      "[1577 - 21] predict 10/10 WI_3 day 16\n",
      "predict day17\n",
      "[1577 - 21] predict 1/10 CA_1 day 17\n",
      "[1577 - 21] predict 2/10 CA_2 day 17\n",
      "[1577 - 21] predict 3/10 CA_3 day 17\n",
      "[1577 - 21] predict 4/10 CA_4 day 17\n",
      "[1577 - 21] predict 5/10 TX_1 day 17\n",
      "[1577 - 21] predict 6/10 TX_2 day 17\n",
      "[1577 - 21] predict 7/10 TX_3 day 17\n",
      "[1577 - 21] predict 8/10 WI_1 day 17\n",
      "[1577 - 21] predict 9/10 WI_2 day 17\n",
      "[1577 - 21] predict 10/10 WI_3 day 17\n",
      "predict day18\n",
      "[1577 - 21] predict 1/10 CA_1 day 18\n",
      "[1577 - 21] predict 2/10 CA_2 day 18\n",
      "[1577 - 21] predict 3/10 CA_3 day 18\n",
      "[1577 - 21] predict 4/10 CA_4 day 18\n",
      "[1577 - 21] predict 5/10 TX_1 day 18\n",
      "[1577 - 21] predict 6/10 TX_2 day 18\n",
      "[1577 - 21] predict 7/10 TX_3 day 18\n",
      "[1577 - 21] predict 8/10 WI_1 day 18\n",
      "[1577 - 21] predict 9/10 WI_2 day 18\n",
      "[1577 - 21] predict 10/10 WI_3 day 18\n",
      "predict day19\n",
      "[1577 - 21] predict 1/10 CA_1 day 19\n",
      "[1577 - 21] predict 2/10 CA_2 day 19\n",
      "[1577 - 21] predict 3/10 CA_3 day 19\n",
      "[1577 - 21] predict 4/10 CA_4 day 19\n",
      "[1577 - 21] predict 5/10 TX_1 day 19\n",
      "[1577 - 21] predict 6/10 TX_2 day 19\n",
      "[1577 - 21] predict 7/10 TX_3 day 19\n",
      "[1577 - 21] predict 8/10 WI_1 day 19\n",
      "[1577 - 21] predict 9/10 WI_2 day 19\n",
      "[1577 - 21] predict 10/10 WI_3 day 19\n",
      "predict day20\n",
      "[1577 - 21] predict 1/10 CA_1 day 20\n",
      "[1577 - 21] predict 2/10 CA_2 day 20\n",
      "[1577 - 21] predict 3/10 CA_3 day 20\n",
      "[1577 - 21] predict 4/10 CA_4 day 20\n",
      "[1577 - 21] predict 5/10 TX_1 day 20\n",
      "[1577 - 21] predict 6/10 TX_2 day 20\n",
      "[1577 - 21] predict 7/10 TX_3 day 20\n",
      "[1577 - 21] predict 8/10 WI_1 day 20\n",
      "[1577 - 21] predict 9/10 WI_2 day 20\n",
      "[1577 - 21] predict 10/10 WI_3 day 20\n",
      "predict day21\n",
      "[1577 - 21] predict 1/10 CA_1 day 21\n",
      "[1577 - 21] predict 2/10 CA_2 day 21\n",
      "[1577 - 21] predict 3/10 CA_3 day 21\n",
      "[1577 - 21] predict 4/10 CA_4 day 21\n",
      "[1577 - 21] predict 5/10 TX_1 day 21\n",
      "[1577 - 21] predict 6/10 TX_2 day 21\n",
      "[1577 - 21] predict 7/10 TX_3 day 21\n",
      "[1577 - 21] predict 8/10 WI_1 day 21\n",
      "[1577 - 21] predict 9/10 WI_2 day 21\n",
      "[1577 - 21] predict 10/10 WI_3 day 21\n",
      "clear work_dir\n",
      "----------------- fold_id 1577 predict_horizon 28\n",
      "################# generate_grid_base ###################\n",
      "melting to convert horizontal to vertical data\n",
      "grid_df.shape after melting (vertical data) (59181090, 8)\n",
      "generate holdout data for validation by removing days before end_train_day_x\n",
      "59181090 --> 48082730\n",
      "add test days\n",
      "convert to category\n",
      "calc release week\n",
      "convert release to int16\n",
      "save grid_base\n",
      "grid_df.shape (48936450, 10)\n",
      "################## generate_grid_price #################\n",
      "loading grid_base\n",
      "merge prices\n",
      "save grid_price\n",
      "################## generate_grid_calendar ####################\n",
      "############## modify_grid_base ####################\n",
      "################ generate_lag features ################\n",
      "load grid_base\n",
      "creating lags\n",
      "create rolling aggs (mean/std)\n",
      "rolling period 7\n",
      "save lag_feature\n",
      "################ generate_target_encoding_feature ################\n",
      "encoding ['state_id']\n",
      "encoding ['store_id']\n",
      "encoding ['cat_id']\n",
      "encoding ['dept_id']\n",
      "encoding ['state_id', 'cat_id']\n",
      "encoding ['state_id', 'dept_id']\n",
      "encoding ['store_id', 'cat_id']\n",
      "encoding ['store_id', 'dept_id']\n",
      "encoding ['item_id']\n",
      "encoding ['item_id', 'state_id']\n",
      "encoding ['item_id', 'store_id']\n",
      "save target_encoding_feature\n",
      "######################################## train_and_predict #####################################################\n",
      "train CA_1\n",
      "Loading the data for store: CA_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 1/10 CA_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.7451\n",
      "train CA_2\n",
      "Loading the data for store: CA_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 2/10 CA_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.05271\n",
      "train CA_3\n",
      "Loading the data for store: CA_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 3/10 CA_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 5.2212\n",
      "train CA_4\n",
      "Loading the data for store: CA_4\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 4/10 CA_4\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 1.88662\n",
      "train TX_1\n",
      "Loading the data for store: TX_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 5/10 TX_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.83485\n",
      "train TX_2\n",
      "Loading the data for store: TX_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 6/10 TX_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.48203\n",
      "train TX_3\n",
      "Loading the data for store: TX_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 7/10 TX_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.87263\n",
      "train WI_1\n",
      "Loading the data for store: WI_1\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 8/10 WI_1\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 2.40907\n",
      "train WI_2\n",
      "Loading the data for store: WI_2\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 9/10 WI_2\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.73389\n",
      "train WI_3\n",
      "Loading the data for store: WI_3\n",
      "############ load_grid_full #####################\n",
      "train_mask horizon: d_1-d_1577\n",
      "valid_mask horizon: d_1550-d_1577\n",
      "pred_mask horizon: d_1478-d_1605\n",
      "[1577 - 28] train 10/10 WI_3\n",
      "#################### log_evaluation #######################\n",
      "[1]\tvalid_0's rmse: 3.00877\n",
      "aggregate feature importance\n",
      "load base_test\n",
      "predict day22\n",
      "[1577 - 28] predict 1/10 CA_1 day 22\n",
      "[1577 - 28] predict 2/10 CA_2 day 22\n",
      "[1577 - 28] predict 3/10 CA_3 day 22\n",
      "[1577 - 28] predict 4/10 CA_4 day 22\n",
      "[1577 - 28] predict 5/10 TX_1 day 22\n",
      "[1577 - 28] predict 6/10 TX_2 day 22\n",
      "[1577 - 28] predict 7/10 TX_3 day 22\n",
      "[1577 - 28] predict 8/10 WI_1 day 22\n",
      "[1577 - 28] predict 9/10 WI_2 day 22\n",
      "[1577 - 28] predict 10/10 WI_3 day 22\n",
      "predict day23\n",
      "[1577 - 28] predict 1/10 CA_1 day 23\n",
      "[1577 - 28] predict 2/10 CA_2 day 23\n",
      "[1577 - 28] predict 3/10 CA_3 day 23\n",
      "[1577 - 28] predict 4/10 CA_4 day 23\n",
      "[1577 - 28] predict 5/10 TX_1 day 23\n",
      "[1577 - 28] predict 6/10 TX_2 day 23\n",
      "[1577 - 28] predict 7/10 TX_3 day 23\n",
      "[1577 - 28] predict 8/10 WI_1 day 23\n",
      "[1577 - 28] predict 9/10 WI_2 day 23\n",
      "[1577 - 28] predict 10/10 WI_3 day 23\n",
      "predict day24\n",
      "[1577 - 28] predict 1/10 CA_1 day 24\n",
      "[1577 - 28] predict 2/10 CA_2 day 24\n",
      "[1577 - 28] predict 3/10 CA_3 day 24\n",
      "[1577 - 28] predict 4/10 CA_4 day 24\n",
      "[1577 - 28] predict 5/10 TX_1 day 24\n",
      "[1577 - 28] predict 6/10 TX_2 day 24\n",
      "[1577 - 28] predict 7/10 TX_3 day 24\n",
      "[1577 - 28] predict 8/10 WI_1 day 24\n",
      "[1577 - 28] predict 9/10 WI_2 day 24\n",
      "[1577 - 28] predict 10/10 WI_3 day 24\n",
      "predict day25\n",
      "[1577 - 28] predict 1/10 CA_1 day 25\n",
      "[1577 - 28] predict 2/10 CA_2 day 25\n",
      "[1577 - 28] predict 3/10 CA_3 day 25\n",
      "[1577 - 28] predict 4/10 CA_4 day 25\n",
      "[1577 - 28] predict 5/10 TX_1 day 25\n",
      "[1577 - 28] predict 6/10 TX_2 day 25\n",
      "[1577 - 28] predict 7/10 TX_3 day 25\n",
      "[1577 - 28] predict 8/10 WI_1 day 25\n",
      "[1577 - 28] predict 9/10 WI_2 day 25\n",
      "[1577 - 28] predict 10/10 WI_3 day 25\n",
      "predict day26\n",
      "[1577 - 28] predict 1/10 CA_1 day 26\n",
      "[1577 - 28] predict 2/10 CA_2 day 26\n",
      "[1577 - 28] predict 3/10 CA_3 day 26\n",
      "[1577 - 28] predict 4/10 CA_4 day 26\n",
      "[1577 - 28] predict 5/10 TX_1 day 26\n",
      "[1577 - 28] predict 6/10 TX_2 day 26\n",
      "[1577 - 28] predict 7/10 TX_3 day 26\n",
      "[1577 - 28] predict 8/10 WI_1 day 26\n",
      "[1577 - 28] predict 9/10 WI_2 day 26\n",
      "[1577 - 28] predict 10/10 WI_3 day 26\n",
      "predict day27\n",
      "[1577 - 28] predict 1/10 CA_1 day 27\n",
      "[1577 - 28] predict 2/10 CA_2 day 27\n",
      "[1577 - 28] predict 3/10 CA_3 day 27\n",
      "[1577 - 28] predict 4/10 CA_4 day 27\n",
      "[1577 - 28] predict 5/10 TX_1 day 27\n",
      "[1577 - 28] predict 6/10 TX_2 day 27\n",
      "[1577 - 28] predict 7/10 TX_3 day 27\n",
      "[1577 - 28] predict 8/10 WI_1 day 27\n",
      "[1577 - 28] predict 9/10 WI_2 day 27\n",
      "[1577 - 28] predict 10/10 WI_3 day 27\n",
      "predict day28\n",
      "[1577 - 28] predict 1/10 CA_1 day 28\n",
      "[1577 - 28] predict 2/10 CA_2 day 28\n",
      "[1577 - 28] predict 3/10 CA_3 day 28\n",
      "[1577 - 28] predict 4/10 CA_4 day 28\n",
      "[1577 - 28] predict 5/10 TX_1 day 28\n",
      "[1577 - 28] predict 6/10 TX_2 day 28\n",
      "[1577 - 28] predict 7/10 TX_3 day 28\n",
      "[1577 - 28] predict 8/10 WI_1 day 28\n",
      "[1577 - 28] predict 9/10 WI_2 day 28\n",
      "[1577 - 28] predict 10/10 WI_3 day 28\n",
      "clear work_dir\n",
      "holdout_df.shape (853720, 3)\n",
      "pred_v_all_df.shape (853720, 3)\n",
      "calc metrics\n",
      "result_df.shape (853720, 3)\n",
      "############################# calc_wrmsse ###################################\n",
      "adjust end of train period\n",
      "(30490, 1947) --> (30490, 1611)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df8dbaa0de340e9a720e6ae2cd10f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wrmsse 2.1684427821158345\n",
      "   fold_id metric_name  metric_value\n",
      "0     1577      wrmsse      2.168443\n",
      "1     1577        rmse      3.697607\n",
      "   fold_id metric_name  metric_value\n",
      "0     1913      wrmsse      2.525628\n",
      "1     1913        rmse      3.639888\n",
      "0     1885      wrmsse      2.426944\n",
      "1     1885        rmse      3.586252\n",
      "0     1857      wrmsse      2.410409\n",
      "1     1857        rmse      3.640562\n",
      "0     1829      wrmsse      2.395455\n",
      "1     1829        rmse      3.698429\n",
      "0     1577      wrmsse      2.168443\n",
      "1     1577        rmse      3.697607\n",
      "                 mean    median\n",
      "metric_name                    \n",
      "rmse         3.652548  3.640562\n",
      "wrmsse       2.385376  2.410409\n"
     ]
    }
   ],
   "source": [
    "print('main')\n",
    "#loading raw data\n",
    "train_df, prices_df, calendar_df, submission_df = load_data()\n",
    "#saving original result, work and model directory which will be changed as we iterate for different end_train_day_x.\n",
    "result_dir_org_path = result_dir_path\n",
    "work_dir_org_path = work_dir_path\n",
    "model_dir_org_path = model_dir_path\n",
    "result_summary_all_df = pd.DataFrame()\n",
    "#iterating for different end_train_day_x to evaluate in different time horizons\n",
    "for end_train_day_x in end_train_day_x_list:\n",
    "    end_train_day_x = end_train_day_x\n",
    "    # updating result, work and model directory depending on the end_train_day_x it is running for\n",
    "    result_dir_path = result_dir_org_path / str(end_train_day_x)\n",
    "    work_dir_path = work_dir_org_path / str(end_train_day_x)\n",
    "    model_dir_path = model_dir_org_path / str(end_train_day_x)\n",
    "    grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "    lag_feature_path, target_encoding_feature_path, result_submission_path =update_file_path()\n",
    "\n",
    "    pred_h_all_df = pd.DataFrame()\n",
    "    pred_v_all_df = pd.DataFrame()\n",
    "    prediction_horizon_prev = 0 #initially prediction_horizon_prev is zero.\n",
    "    # iterate for different prediction_horizons [7, 14, 21, 28] since we are building different models for different prediction_horizon.\n",
    "    for predict_horizon in prediction_horizon_list:\n",
    "        print('-----------------', 'fold_id', end_train_day_x, 'predict_horizon', predict_horizon)\n",
    "        prediction_horizon = predict_horizon\n",
    "        # get the different lags to generate the feature for for different prediction_horizon.\n",
    "        num_lag_day_list, num_shift_rolling_day_list = update_predict_horizon()\n",
    "        #generate base, price and calendar related features\n",
    "        generate_grid_full(train_df, prices_df, calendar_df)\n",
    "        # generate lag features\n",
    "        generate_lag_feature(grid_base_path, lag_feature_path, num_lag_day_list, num_rolling_day_list, recursive_feature_flag, num_shift_rolling_day_list)\n",
    "        #generate target encoding for categorical variables/pairs\n",
    "        generate_target_encoding_feature(grid_base_path)\n",
    "        #train and get prediction with different models for each prediction_horizon and stores.\n",
    "        pred_h_df, pred_v_df = train_and_predict(train_df, calendar_df, prices_df, submission_df)\n",
    "        # append pred_h and pred_v for [7, 14, 21, 28] prediction_horizon to get the overall pred_h_all and pred_v_all\n",
    "        if pred_h_all_df.shape[1] == 0:\n",
    "            pred_h_all_df = pred_h_df\n",
    "        else:\n",
    "            pred_h_all_df = pred_h_all_df.merge(pred_h_df, on='id')\n",
    "        pred_v_all_df = pd.concat([pred_v_all_df, pred_v_df], axis=0)\n",
    "        prediction_horizon_prev = predict_horizon\n",
    "\n",
    "        try:\n",
    "            print('clear work_dir')\n",
    "            shutil.rmtree(work_dir_path)\n",
    "            os.mkdir(work_dir_path)\n",
    "        except Exception:\n",
    "            log.exception()\n",
    "    #save pred_h_all and pred_v_all\n",
    "    pred_h_all_df.to_csv(result_dir_path / 'pred_h_all.csv', index=False)\n",
    "    pred_v_all_df.to_csv(result_dir_path / 'pred_v_all.csv', index=False)\n",
    "    #read the holdout data for different end_train_day_x and prediction_horizon which was saved before.\n",
    "    holdout_df = pd.read_csv(holdout_path)\n",
    "    print('holdout_df.shape', holdout_df.shape)\n",
    "    print('pred_v_all_df.shape', pred_v_all_df.shape)\n",
    "\n",
    "    #following if condition will correspond to final submission holdout data.\n",
    "    if holdout_df.shape[0] == 0:\n",
    "        print('no holdout')\n",
    "        print('generate submission')\n",
    "        pred_h_all_df = pred_h_all_df.reset_index(drop=True)\n",
    "        submission = pd.read_csv(raw_submission_path)[['id']]\n",
    "        submission = submission.merge(pred_h_all_df, on=['id'], how='left').fillna(0)\n",
    "        #submission csv file is saved in result folder in the output directory\n",
    "        submission.to_csv(result_submission_path, index=False)\n",
    "        result_summary_df = None\n",
    "    else:\n",
    "        print('calc metrics')\n",
    "        #merge the actual holdout data and predictions for it to get the evaluation metrics.\n",
    "        result_df = holdout_df.merge(pred_v_all_df, on=['id', 'd'], how='inner')\n",
    "        result_df.columns = ['id', 'd', 'y_test', 'y_pred']\n",
    "        print('result_df.shape', pred_v_all_df.shape)\n",
    "        result_df.to_csv(result_dir_path / 'result.csv', index=False)\n",
    "        # calculate the value of evaluation metrics on holdout actuals and preds.\n",
    "        wrmsse = calc_wrmsse(train_df, prices_df, calendar_df, submission_df, pred_h_all_df)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(result_df['y_test'], result_df['y_pred']))\n",
    "        #save these metrics value in a dataframe corresponding to different end_train_day_x\n",
    "        result_summary_df = pd.DataFrame(\n",
    "            [\n",
    "                [end_train_day_x, 'wrmsse', wrmsse],\n",
    "                [end_train_day_x, 'rmse', rmse],\n",
    "            ],\n",
    "            columns=['fold_id', 'metric_name', 'metric_value'])\n",
    "        print(result_summary_df)\n",
    "        result_summary_df.to_csv(result_dir_path / 'result_summary.csv', index=False)\n",
    "        result_summary_all_df = pd.concat([result_summary_all_df, result_summary_df])\n",
    "\n",
    "#get the aggregated metrics mean and std across different end_train_day_x.\n",
    "if result_summary_all_df.shape[0] == 0:\n",
    "    pass\n",
    "else:\n",
    "    print(result_summary_all_df)\n",
    "    print(result_summary_all_df.groupby('metric_name')['metric_value'].agg(['mean', 'median']))\n",
    "    result_summary_all_df.to_csv(result_dir_org_path / 'result_summary_all.csv', index=False)\n",
    "reset_dir_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_h_df, pred_v_df= train_and_predict(train_df, calendar_df, prices_df, submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     print('main')\n",
    "#     global raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, \\\n",
    "#            prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path, \\\n",
    "#            remove_features, enable_features, mean_features, grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "#             lag_feature_path, target_encoding_feature_path, result_submission_path\n",
    "    \n",
    "#     train_df, prices_df, calendar_df, submission_df = load_data()\n",
    "#     result_dir_org_path = result_dir_path\n",
    "#     work_dir_org_path = work_dir_path\n",
    "#     model_dir_org_path = model_dir_path\n",
    "#     result_summary_all_df = pd.DataFrame()\n",
    "#     for end_train_day_x in end_train_day_x_list:\n",
    "#         end_train_day_x = end_train_day_x\n",
    "#         result_dir_path = result_dir_org_path / str(end_train_day_x)\n",
    "#         work_dir_path = work_dir_org_path / str(end_train_day_x)\n",
    "#         model_dir_path = model_dir_org_path / str(end_train_day_x)\n",
    "#         grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "#         lag_feature_path, target_encoding_feature_path, result_submission_path = update_file_path()\n",
    "\n",
    "#         pred_h_all_df = pd.DataFrame()\n",
    "#         pred_v_all_df = pd.DataFrame()\n",
    "#         prediction_horizon_prev = 0\n",
    "#         for predict_horizon in prediction_horizon_list:\n",
    "#             print('-----------------', 'fold_id', end_train_day_x, 'predict_horizon', predict_horizon)\n",
    "#             prediction_horizon = predict_horizon\n",
    "#             num_lag_day_list, num_shift_rolling_day_list = update_predict_horizon()\n",
    "#             generate_grid_full(train_df, prices_df, calendar_df)\n",
    "#             generate_lag_feature(grid_base_path, lag_feature_path, num_lag_day_list, num_rolling_day_list, recursive_feature_flag, num_shift_rolling_day_list)\n",
    "#             generate_target_encoding_feature(grid_base_path)\n",
    "#             pred_h_df, pred_v_df = train_and_predict(train_df, calendar_df, prices_df, submission_df)\n",
    "#             if pred_h_all_df.shape[1] == 0:\n",
    "#                 pred_h_all_df = pred_h_df\n",
    "#             else:\n",
    "#                 pred_h_all_df = pred_h_all_df.merge(pred_h_df, on='id')\n",
    "#             pred_v_all_df = pd.concat([pred_v_all_df, pred_v_df], axis=0)\n",
    "#             prediction_horizon_prev = predict_horizon\n",
    "\n",
    "#             try:\n",
    "#                 print('clear work_dir')\n",
    "#                 shutil.rmtree(work_dir_path)\n",
    "#                 os.mkdir(work_dir_path)\n",
    "#             except Exception:\n",
    "#                 log.exception()\n",
    "\n",
    "#         pred_h_all_df.to_csv(result_dir_path / 'pred_h_all.csv', index=False)\n",
    "#         pred_v_all_df.to_csv(result_dir_path / 'pred_v_all.csv', index=False)\n",
    "\n",
    "#         holdout_df = pd.read_csv(holdout_path)\n",
    "#         print('holdout_df.shape', holdout_df.shape)\n",
    "#         print('pred_v_all_df.shape', pred_v_all_df.shape)\n",
    "\n",
    "#         if holdout_df.shape[0] == 0:\n",
    "#             print('no holdout')\n",
    "#             print('generate submission')\n",
    "#             pred_h_all_df = pred_h_all_df.reset_index(drop=True)\n",
    "#             submission = pd.read_csv(raw_submission_path)[['id']]\n",
    "#             submission = submission.merge(pred_h_all_df, on=['id'], how='left').fillna(0)\n",
    "#             submission.to_csv(result_submission_path, index=False)\n",
    "#             result_summary_df = None\n",
    "#         else:\n",
    "#             print('calc metrics')\n",
    "#             result_df = holdout_df.merge(pred_v_all_df, on=['id', 'd'], how='inner')\n",
    "#             result_df.columns = ['id', 'd', 'y_test', 'y_pred']\n",
    "#             print('result_df.shape', pred_v_all_df.shape)\n",
    "#             result_df.to_csv(result_dir_path / 'result.csv', index=False)\n",
    "\n",
    "#             wrmsse = calc_wrmsse(train_df, prices_df, calendar_df, submission_df, pred_h_all_df)\n",
    "\n",
    "#             rmse = np.sqrt(mean_squared_error(result_df['y_test'], result_df['y_pred']))\n",
    "\n",
    "#             result_summary_df = pd.DataFrame(\n",
    "#                 [\n",
    "#                     [end_train_day_x, 'wrmsse', wrmsse],\n",
    "#                     [end_train_day_x, 'rmse', rmse],\n",
    "#                 ],\n",
    "#                 columns=['fold_id', 'metric_name', 'metric_value'])\n",
    "#             print(result_summary_df)\n",
    "#             result_summary_df.to_csv(result_dir_path / 'result_summary.csv', index=False)\n",
    "#             result_summary_all_df = pd.concat([result_summary_all_df, result_summary_df])\n",
    "\n",
    "#     if result_summary_all_df.shape[0] == 0:\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(result_summary_all_df)\n",
    "#         print(result_summary_all_df.groupby('metric_name')['metric_value'].agg(['mean', 'median']))\n",
    "#         result_summary_all_df.to_csv(result_dir_org_path / 'result_summary_all.csv', index=False)\n",
    "#     reset_dir_path()\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df, enable_features = load_grid_by_store(\"CA_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.dtypes[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = \"/home/jupyter/m5-forecasting-kaggle/\"\n",
    "# raw_train_file = \"sales_train_evaluation.csv\"\n",
    "# raw_price_file = \"sell_prices.csv\"\n",
    "# raw_calendar_file = \"calendar.csv\"\n",
    "# raw_submission_file = \"sample_submission.csv\"\n",
    "# end_train_day_x = 1913 #till d_1913 will go to train data and last 28 days will go into validation data. --re\n",
    "\n",
    "# main_index_list = ['id', 'd']\n",
    "# target= \"sales\"\n",
    "# start_train_day_x = 1\n",
    "# prediction_horizon= 7 #--re\n",
    "# num_lag_day = 15\n",
    "# recursive_feature_flag = False\n",
    "# seed = 42\n",
    "# export_all_flag = False\n",
    "# recursive_feature_flag = False\n",
    "\n",
    "# num_lag_day = 15\n",
    "# num_rolling_day_list = [7, 14, 30, 60, 180]\n",
    "\n",
    "# prediction_horizon_prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting = dict()\n",
    "# setting['data_dir_path'] =  base_dir +\"Data/\"\n",
    "# setting['output_name'] = \"default\"\n",
    "# setting['fold_id_list_csv'] = \"1913\"\n",
    "# setting['prediction_horizon_list_csv']= \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, \\\n",
    "#            prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path, \\\n",
    "#            remove_features, enable_features, mean_features = set_params(setting, seed, raw_train_file, raw_price_file, raw_calendar_file, raw_submission_file)\n",
    "\n",
    "# raw_dir_path, output_dir_path, result_dir_path, work_dir_path, model_dir_path, end_train_day_x_list, \\\n",
    "#            prediction_horizon_list, raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path, \\\n",
    "#            remove_features, enable_features, mean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, prices_df, calendar_df, submission_df = load_data(raw_train_path, raw_price_path, raw_calendar_path, raw_submission_path)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dir_org_path = result_dir_path\n",
    "# work_dir_org_path = work_dir_path\n",
    "# model_dir_org_path = model_dir_path\n",
    "# result_summary_all_df = pd.DataFrame()\n",
    "# end_train_day_x = end_train_day_x\n",
    "# result_dir_path = result_dir_org_path / str(end_train_day_x)\n",
    "# work_dir_path = work_dir_org_path / str(end_train_day_x)\n",
    "# model_dir_path = model_dir_org_path / str(end_train_day_x)\n",
    "# grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "# lag_feature_path, target_encoding_feature_path, result_submission_path = update_file_path()\n",
    "# grid_base_path, grid_price_path, grid_calendar_path, holdout_path, \\\n",
    "# lag_feature_path, target_encoding_feature_path, result_submission_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid_df will have data till d_(end_train_day_x + predcition_horizon). \n",
    "# #e.g: if end_train_day_x= 1913, predcition_horizon = 13, then d_1 to d_1920 data is there.\n",
    "# #d_1913 is there for train and last 7 days of data is there for holdout/validation.\n",
    "# #after training with data till 1913, we will generate the forecast for d_1914 to d_1920 and compare\n",
    "# #with actual sales numbers.\n",
    "# #same goes for grid_df for prices and calendars. They will have the same number of rows which will be concatenated\n",
    "# #along the columns later to get all the base features, and price and calendar related features. \n",
    "# grid_df= generate_grid_base(train_df, prices_df, calendar_df)\n",
    "# # grid_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.columns, prices_df.columns, calendar_df.columns, grid_price_df.columns, grid_calendar_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_price_df = generate_grid_price(prices_df, calendar_df)\n",
    "# grid_price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_calendar_df= generate_grid_calendar(calendar_df)\n",
    "# grid_calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify_grid_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_lag_day_list, num_shift_rolling_day_list = update_predict_horizon(num_lag_day)\n",
    "# num_lag_day_list, num_shift_rolling_day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_grid_full(train_df, prices_df, calendar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = generate_lag_feature(grid_base_path, lag_feature_path, num_lag_day_list, num_rolling_day_list, recursive_feature_flag, num_shift_rolling_day_list)\n",
    "# grid_df.loc[grid_df[\"id\"] == \"HOBBIES_2_006_CA_1_evaluation\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df= generate_target_encoding_feature(grid_base_path)\n",
    "# print(grid_df.columns)\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_calendar_df.tm_d.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"Data/sales_train_evaluation.csv\")\n",
    "# prices_df = pd.read_csv(\"Data/sell_prices.csv\")\n",
    "# calendar_df = pd.read_csv(\"Data/calendar.csv\")\n",
    "# submission_df = pd.read_csv(\"Data/sample_submission.csv\")\n",
    "# print(train_df.shape, prices_df.shape, calendar_df.shape, submission_df.shape)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('generate_grid_base')\n",
    "# index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "# grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name= target)\n",
    "# print(grid_df.shape)\n",
    "# assert grid_df.shape[0] == train_df.shape[0]*1941\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('remove days before end_train_day_x / generate holdout')\n",
    "# num_before = grid_df.shape[0]\n",
    "# grid_df['d_org'] = grid_df['d']\n",
    "# grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_df = grid_df[(grid_df['d'] > end_train_day_x) & \\\n",
    "#                              (grid_df['d'] <= end_train_day_x + prediction_horizon)][main_index_list + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_df.to_csv(\"Data/holdout_df_1914_1941_28.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = reduce_mem_usage(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = grid_df[grid_df['d'] <= end_train_day_x]\n",
    "# grid_df['d'] = grid_df['d_org']\n",
    "# grid_df = grid_df.drop('d_org', axis=1)\n",
    "# num_after = grid_df.shape[0]\n",
    "# print(\"Number of train data after separating holdout set: \", num_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_grid = pd.DataFrame()\n",
    "# for i in range(prediction_horizon):\n",
    "#     temp_df = train_df[index_columns]\n",
    "#     temp_df = temp_df.drop_duplicates()\n",
    "#     temp_df['d'] = 'd_' + str(end_train_day_x + i + 1)\n",
    "#     temp_df[target] = np.nan\n",
    "#     add_grid = pd.concat([add_grid, temp_df])\n",
    "# print(add_grid.shape)\n",
    "# add_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = pd.concat([grid_df, add_grid])\n",
    "# grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# del temp_df, add_grid\n",
    "# del train_df\n",
    "# gc.collect()\n",
    "# print(grid_df.shape)\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('convert to category')\n",
    "# for col in index_columns:\n",
    "#     grid_df[col] = grid_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = reduce_mem_usage(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('calc release week') #when this item in an store was sold for first time\n",
    "# release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "# release_df.columns = ['store_id', 'item_id', 'release']\n",
    "# grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
    "# del release_df\n",
    "# gc.collect()\n",
    "# grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
    "# grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# print('convert release to int16')\n",
    "# grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "# grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# print('save grid_base')\n",
    "# grid_df.to_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "\n",
    "# print('grid_df.shape', grid_df.shape)\n",
    "# print(grid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_df.shape)\n",
    "# print(grid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('generate_grid_price')\n",
    "# print('load grid_base')\n",
    "# grid_df = pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "\n",
    "# prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "# prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "# prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "# prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "# prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
    "# prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "# prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
    "# calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "# prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "# del calendar_prices\n",
    "\n",
    "# prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])[\n",
    "#     'sell_price'].transform(lambda x: x.shift(1))\n",
    "# prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])[\n",
    "#     'sell_price'].transform('mean')\n",
    "# prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])[\n",
    "#     'sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]\n",
    "# prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]\n",
    "# prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]\n",
    "\n",
    "# del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_df= reduce_mem_usage(prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('merge prices')\n",
    "# original_columns = list(grid_df)\n",
    "# grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "# keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "# grid_df = grid_df[main_index_list + keep_columns]\n",
    "# grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# print('save grid_price')\n",
    "# grid_df.to_pickle(\"Data/grid_price_1914_1941_28.pkl\")\n",
    "# del prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.loc[grid_df[\"id\"] == \"HOBBIES_2_006_CA_1_evaluation\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "#     diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)\n",
    "#     days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "#     lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "#     phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "#     return int(phase_index) & 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('generate_grid_calendar')\n",
    "# grid_df = pd.read_pickle(\"Data/grid_price_1914_1941_28.pkl\")\n",
    "# grid_df = grid_df[main_index_list]\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)\n",
    "# calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge calendar partly\n",
    "# icols = ['date', 'd','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI','moon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "# for col in icols:\n",
    "#     grid_df[col] = grid_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "# grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "# grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "# grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "# grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "# grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x / 7)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "# grid_df['tm_w_end'] = (grid_df['tm_dw'] >= 5).astype(np.int8)\n",
    "# del grid_df['date']\n",
    "\n",
    "# grid_df.to_pickle(\"Data/grid_calendar_1914_1941_28.pkl\")\n",
    "\n",
    "# del calendar_df\n",
    "# del grid_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('modify_grid_base')\n",
    "# grid_df = pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "# grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# del grid_df['wm_yr_wk']\n",
    "# grid_df.to_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "\n",
    "# del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('load_grid_full')\n",
    "# grid_df = pd.concat([pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\"),\n",
    "#                      pd.read_pickle(\"Data/grid_price_1914_1941_28.pkl\").iloc[:, 2:],\n",
    "#                      pd.read_pickle(\"Data/grid_calendar_1914_1941_28.pkl\").iloc[:, 2:]],\n",
    "#                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('generate_lag')\n",
    "# print('load grid_base')\n",
    "# grid_df = pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "\n",
    "# grid_df = grid_df[['id', 'd', 'sales']]\n",
    "\n",
    "# start_time = time.time()\n",
    "# print('create lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # why lag from 28-42 and not 1-15? Because for test data (future dates),we wont have the sales numbers for 1-27 lags since these lags will also\n",
    "# # be in future for 1-28 days out forecast.\n",
    "# # The lags are different for each week model. Model w1's (1-7 days out) lags are {7, 8, 9, ..., 13 }. \n",
    "# # Model w4's (21-28 days out) lags are {28, 29, 30, ..., 34}\n",
    "# # why it is getting shifted by \"prediction_horizon\" before taking rolling mean/std? for the test data (future dates), during our forecast horizon\n",
    "# # we wont have the sales numbers to take the rolling mean/std, that's why first we have to shift by \"prediction_horizon\" and then take the rolling\n",
    "# # mean/std. So effectively, for a prediction_horizon of 28 and rolling mean/std of 7 days is effcetively rolling mean/std of 35 days.\n",
    "# #for each week you would have to shift those features forward by 7 days for the first week (1-7 days), 14 days for the second (8-14 days), etc.\n",
    "# num_lag_day_list = []\n",
    "# for col in range(prediction_horizon, prediction_horizon + num_lag_day):\n",
    "#     num_lag_day_list.append(col)\n",
    "# print(num_lag_day_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = grid_df.assign(**{\n",
    "#     '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "#     for l in num_lag_day_list\n",
    "#     for col in [target]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.loc[grid_df[\"id\"] == \"HOBBIES_1_001_CA_1_evaluation\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rolling_day_list = [7, 14, 30, 60, 180]\n",
    "\n",
    "# num_shift_rolling_day_list = []\n",
    "# for num_shift_day in [1, 7, 14]:\n",
    "#     for num_rolling_day in [7, 14, 30, 60]:\n",
    "#         num_shift_rolling_day_list.append([num_shift_day, num_rolling_day])\n",
    "# num_shift_rolling_day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in list(grid_df):\n",
    "#     if 'lag' in col:\n",
    "#         grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "# start_time = time.time()\n",
    "# print('create rolling aggs')\n",
    "# # why it is getting shifted by \"prediction_horizon\" before taking rolling mean/std? for the test data (future dates), during our forecast horizon\n",
    "# # we wont have the sales numbers to take the rolling mean/std, that's why first we have to shift by \"prediction_horizon\" and then take the rolling\n",
    "# # mean/std. So effectively, for a prediction_horizon of 28 and rolling mean/std of 7 days is effcetively rolling mean/std of 35 days.\n",
    "# #for each week you would have to shift those features forward by 7 days for the first week (1-7 days), 14 days for the second (8-14 days), etc.\n",
    "# for num_rolling_day in num_rolling_day_list:\n",
    "#     print('rolling period', num_rolling_day)\n",
    "#     grid_df['rolling_mean_' + str(num_rolling_day)] = grid_df.groupby(['id'])[target].transform(\n",
    "#         lambda x: x.shift(prediction_horizon).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "#     grid_df['rolling_std_' + str(num_rolling_day)] = grid_df.groupby(['id'])[target].transform(\n",
    "#         lambda x: x.shift(prediction_horizon).rolling(num_rolling_day).std()).astype(np.float16)\n",
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.loc[grid_df[\"id\"] == \"HOBBIES_1_001_CA_1_evaluation\"].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if recursive_feature_flag:\n",
    "#     for num_shift_rolling_day in num_shift_rolling_day_list:\n",
    "#         num_shift_day = num_shift_rolling_day[0]\n",
    "#         num_rolling_day = num_shift_rolling_day[1]\n",
    "#         col_name = 'rolling_mean_tmp_' + str(num_shift_day) + '_' + str(num_rolling_day)\n",
    "#         grid_df[col_name] = grid_df.groupby(['id'])[target].transform(\n",
    "#             lambda x: x.shift(num_shift_day).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "\n",
    "# print('save lag_feature')\n",
    "# grid_df.to_pickle(\"Data/lag_feature_1914_1941_28.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(seed)\n",
    "# grid_df = pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\")\n",
    "# grid_df[target][grid_df['d'] > (end_train_day_x - prediction_horizon)] = np.nan\n",
    "# base_cols = list(grid_df)\n",
    "\n",
    "# icols = [['state_id'], ['store_id'], ['cat_id'], ['dept_id'], ['state_id', 'cat_id'], ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
    "#          ['store_id', 'dept_id'], ['item_id'], ['item_id', 'state_id'], ['item_id', 'store_id']]\n",
    "\n",
    "# for col in icols:\n",
    "#     print('encoding', col)\n",
    "#     col_name = '_' + '_'.join(col) + '_'\n",
    "#     grid_df['enc' + col_name + 'mean'] = grid_df.groupby(col)[target].transform('mean').astype(np.float16)\n",
    "#     grid_df['enc' + col_name + 'std'] = grid_df.groupby(col)[target].transform('std').astype(np.float16)\n",
    "\n",
    "# keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "# grid_df = grid_df[['id', 'd'] + keep_cols]\n",
    "\n",
    "# print('save target_encoding_feature')\n",
    "# grid_df.to_pickle(\"Data/target_encoding_1914_1941_28.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_grid_by_store(store_id):\n",
    "#     print('load_grid_full')\n",
    "#     df = pd.concat([pd.read_pickle(\"Data/grid_base_1914_1941_28.pkl\"),\n",
    "#                     pd.read_pickle(\"Data/grid_price_1914_1941_28.pkl\").iloc[:, 2:],\n",
    "#                     pd.read_pickle(\"Data/grid_calendar_1914_1941_28.pkl\").iloc[:, 2:]],\n",
    "#                     axis=1)\n",
    "#     store_id = \"all\"\n",
    "#     if store_id != 'all':\n",
    "#         df = df[df['store_id'] == store_id]\n",
    "#     mean_features = ['enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std']\n",
    "#     df2 = pd.read_pickle(\"Data/target_encoding_1914_1941_28.pkl\")[mean_features]\n",
    "#     df2 = df2[df2.index.isin(df.index)]\n",
    "\n",
    "#     df3 = pd.read_pickle(\"Data/lag_feature_1914_1941_28.pkl\").iloc[:, 3:]\n",
    "#     df3 = df3[df3.index.isin(df.index)]\n",
    "\n",
    "#     df = pd.concat([df, df2], axis=1)\n",
    "#     del df2\n",
    "\n",
    "#     df = pd.concat([df, df3], axis=1)\n",
    "#     del df3\n",
    "\n",
    "#     remove_features = ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', target]\n",
    "#     enable_features = [col for col in list(df) if col not in remove_features]\n",
    "#     df = df[['id', 'd', target] + enable_features]\n",
    "\n",
    "#     df = df[df['d'] >= start_train_day_x].reset_index(drop=True)\n",
    "#     df= reduce_mem_usage(df)\n",
    "#     return df, enable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_base_test(store_id_set_list):\n",
    "#     base_test = pd.DataFrame()\n",
    "\n",
    "#     for store_id in store_id_set_list:\n",
    "#         temp_df = pd.read_pickle(\"Data/\"f'test_1914_1941_{store_id}_{prediction_horizon}.pkl')\n",
    "#         temp_df['store_id'] = store_id\n",
    "#         base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "#     return base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_test = pd.DataFrame()\n",
    "\n",
    "# for store_id in store_id_set_list:\n",
    "#     temp_df = pd.read_pickle(work_dir_path / f'test_{store_id}_{prediction_horizon}.pkl')\n",
    "#     temp_df['store_id'] = store_id\n",
    "#     base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'tweedie',\n",
    "#     'tweedie_variance_power': 1.1,\n",
    "#     'metric': 'rmse',\n",
    "#     'subsample': 0.5,\n",
    "#     'subsample_freq': 1,\n",
    "#     'learning_rate': 0.03,\n",
    "#     'num_leaves': 2 ** 11 - 1,\n",
    "#     'min_data_in_leaf': 2 ** 12 - 1,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'max_bin': 100,\n",
    "#     'n_estimators': 1400,\n",
    "#     'boost_from_average': False,\n",
    "#     'verbose': -1,\n",
    "# }\n",
    "\n",
    "# set_seed(seed)\n",
    "# lgb_params['seed'] = seed\n",
    "\n",
    "# store_id_set_list = list(train_df['store_id'].unique())\n",
    "# store_id_set_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_index, store_id = 0, store_id_set_list[0]\n",
    "# grid_df, enable_features = load_grid_by_store(store_id)\n",
    "# enable_features = enable_features\n",
    "\n",
    "# train_mask = grid_df['d'] <= end_train_day_x\n",
    "# valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - prediction_horizon))\n",
    "# preds_mask = grid_df['d'] > (end_train_day_x - 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = reduce_mem_usage(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('[{3} - {4}] train {0}/{1} {2}'.format(store_index + 1, len(store_id_set_list), store_id, end_train_day_x, prediction_horizon))\n",
    "# train_data = lgb.Dataset(grid_df[train_mask][enable_features],\n",
    "#                          label=grid_df[train_mask][target])\n",
    "\n",
    "# valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "#                          label=grid_df[valid_mask][target])\n",
    "\n",
    "# valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "#                          label=grid_df[valid_mask][target])\n",
    "\n",
    "# # Saving part of the dataset for later predictions\n",
    "# # Removing features that we need to calculate recursively\n",
    "# grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "# grid_df.to_pickle(\"Data/\"f'test_{store_id}_1914_1941_{prediction_horizon}.pkl')\n",
    "\n",
    "# del grid_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data= reduce_mem_usage(train_data)\n",
    "# valid_data= reduce_mem_usage(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_all_df = pd.DataFrame()\n",
    "# set_seed(seed)\n",
    "# estimator = lgb.train(lgb_params, train_data, valid_sets=[valid_data], verbose_eval=False,\n",
    "#                       callbacks=[log_evaluation(period=100)])\n",
    "\n",
    "# model_name = str(\"Data/\"f'lgb_model_{store_id}_1914_1941_{prediction_horizon}.bin')\n",
    "# feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),\n",
    "#                                            columns=['feature_name', 'importance'])\n",
    "# feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)\n",
    "# feature_importance_store_df['store_id'] = store_id\n",
    "# feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])\n",
    "# pickle.dump(estimator, open(model_name, 'wb'))\n",
    "# del train_data, valid_data, estimator\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('aggregate feature importance')\n",
    "# feature_importance_all_df.to_csv(\"Data/feature_importance_1914_1941_{0}_{1}.csv\".format(store_id, prediction_horizon), index=False)\n",
    "# feature_importance_agg_df = feature_importance_all_df.groupby('feature_name')['importance'].agg(['mean', 'std']).reset_index()\n",
    "# feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']\n",
    "# feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)\n",
    "# feature_importance_agg_df.to_csv(\"Data/feature_importance_agg_1914_1941_{0}_{1}.csv\".format(store_id, prediction_horizon), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('load base_test')\n",
    "# store_id_set_list= list(store_id)\n",
    "# base_test = load_base_test(store_id_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_time = time.time()\n",
    "# pred_h_df = pd.DataFrame()\n",
    "# for predict_day in range(prediction_horizon_prev + 1, prediction_horizon + 1):\n",
    "#     print('predict day{:02d}'.format(predict_day))\n",
    "#     start_time = time.time()\n",
    "#     grid_df = base_test.copy()\n",
    "\n",
    "#     if self.params.recursive_feature_flag:\n",
    "#         self.log.info('[{0} - {1}] calculate recursive features'.format(\n",
    "#             self.params.end_train_day_x, self.params.prediction_horizon))\n",
    "#         for num_shift_rolling_day in self.params.num_shift_rolling_day_list:\n",
    "#             num_shift_day = num_shift_rolling_day[0]\n",
    "#             num_rolling_day = num_shift_rolling_day[1]\n",
    "#             lag_df = base_test[['id', 'd', self.params.target]]\n",
    "#             col_name = 'rolling_mean_tmp_' + str(num_shift_day) + '_' + str(num_rolling_day)\n",
    "#             lag_df[col_name] = lag_df.groupby(['id'])[self.params.target].transform(\n",
    "#                 lambda x: x.shift(num_shift_day).rolling(num_rolling_day).mean())\n",
    "#             grid_df = pd.concat([grid_df, lag_df[[col_name]]], axis=1)\n",
    "\n",
    "#     day_mask = base_test['d'] == (self.params.end_train_day_x + predict_day)\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export recursive_features')\n",
    "#         grid_df[day_mask].to_csv(self.params.result_dir_path / 'exp_recursive_features_{0}_{1}.csv'.format(\n",
    "#             self.params.prediction_horizon, predict_day), index=False)\n",
    "#     for store_index, store_id in enumerate(store_id_set_list):\n",
    "#         self.log.info('[{3} - {4}] predict {0}/{1} {2} day {5}'.format(\n",
    "#             store_index + 1, len(store_id_set_list), store_id,\n",
    "#             self.params.end_train_day_x, self.params.prediction_horizon, predict_day))\n",
    "\n",
    "#         model_path = str(\n",
    "#             self.params.model_dir_path / f'lgb_model_{store_id}_{self.params.prediction_horizon}.bin')\n",
    "\n",
    "#         estimator = pickle.load(open(model_path, 'rb'))\n",
    "#         if store_id != 'all':\n",
    "#             store_mask = base_test['store_id'] == store_id\n",
    "#             mask = (day_mask) & (store_mask)\n",
    "#         else:\n",
    "#             mask = day_mask\n",
    "\n",
    "#         if self.params.export_all_flag:\n",
    "#             self.log.info('export pred')\n",
    "#             grid_df[mask].to_csv(\n",
    "#                 self.params.result_dir_path / (\n",
    "#                         'exp_pred_' + store_id + '_day_' + str(predict_day) + '.csv'), index=False)\n",
    "#         base_test[self.params.target][mask] = estimator.predict(grid_df[mask][self.params.enable_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if  prediction_horizon_prev > 0:\n",
    "#     pred_v_prev_df = None\n",
    "#     for ph in prediction_horizon_list:\n",
    "#         if ph <= prediction_horizon_prev:\n",
    "#             pred_v_temp_df = pd.read_csv(result_dir_path / 'pred_v_{}.csv'.format(ph))\n",
    "#             pred_v_prev_df = pd.concat([pred_v_prev_df, pred_v_temp_df])\n",
    "#     for predict_day in range(1, prediction_horizon_prev + 1):\n",
    "#         base_test[target][base_test['d'] == (end_train_day_x + predict_day)] = \\\n",
    "#             pred_v_prev_df[target][\n",
    "#                 pred_v_prev_df['d'] == (end_train_day_x + predict_day)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = str(\"Data/\"f'lgb_model_{store_id}_1914_1941_{prediction_horizon}.bin')\n",
    "# model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_mask[-100:])\n",
    "# print(valid_mask[-100:])\n",
    "# print(preds_mask[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_and_predict(self, train_df, calendar_df, prices_df, submission_df):\n",
    "# feature_importance_all_df = pd.DataFrame()\n",
    "# for store_index, store_id in enumerate(store_id_set_list):\n",
    "#     print('train', store_id)\n",
    "\n",
    "#     grid_df, enable_features = load_grid_by_store(store_id)\n",
    "#     enable_features = enable_features\n",
    "\n",
    "#     train_mask = grid_df['d'] <= end_train_day_x\n",
    "#     valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - prediction_horizon))\n",
    "#     preds_mask = grid_df['d'] > (end_train_day_x - 100)\n",
    "\n",
    "#     self.log.info('[{3} - {4}] train {0}/{1} {2}'.format(\n",
    "#         store_index + 1, len(store_id_set_list), store_id,\n",
    "#         self.params.end_train_day_x, self.params.prediction_horizon))\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export train')\n",
    "#         grid_df[train_mask].to_csv(\n",
    "#             self.params.result_dir_path / ('exp_train_' + store_id + '.csv'), index=False)\n",
    "#     train_data = lgb.Dataset(grid_df[train_mask][enable_features],\n",
    "#                              label=grid_df[train_mask][self.params.target])\n",
    "\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export valid')\n",
    "#         grid_df[valid_mask].to_csv(\n",
    "#             self.params.result_dir_path / ('exp_valid_' + store_id + '.csv'), index=False)\n",
    "#     valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "#                              label=grid_df[valid_mask][self.params.target])\n",
    "\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export test')\n",
    "#         grid_df[preds_mask].to_csv(\n",
    "#             self.params.result_dir_path / ('exp_test_' + store_id + '.csv'), index=False)\n",
    "\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export train_valid_test')\n",
    "#         grid_df[train_mask | valid_mask | preds_mask].to_csv(\n",
    "#             self.params.result_dir_path / ('exp_train_valid_test_' + store_id + '.csv'), index=False)\n",
    "#     valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "#                              label=grid_df[valid_mask][self.params.target])\n",
    "\n",
    "#     # Saving part of the dataset for later predictions\n",
    "#     # Removing features that we need to calculate recursively\n",
    "#     grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "#     if self.params.recursive_feature_flag:\n",
    "#         keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "#         grid_df = grid_df[keep_cols]\n",
    "#     grid_df.to_pickle(self.params.work_dir_path / f'test_{store_id}_{self.params.prediction_horizon}.pkl')\n",
    "#     del grid_df\n",
    "\n",
    "#     Util.set_seed(self.params.seed)\n",
    "#     estimator = lgb.train(lgb_params,\n",
    "#                           train_data,\n",
    "#                           valid_sets=[valid_data],\n",
    "#                           verbose_eval=False,\n",
    "#                           callbacks=[self.log.log_evaluation(period=100)],\n",
    "#                           )\n",
    "\n",
    "#     model_name = str(\n",
    "#         self.params.model_dir_path / f'lgb_model_{store_id}_{self.params.prediction_horizon}.bin')\n",
    "#     feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),\n",
    "#                                                columns=['feature_name', 'importance'])\n",
    "#     feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)\n",
    "#     feature_importance_store_df['store_id'] = store_id\n",
    "#     feature_importance_store_df.to_csv(\n",
    "#         self.params.result_dir_path / ('feature_importance_{0}_{1}.csv'.format(\n",
    "#             store_id, self.params.prediction_horizon)), index=False)\n",
    "#     feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])\n",
    "#     pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "#     del train_data, valid_data, estimator\n",
    "#     gc.collect()\n",
    "\n",
    "# self.log.info('aggregate feature importance')\n",
    "# feature_importance_all_df.to_csv(self.params.result_dir_path / 'feature_importance_all_{0}.csv'.format(\n",
    "#     self.params.prediction_horizon), index=False)\n",
    "# feature_importance_agg_df = feature_importance_all_df.groupby(\n",
    "#     'feature_name')['importance'].agg(['mean', 'std']).reset_index()\n",
    "# feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']\n",
    "# feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)\n",
    "# feature_importance_agg_df.to_csv(self.params.result_dir_path / 'feature_importance_agg_{0}.csv'.format(\n",
    "#     self.params.prediction_horizon), index=False)\n",
    "\n",
    "# self.log.info('load base_test')\n",
    "# base_test = self.load_base_test(store_id_set_list)\n",
    "\n",
    "# if self.params.export_all_flag:\n",
    "#     base_test.to_csv(\n",
    "#         self.params.result_dir_path / 'exp_base_test_{0}_a.csv'.format(self.params.prediction_horizon),\n",
    "#         index=False)\n",
    "# if self.params.prediction_horizon_prev > 0:\n",
    "#     pred_v_prev_df = None\n",
    "#     for ph in self.params.prediction_horizon_list:\n",
    "#         if ph <= self.params.prediction_horizon_prev:\n",
    "#             pred_v_temp_df = pd.read_csv(self.params.result_dir_path / 'pred_v_{}.csv'.format(ph))\n",
    "#             pred_v_prev_df = pd.concat([pred_v_prev_df, pred_v_temp_df])\n",
    "#     for predict_day in range(1, self.params.prediction_horizon_prev + 1):\n",
    "#         base_test[self.params.target][base_test['d'] == (self.params.end_train_day_x + predict_day)] = \\\n",
    "#             pred_v_prev_df[self.params.target][\n",
    "#                 pred_v_prev_df['d'] == (self.params.end_train_day_x + predict_day)].values\n",
    "\n",
    "# if self.params.export_all_flag:\n",
    "#     base_test.to_csv(\n",
    "#         self.params.result_dir_path / 'exp_base_test_{0}_b.csv'.format(self.params.prediction_horizon),\n",
    "#         index=False)\n",
    "\n",
    "# main_time = time.time()\n",
    "# pred_h_df = pd.DataFrame()\n",
    "# for predict_day in range(self.params.prediction_horizon_prev + 1, self.params.prediction_horizon + 1):\n",
    "#     self.log.info('predict day{:02d}'.format(predict_day))\n",
    "#     start_time = time.time()\n",
    "#     grid_df = base_test.copy()\n",
    "\n",
    "#     if self.params.recursive_feature_flag:\n",
    "#         self.log.info('[{0} - {1}] calculate recursive features'.format(\n",
    "#             self.params.end_train_day_x, self.params.prediction_horizon))\n",
    "#         for num_shift_rolling_day in self.params.num_shift_rolling_day_list:\n",
    "#             num_shift_day = num_shift_rolling_day[0]\n",
    "#             num_rolling_day = num_shift_rolling_day[1]\n",
    "#             lag_df = base_test[['id', 'd', self.params.target]]\n",
    "#             col_name = 'rolling_mean_tmp_' + str(num_shift_day) + '_' + str(num_rolling_day)\n",
    "#             lag_df[col_name] = lag_df.groupby(['id'])[self.params.target].transform(\n",
    "#                 lambda x: x.shift(num_shift_day).rolling(num_rolling_day).mean())\n",
    "#             grid_df = pd.concat([grid_df, lag_df[[col_name]]], axis=1)\n",
    "\n",
    "#     day_mask = base_test['d'] == (self.params.end_train_day_x + predict_day)\n",
    "#     if self.params.export_all_flag:\n",
    "#         self.log.info('export recursive_features')\n",
    "#         grid_df[day_mask].to_csv(self.params.result_dir_path / 'exp_recursive_features_{0}_{1}.csv'.format(\n",
    "#             self.params.prediction_horizon, predict_day), index=False)\n",
    "#     for store_index, store_id in enumerate(store_id_set_list):\n",
    "#         self.log.info('[{3} - {4}] predict {0}/{1} {2} day {5}'.format(\n",
    "#             store_index + 1, len(store_id_set_list), store_id,\n",
    "#             self.params.end_train_day_x, self.params.prediction_horizon, predict_day))\n",
    "\n",
    "#         model_path = str(\n",
    "#             self.params.model_dir_path / f'lgb_model_{store_id}_{self.params.prediction_horizon}.bin')\n",
    "\n",
    "#         estimator = pickle.load(open(model_path, 'rb'))\n",
    "#         if store_id != 'all':\n",
    "#             store_mask = base_test['store_id'] == store_id\n",
    "#             mask = (day_mask) & (store_mask)\n",
    "#         else:\n",
    "#             mask = day_mask\n",
    "\n",
    "#         if self.params.export_all_flag:\n",
    "#             self.log.info('export pred')\n",
    "#             grid_df[mask].to_csv(\n",
    "#                 self.params.result_dir_path / (\n",
    "#                         'exp_pred_' + store_id + '_day_' + str(predict_day) + '.csv'), index=False)\n",
    "#         base_test[self.params.target][mask] = estimator.predict(grid_df[mask][self.params.enable_features])\n",
    "\n",
    "#     temp_df = base_test[day_mask][['id', self.params.target]]\n",
    "#     temp_df.columns = ['id', 'F' + str(predict_day)]\n",
    "#     if 'id' in list(pred_h_df):\n",
    "#         pred_h_df = pred_h_df.merge(temp_df, on=['id'], how='left')\n",
    "#     else:\n",
    "#         pred_h_df = temp_df.copy()\n",
    "\n",
    "#     del temp_df\n",
    "\n",
    "# if self.params.export_all_flag:\n",
    "#     base_test.to_csv(\n",
    "#         self.params.result_dir_path / 'exp_base_test_{0}_c.csv'.format(self.params.prediction_horizon),\n",
    "#         index=False)\n",
    "# pred_h_df.to_csv(self.params.result_dir_path / 'pred_h_{}.csv'.format(\n",
    "#     self.params.prediction_horizon), index=False)\n",
    "\n",
    "# pred_v_df = base_test[\n",
    "#     (base_test['d'] >= self.params.end_train_day_x + self.params.prediction_horizon_prev + 1) *\n",
    "#     (base_test['d'] < self.params.end_train_day_x + self.params.prediction_horizon + 1)\n",
    "#     ][\n",
    "#     self.params.main_index_list + [self.params.target]\n",
    "#     ]\n",
    "# pred_v_df.to_csv(self.params.result_dir_path / 'pred_v_{}.csv'.format(self.params.prediction_horizon),\n",
    "#                  index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
